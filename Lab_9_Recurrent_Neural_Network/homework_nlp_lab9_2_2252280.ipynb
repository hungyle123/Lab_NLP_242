{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed930e3b",
   "metadata": {
    "papermill": {
     "duration": 0.003083,
     "end_time": "2025-03-25T11:01:17.740529",
     "exception": false,
     "start_time": "2025-03-25T11:01:17.737446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Home Exercise 1 on Text Classification\n",
    "- Implement a Recurrent Neural Network model (Vanilla RNN, GRU, and LSTM) to predict whether a review is positive or negative.\n",
    "\n",
    "- Data: IMDB Dataset of 50K Movie Reviews (the last 10% of rows serve as the test set).\n",
    "\n",
    "  \n",
    "- Compare the performance of the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdee31",
   "metadata": {
    "papermill": {
     "duration": 0.002158,
     "end_time": "2025-03-25T11:01:17.745365",
     "exception": false,
     "start_time": "2025-03-25T11:01:17.743207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044309ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T11:01:17.751233Z",
     "iopub.status.busy": "2025-03-25T11:01:17.750956Z",
     "iopub.status.idle": "2025-03-25T11:01:22.853887Z",
     "shell.execute_reply": "2025-03-25T11:01:22.852761Z"
    },
    "papermill": {
     "duration": 5.107764,
     "end_time": "2025-03-25T11:01:22.855471",
     "exception": false,
     "start_time": "2025-03-25T11:01:17.747707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=15XOhuhoOde7J4y8cCPAKEwDS3PlWcoHW\r\n",
      "To: /kaggle/working/archive.zip\r\n",
      "100%|█████████████████████████████████████████| 473k/473k [00:00<00:00, 117MB/s]\r\n",
      "Archive:  archive.zip\r\n",
      "  inflating: GMB_dataset.txt         \r\n"
     ]
    }
   ],
   "source": [
    "!gdown 15XOhuhoOde7J4y8cCPAKEwDS3PlWcoHW -O archive.zip\n",
    "# Unzip the specific file from the archive\n",
    "!unzip archive.zip GMB_dataset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9a11b",
   "metadata": {
    "papermill": {
     "duration": 0.002798,
     "end_time": "2025-03-25T11:01:22.862242",
     "exception": false,
     "start_time": "2025-03-25T11:01:22.859444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EDA and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5ee783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T11:01:22.869216Z",
     "iopub.status.busy": "2025-03-25T11:01:22.868957Z",
     "iopub.status.idle": "2025-03-25T11:01:28.341963Z",
     "shell.execute_reply": "2025-03-25T11:01:28.341034Z"
    },
    "papermill": {
     "duration": 5.47754,
     "end_time": "2025-03-25T11:01:28.343255",
     "exception": false,
     "start_time": "2025-03-25T11:01:22.865715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing and EDA...\n",
      "Data loaded successfully.\n",
      "\n",
      "Performing Exploratory Data Analysis...\n",
      "\n",
      "1. Basic dataset statistics:\n",
      "- Total rows: 66161\n",
      "- Columns: ['Unnamed: 0', 'Sentence #', 'Word', 'POS', 'Tag']\n",
      "- Sample data:\n",
      "   Unnamed: 0  Sentence #           Word  POS Tag\n",
      "0           0         1.0      Thousands  NNS   O\n",
      "1           1         1.0             of   IN   O\n",
      "2           2         1.0  demonstrators  NNS   O\n",
      "3           3         1.0           have  VBP   O\n",
      "4           4         1.0        marched  VBN   O\n",
      "\n",
      "2. Missing values analysis:\n",
      "Unnamed: 0    0\n",
      "Sentence #    0\n",
      "Word          0\n",
      "POS           0\n",
      "Tag           0\n",
      "dtype: int64\n",
      "\n",
      "Filled NaN values in 'Sentence #' column.\n",
      "\n",
      "3. Tag distribution:\n",
      "Tag\n",
      "O        56217\n",
      "B-geo     2070\n",
      "B-org     1237\n",
      "I-per     1234\n",
      "B-gpe     1230\n",
      "B-tim     1160\n",
      "B-per     1107\n",
      "I-org      926\n",
      "I-geo      414\n",
      "I-tim      334\n",
      "B-art       53\n",
      "B-eve       45\n",
      "I-eve       37\n",
      "I-gpe       34\n",
      "I-art       34\n",
      "B-nat       20\n",
      "I-nat        9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4. Sentence length statistics:\n",
      "- Average length: 22.06\n",
      "- Min length: 2\n",
      "- Max length: 70\n",
      "- 90th percentile: 32.0\n",
      "\n",
      "5. Example sentences:\n",
      "\n",
      "Sentence 1:\n",
      "Words: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Tags: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "Sentence 2:\n",
      "Words: Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"\n",
      "Tags: O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAHWCAYAAAB9mLjgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABE5klEQVR4nO3de1yUdf7//+dwVMAZDsopUQyPeMpQaUrNFEVj20x3zdYMrbRctNLdUj5rHivMbTub1rZpu+XX1lYrNQ94oi3RFDNP6Srp6qaAZTAeEhSu3x/dmF8jmFwEziCP++123W7M+/2e63pd84aap9dc77EYhmEIAAAAAFBlXu4uAAAAAADqGoIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAJo0cOVJBQUFX9ZixsbEaOXJkrR/nyJEjslgsWrhwobPtap+vxWLR9OnTr9rxAKA6CFIAYNLu3bv1m9/8Rs2bN1eDBg103XXXqV+/fnrllVdq9bjHjx/X9OnTtXPnzlo9ztWyadMmWSwWvf/+++4upVLnzp3T9OnTtWnTphrfd+/evWWxWGSxWOTl5SWr1ao2bdpoxIgRyszMrLHjfPzxxx4bSDy5NgCoCh93FwAAdcnmzZt12223qVmzZho9erQiIyN17NgxbdmyRS+99JLGjx9fa8c+fvy4ZsyYodjYWN1www21dhz86Ny5c5oxY4akH4NPTWvatKkyMjIkSWfPntWhQ4e0dOlSvfPOOxo6dKjeeecd+fr6OscfOHBAXl7m/v3z448/1ty5c00FlubNm+uHH35wOXZt+LnafvjhB/n48BYFgGfjv1IAYMLTTz8tm82mbdu2KTg42KWvoKDAPUWhTrLZbLr33ntd2mbPnq1HHnlEr732mmJjY/Xss886+/z9/Wu1nosXL6qsrEx+fn5q0KBBrR7rStx9fACoCj7aBwAm5Obmqn379hVClCSFh4dXaHvnnXeUkJCghg0bKjQ0VMOGDdOxY8dcxvTu3VsdOnTQvn37dNtttykgIEDXXXed5syZ4xyzadMmdevWTZI0atQo58fCfnofy9atWzVgwADZbDYFBATo1ltv1WeffeZyrOnTp8tisejQoUMaOXKkgoODZbPZNGrUKJ07d67S+rt3766AgACFhISoV69eWrt2rcuYVatWqWfPngoMDFSjRo2UkpKivXv3XvG1rKrCwkI99thjiomJkb+/v1q2bKlnn31WZWVlzjHl9/U899xzeuONNxQXFyd/f39169ZN27Ztq7DPJUuWKD4+Xg0aNFCHDh20bNkyjRw5UrGxsc79NWnSRJI0Y8YM5+t96dWTb775RoMGDVJQUJCaNGmiP/7xjyotLa32uXp7e+vll19WfHy8Xn31VRUVFTn7Lr1H6sKFC5oxY4ZatWqlBg0aKCwsTD169HB+NHDkyJGaO3euJDnrt1gsFV6vF1980fl67du3r9J7pMp9/fXXSk5OVmBgoKKjozVz5kwZhuHsL/+45qUfh7x0nz9XW3nbpa/1F198oYEDB8pqtSooKEh9+/bVli1bXMYsXLhQFotFn332mSZOnKgmTZooMDBQd911l06ePHnlCQAAE7giBQAmNG/eXNnZ2dqzZ486dOjws2OffvppPfnkkxo6dKgefPBBnTx5Uq+88op69eqlL774wiWMff/99xowYIAGDx6soUOH6v3339ekSZPUsWNHDRw4UO3atdPMmTM1depUjRkzRj179pQk3XzzzZKkDRs2aODAgUpISNC0adPk5eWlBQsWqE+fPvr3v/+t7t27u9Q2dOhQtWjRQhkZGdqxY4fefPNNhYeHu1wBmTFjhqZPn66bb75ZM2fOlJ+fn7Zu3aoNGzaof//+kqR//OMfSk1NVXJysp599lmdO3dO8+bNU48ePfTFF184g0l1nTt3Trfeequ++eYbPfTQQ2rWrJk2b96s9PR0nThxQi+++KLL+EWLFun06dN66KGHZLFYNGfOHA0ePFhff/2186NqK1eu1N13362OHTsqIyND33//vR544AFdd911zv00adJE8+bN09ixY3XXXXdp8ODBkqROnTo5x5SWlio5OVmJiYl67rnntG7dOv3lL39RXFycxo4dW+1z9vb21j333KMnn3xSn376qVJSUiodN336dGVkZOjBBx9U9+7d5XA4tH37du3YsUP9+vXTQw89pOPHjyszM1P/+Mc/Kt3HggULdP78eY0ZM0b+/v4KDQ11Cag/VVpaqgEDBuimm27SnDlztHr1ak2bNk0XL17UzJkzTZ1jVWr7qb1796pnz56yWq164okn5Ovrq9dff129e/dWVlaWEhMTXcaPHz9eISEhmjZtmo4cOaIXX3xR48aN03vvvWeqTgD4WQYAoMrWrl1reHt7G97e3obdbjeeeOIJY82aNUZJSYnLuCNHjhje3t7G008/7dK+e/duw8fHx6X91ltvNSQZf//7351txcXFRmRkpDFkyBBn27Zt2wxJxoIFC1z2WVZWZrRq1cpITk42ysrKnO3nzp0zWrRoYfTr18/ZNm3aNEOScf/997vs46677jLCwsKcjw8ePGh4eXkZd911l1FaWlrheIZhGKdPnzaCg4ON0aNHu/Tn5eUZNputQvulNm7caEgylixZctkxs2bNMgIDA43//Oc/Lu2TJ082vL29jaNHjxqGYRiHDx82JBlhYWHGqVOnnOM+/PBDQ5KxfPlyZ1vHjh2Npk2bGqdPn3a2bdq0yZBkNG/e3Nl28uRJQ5Ixbdq0CnWlpqYakoyZM2e6tHfp0sVISEj42fM2jB/nvH379pftX7ZsmSHJeOmll5xtzZs3N1JTU52PO3fubKSkpPzscdLS0ozK/ldf/npZrVajoKCg0r6f/p6Vn+/48eOdbWVlZUZKSorh5+dnnDx50jCM/39ON27ceMV9Xq42wzAqvO6DBg0y/Pz8jNzcXGfb8ePHjUaNGhm9evVyti1YsMCQZCQlJbn8LUyYMMHw9vY2CgsLKz0eAFQHH+0DABP69eun7Oxs/frXv9aXX36pOXPmKDk5Wdddd50++ugj57ilS5eqrKxMQ4cO1bfffuvcIiMj1apVK23cuNFlv0FBQS73y/j5+al79+76+uuvr1jTzp07dfDgQf3ud7/Td9995zzW2bNn1bdvX33yyScVrjI8/PDDLo979uyp7777Tg6HQ5L0wQcfqKysTFOnTq2wwEH5R7AyMzNVWFioe+65x+Ucvb29lZiYWOEcq2PJkiXq2bOnQkJCXI6RlJSk0tJSffLJJy7j7777boWEhLiclyTn63j8+HHt3r1b9913n8ty3rfeeqs6duxour7KXseqzNmVlNd2+vTpy44JDg7W3r17dfDgwWofZ8iQIc6PMFbFuHHjnD9bLBaNGzdOJSUlWrduXbVruJLS0lKtXbtWgwYN0vXXX+9sj4qK0u9+9zt9+umnzt/bcmPGjHH5qGDPnj1VWlqq//73v7VWJ4D6h4/2AYBJ3bp109KlS1VSUqIvv/xSy5Yt0wsvvKDf/OY32rlzp+Lj43Xw4EEZhqFWrVpVuo9LV0Rr2rSpyxs/SQoJCdGuXbuuWE/5G+nU1NTLjikqKnIJGM2aNatwLOnHjxharVbl5ubKy8tL8fHxVzxunz59Ku23Wq1XrP1KDh48qF27dl32zf6lC3z83HlJcr6RbtmyZYV9tWzZUjt27KhybQ0aNKhQV0hIiPNYv8SZM2ckSY0aNbrsmJkzZ+rOO+9U69at1aFDBw0YMEAjRoxw+fjhlbRo0aLKY728vFyCjCS1bt1a0o/3QNWWkydP6ty5c2rTpk2Fvnbt2qmsrEzHjh1T+/btne1X+j0AgJpAkAKAavLz81O3bt3UrVs3tW7dWqNGjdKSJUs0bdo0lZWVyWKxaNWqVfL29q7w3Eu/3LSyMZJcbuS/nPKrTX/+858vuyx6TR7v0uP+4x//UGRkZIX+mli+uqysTP369dMTTzxRaX/5G/lyNXFeVXW5Y9WEPXv2SKo88JXr1auXcnNz9eGHH2rt2rV688039cILL2j+/Pl68MEHq3Schg0b1ki95S79x4Byv2QBjuq4mr8HAOovghQA1ICuXbtKkk6cOCFJiouLk2EYatGiRYU3+9V1uTepcXFxkn68ApSUlFQjx4qLi1NZWZn27dt32XBWftzw8PAaO25lxzhz5kyN7b958+aSpEOHDlXou7Ttcq93bSstLdWiRYsUEBCgHj16/OzY0NBQjRo1SqNGjdKZM2fUq1cvTZ8+3RmkavIcysrK9PXXX7v8Pv/nP/+RJOeiIuVXfgoLC12eW9lH6qpaW5MmTRQQEKADBw5U6Nu/f7+8vLwUExNTpX0BQE3iHikAMGHjxo2V/qv2xx9/LEnOjx8NHjxY3t7emjFjRoXxhmHou+++M33swMBASRXfpCYkJCguLk7PPfec8yNhP1WdZZ8HDRokLy8vzZw5s8L9VeXnk5ycLKvVqmeeeUYXLlyokeNeaujQocrOztaaNWsq9BUWFurixYum9hcdHa0OHTro73//u8trlZWVpd27d7uMDQgIcB7naiktLdUjjzyir776So888sjPfjzy0t+hoKAgtWzZUsXFxc62y/3OVNerr77q/NkwDL366qvy9fVV3759Jf0YVL29vSvcu/baa69V2FdVa/P29lb//v314YcfunyEMD8/X4sWLVKPHj1q5GOkAGAWV6QAwITx48fr3Llzuuuuu9S2bVuVlJRo8+bNeu+99xQbG6tRo0ZJ+vFKylNPPaX09HQdOXJEgwYNUqNGjXT48GEtW7ZMY8aM0R//+EdTx46Li1NwcLDmz5+vRo0aKTAwUImJiWrRooXefPNNDRw4UO3bt9eoUaN03XXX6ZtvvtHGjRtltVq1fPlyU8dq2bKl/vSnP2nWrFnq2bOnBg8eLH9/f23btk3R0dHKyMiQ1WrVvHnzNGLECN14440aNmyYmjRpoqNHj2rlypW65ZZbXN54X86//vUv7d+/v0J7amqqHn/8cX300Uf61a9+pZEjRyohIUFnz57V7t279f777+vIkSNq3LixqXN75plndOedd+qWW27RqFGj9P333+vVV19Vhw4dXMJVw4YNFR8fr/fee0+tW7dWaGioOnTocMVl76uqqKhI77zzjqQfl3k/dOiQli5dqtzcXA0bNkyzZs362efHx8erd+/eSkhIUGhoqLZv367333/fZUGIhIQESdIjjzyi5ORkeXt7a9iwYdWqt0GDBlq9erVSU1OVmJioVatWaeXKlfq///s/571iNptNv/3tb/XKK6/IYrEoLi5OK1asqPTLqs3U9tRTTykzM1M9evTQ73//e/n4+Oj1119XcXGxy/etAcBV5abVAgGgTlq1apVx//33G23btjWCgoIMPz8/o2XLlsb48eON/Pz8CuP/9a9/GT169DACAwONwMBAo23btkZaWppx4MAB55jLLYWdmprqshy3Yfy4nHd8fLzh4+NTYTnpL774whg8eLARFhZm+Pv7G82bNzeGDh1qrF+/3jmmfPnz8uWqy5UvG3348GGX9rfeesvo0qWL4e/vb4SEhBi33nqrkZmZ6TJm48aNRnJysmGz2YwGDRoYcXFxxsiRI43t27f/7GtZvlT25bZ///vfhmH8uMx6enq60bJlS8PPz89o3LixcfPNNxvPPfecc9n58uW1//znP1c4jipZwnzx4sVG27ZtDX9/f6NDhw7GRx99ZAwZMsRo27aty7jNmzcbCQkJhp+fn8t+UlNTjcDAwArHKn99r6R8yfvyLSgoyGjVqpVx7733GmvXrq30OZcuf/7UU08Z3bt3N4KDg42GDRsabdu2NZ5++mmXpfgvXrxojB8/3mjSpIlhsVictf3c63W55c8DAwON3Nxco3///kZAQIARERFhTJs2rcLy+CdPnjSGDBliBAQEGCEhIcZDDz1k7Nmzp8I+L1ebYVQ+Zzt27DCSk5ONoKAgIyAgwLjtttuMzZs3u4wp/z3etm2bS/vllmUHgF/CYhjceQkAwA033KAmTZooMzPT3aUAAOoA7pECANQrFy5cqHBv1aZNm/Tll1+qd+/e7ikKAFDncEUKAFCvHDlyRElJSbr33nsVHR2t/fv3a/78+bLZbNqzZ4/CwsLcXSIAoA5gsQkAQL0SEhKihIQEvfnmmzp58qQCAwOVkpKi2bNnE6IAAFXGFSkAAAAAMIl7pAAAAADAJIIUAAAAAJjEPVKSysrKdPz4cTVq1EgWi8Xd5QAAAABwE8MwdPr0aUVHR8vL6/LXnQhSko4fP66YmBh3lwEAAADAQxw7dkxNmza9bD9BSlKjRo0k/fhiWa1WN1cDAAAAwF0cDodiYmKcGeFyCFKS8+N8VquVIAUAAADgirf8sNgEAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJvm4uwAAni128spa3f+R2Sm1un8AAIDawBUpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATPKYIDV79mxZLBY99thjzrbz588rLS1NYWFhCgoK0pAhQ5Sfn+/yvKNHjyolJUUBAQEKDw/X448/rosXL17l6gEAAADUJx4RpLZt26bXX39dnTp1cmmfMGGCli9friVLligrK0vHjx/X4MGDnf2lpaVKSUlRSUmJNm/erLffflsLFy7U1KlTr/YpAAAAAKhH3B6kzpw5o+HDh+uvf/2rQkJCnO1FRUX629/+pueff159+vRRQkKCFixYoM2bN2vLli2SpLVr12rfvn165513dMMNN2jgwIGaNWuW5s6dq5KSEnedEgAAAIBrnNuDVFpamlJSUpSUlOTSnpOTowsXLri0t23bVs2aNVN2drYkKTs7Wx07dlRERIRzTHJyshwOh/bu3XvZYxYXF8vhcLhsAAAAAFBVPu48+OLFi7Vjxw5t27atQl9eXp78/PwUHBzs0h4REaG8vDznmJ+GqPL+8r7LycjI0IwZM35h9QAAAADqK7ddkTp27JgeffRRvfvuu2rQoMFVPXZ6erqKioqc27Fjx67q8QEAAADUbW4LUjk5OSooKNCNN94oHx8f+fj4KCsrSy+//LJ8fHwUERGhkpISFRYWujwvPz9fkZGRkqTIyMgKq/iVPy4fUxl/f39ZrVaXDQAAAACqym1Bqm/fvtq9e7d27tzp3Lp27arhw4c7f/b19dX69eudzzlw4ICOHj0qu90uSbLb7dq9e7cKCgqcYzIzM2W1WhUfH3/VzwkAAABA/eC2e6QaNWqkDh06uLQFBgYqLCzM2f7AAw9o4sSJCg0NldVq1fjx42W323XTTTdJkvr376/4+HiNGDFCc+bMUV5enqZMmaK0tDT5+/tf9XMCAAAAUD+4dbGJK3nhhRfk5eWlIUOGqLi4WMnJyXrttdec/d7e3lqxYoXGjh0ru92uwMBApaamaubMmW6sGgAAAMC1zmIYhuHuItzN4XDIZrOpqKiI+6WAS8ROXlmr+z8yO6VW9w8AAGBGVbOB279HCgAAAADqGoIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmuTVIzZs3T506dZLVapXVapXdbteqVauc/b1795bFYnHZHn74YZd9HD16VCkpKQoICFB4eLgef/xxXbx48WqfCgAAAIB6xMedB2/atKlmz56tVq1ayTAMvf3227rzzjv1xRdfqH379pKk0aNHa+bMmc7nBAQEOH8uLS1VSkqKIiMjtXnzZp04cUL33XeffH199cwzz1z18wEAAABQP7g1SN1xxx0uj59++mnNmzdPW7ZscQapgIAARUZGVvr8tWvXat++fVq3bp0iIiJ0ww03aNasWZo0aZKmT58uPz+/Wj8HAAAAAPWPx9wjVVpaqsWLF+vs2bOy2+3O9nfffVeNGzdWhw4dlJ6ernPnzjn7srOz1bFjR0VERDjbkpOT5XA4tHfv3sseq7i4WA6Hw2UDAAAAgKpy6xUpSdq9e7fsdrvOnz+voKAgLVu2TPHx8ZKk3/3ud2revLmio6O1a9cuTZo0SQcOHNDSpUslSXl5eS4hSpLzcV5e3mWPmZGRoRkzZtTSGQEAAAC41rk9SLVp00Y7d+5UUVGR3n//faWmpiorK0vx8fEaM2aMc1zHjh0VFRWlvn37Kjc3V3FxcdU+Znp6uiZOnOh87HA4FBMT84vOAwAAAED94faP9vn5+ally5ZKSEhQRkaGOnfurJdeeqnSsYmJiZKkQ4cOSZIiIyOVn5/vMqb88eXuq5Ikf39/50qB5RsAAAAAVJXbg9SlysrKVFxcXGnfzp07JUlRUVGSJLvdrt27d6ugoMA5JjMzU1ar1fnxQAAAAACoaW79aF96eroGDhyoZs2a6fTp01q0aJE2bdqkNWvWKDc3V4sWLdLtt9+usLAw7dq1SxMmTFCvXr3UqVMnSVL//v0VHx+vESNGaM6cOcrLy9OUKVOUlpYmf39/d54aAAAAgGuYW4NUQUGB7rvvPp04cUI2m02dOnXSmjVr1K9fPx07dkzr1q3Tiy++qLNnzyomJkZDhgzRlClTnM/39vbWihUrNHbsWNntdgUGBio1NdXle6cAAAAAoKZZDMMw3F2EuzkcDtlsNhUVFXG/FHCJ2Mkra3X/R2an1Or+AQAAzKhqNvC4e6QAAAAAwNMRpAAAAADAJIIUAAAAAJjk9i/kBQBUxL1pAAB4Nq5IAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJjk4+4CANRvsZNX1tq+j8xOqbV9AwCA+o0rUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmOTWIDVv3jx16tRJVqtVVqtVdrtdq1atcvafP39eaWlpCgsLU1BQkIYMGaL8/HyXfRw9elQpKSkKCAhQeHi4Hn/8cV28ePFqnwoAAACAesStQapp06aaPXu2cnJytH37dvXp00d33nmn9u7dK0maMGGCli9friVLligrK0vHjx/X4MGDnc8vLS1VSkqKSkpKtHnzZr399ttauHChpk6d6q5TAgAAAFAPWAzDMNxdxE+Fhobqz3/+s37zm9+oSZMmWrRokX7zm99Ikvbv36927dopOztbN910k1atWqVf/epXOn78uCIiIiRJ8+fP16RJk3Ty5En5+flV6ZgOh0M2m01FRUWyWq21dm5AXRQ7eaW7S6i2I7NT3F1CtdX2616XXxsAAGpTVbOBx9wjVVpaqsWLF+vs2bOy2+3KycnRhQsXlJSU5BzTtm1bNWvWTNnZ2ZKk7OxsdezY0RmiJCk5OVkOh8N5VasyxcXFcjgcLhsAAAAAVJXbg9Tu3bsVFBQkf39/Pfzww1q2bJni4+OVl5cnPz8/BQcHu4yPiIhQXl6eJCkvL88lRJX3l/ddTkZGhmw2m3OLiYmp2ZMCAAAAcE1ze5Bq06aNdu7cqa1bt2rs2LFKTU3Vvn37avWY6enpKioqcm7Hjh2r1eMBAAAAuLb4uLsAPz8/tWzZUpKUkJCgbdu26aWXXtLdd9+tkpISFRYWulyVys/PV2RkpCQpMjJSn3/+ucv+ylf1Kx9TGX9/f/n7+9fwmQAAAACoL9x+RepSZWVlKi4uVkJCgnx9fbV+/Xpn34EDB3T06FHZ7XZJkt1u1+7du1VQUOAck5mZKavVqvj4+KteOwAAAID6wa1XpNLT0zVw4EA1a9ZMp0+f1qJFi7Rp0yatWbNGNptNDzzwgCZOnKjQ0FBZrVaNHz9edrtdN910kySpf//+io+P14gRIzRnzhzl5eVpypQpSktL44oTAAAAgFrj1iBVUFCg++67TydOnJDNZlOnTp20Zs0a9evXT5L0wgsvyMvLS0OGDFFxcbGSk5P12muvOZ/v7e2tFStWaOzYsbLb7QoMDFRqaqpmzpzprlMCAAAAUA943PdIuQPfIwVcHt8j5R58jxQAAO5R575HCgAAAADqCoIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMMmtX8gLALWJ72ICAAC1hStSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAk9wapDIyMtStWzc1atRI4eHhGjRokA4cOOAypnfv3rJYLC7bww8/7DLm6NGjSklJUUBAgMLDw/X444/r4sWLV/NUAAAAANQjPu48eFZWltLS0tStWzddvHhR//d//6f+/ftr3759CgwMdI4bPXq0Zs6c6XwcEBDg/Lm0tFQpKSmKjIzU5s2bdeLECd13333y9fXVM888c1XPBwAAAED94NYgtXr1apfHCxcuVHh4uHJyctSrVy9ne0BAgCIjIyvdx9q1a7Vv3z6tW7dOERERuuGGGzRr1ixNmjRJ06dPl5+fX62eAwAAAID6x6PukSoqKpIkhYaGurS/++67aty4sTp06KD09HSdO3fO2Zedna2OHTsqIiLC2ZacnCyHw6G9e/dWepzi4mI5HA6XDQAAAACqyq1XpH6qrKxMjz32mG655RZ16NDB2f673/1OzZs3V3R0tHbt2qVJkybpwIEDWrp0qSQpLy/PJURJcj7Oy8ur9FgZGRmaMWNGLZ0JAAAAgGudxwSptLQ07dmzR59++qlL+5gxY5w/d+zYUVFRUerbt69yc3MVFxdXrWOlp6dr4sSJzscOh0MxMTHVKxwAAABAveMRQWrcuHFasWKFPvnkEzVt2vRnxyYmJkqSDh06pLi4OEVGRurzzz93GZOfny9Jl72vyt/fX/7+/jVQOYD6LHbySneXAAAA3MSt90gZhqFx48Zp2bJl2rBhg1q0aHHF5+zcuVOSFBUVJUmy2+3avXu3CgoKnGMyMzNltVoVHx9fK3UDAAAAqN/cekUqLS1NixYt0ocffqhGjRo572my2Wxq2LChcnNztWjRIt1+++0KCwvTrl27NGHCBPXq1UudOnWSJPXv31/x8fEaMWKE5syZo7y8PE2ZMkVpaWlcdQIAAABQK9x6RWrevHkqKipS7969FRUV5dzee+89SZKfn5/WrVun/v37q23btvrDH/6gIUOGaPny5c59eHt7a8WKFfL29pbdbte9996r++67z+V7pwAAAACgJlXritT111+vbdu2KSwszKW9sLBQN954o77++usq7ccwjJ/tj4mJUVZW1hX307x5c3388cdVOiYAAAAA/FLVuiJ15MgRlZaWVmgvLi7WN99884uLAgAAAABPZuqK1EcffeT8ec2aNbLZbM7HpaWlWr9+vWJjY2usOAAAAADwRKaC1KBBgyRJFotFqampLn2+vr6KjY3VX/7ylxorDgAAAAA8kakgVVZWJklq0aKFtm3bpsaNG9dKUQAAAADgyaq12MThw4drug4AAAAAqDOq/T1S69ev1/r161VQUOC8UlXurbfe+sWFAQAAAICnqlaQmjFjhmbOnKmuXbsqKipKFoulpusCAAAAAI9VrSA1f/58LVy4UCNGjKjpegAAAADA41Xre6RKSkp0880313QtAAAAAFAnVCtIPfjgg1q0aFFN1wIAAAAAdUK1Ptp3/vx5vfHGG1q3bp06deokX19fl/7nn3++RooDAAAAAE9UrSC1a9cu3XDDDZKkPXv2uPSx8AQAAACAa121gtTGjRtrug4AAAAAqDOqdY8UAAAAANRn1boiddttt/3sR/g2bNhQ7YIAAAAAwNNVK0iV3x9V7sKFC9q5c6f27Nmj1NTUmqgLAAAAADxWtYLUCy+8UGn79OnTdebMmV9UEHAtip28slb3f2R2Sq3uHwAAAK5q9B6pe++9V2+99VZN7hIAAAAAPE6NBqns7Gw1aNCgJncJAAAAAB6nWh/tGzx4sMtjwzB04sQJbd++XU8++WSNFAYAAAAAnqpaQcpms7k89vLyUps2bTRz5kz179+/RgoDAAAAAE9VrSC1YMGCmq4DwC9Q24tZAAAAwFW1glS5nJwcffXVV5Kk9u3bq0uXLjVSFAAAAAB4smoFqYKCAg0bNkybNm1ScHCwJKmwsFC33XabFi9erCZNmtRkjQAAAADgUaq1at/48eN1+vRp7d27V6dOndKpU6e0Z88eORwOPfLIIzVdIwAAAAB4lGpdkVq9erXWrVundu3aOdvi4+M1d+5cFpsAAAAAcM2r1hWpsrIy+fr6Vmj39fVVWVnZLy4KAAAAADxZtYJUnz599Oijj+r48ePOtm+++UYTJkxQ3759a6w4AAAAAPBE1QpSr776qhwOh2JjYxUXF6e4uDi1aNFCDodDr7zySk3XCAAAAAAepVr3SMXExGjHjh1at26d9u/fL0lq166dkpKSarQ4AAAAAPBEpq5IbdiwQfHx8XI4HLJYLOrXr5/Gjx+v8ePHq1u3bmrfvr3+/e9/11atAAAAAOARTAWpF198UaNHj5bVaq3QZ7PZ9NBDD+n555+vseIAAAAAwBOZClJffvmlBgwYcNn+/v37Kycnp8r7y8jIULdu3dSoUSOFh4dr0KBBOnDggMuY8+fPKy0tTWFhYQoKCtKQIUOUn5/vMubo0aNKSUlRQECAwsPD9fjjj+vixYtmTg0AAAAAqsxUkMrPz6902fNyPj4+OnnyZJX3l5WVpbS0NG3ZskWZmZm6cOGC+vfvr7NnzzrHTJgwQcuXL9eSJUuUlZWl48ePa/Dgwc7+0tJSpaSkqKSkRJs3b9bbb7+thQsXaurUqWZODQAAAACqzNRiE9ddd5327Nmjli1bVtq/a9cuRUVFVXl/q1evdnm8cOFChYeHKycnR7169VJRUZH+9re/adGiRerTp48kacGCBWrXrp22bNmim266SWvXrtW+ffu0bt06RURE6IYbbtCsWbM0adIkTZ8+XX5+fmZOEQAAAACuyNQVqdtvv11PPvmkzp8/X6Hvhx9+0LRp0/SrX/2q2sUUFRVJkkJDQyVJOTk5unDhgstqgG3btlWzZs2UnZ0tScrOzlbHjh0VERHhHJOcnCyHw6G9e/dWepzi4mI5HA6XDQAAAACqytQVqSlTpmjp0qVq3bq1xo0bpzZt2kiS9u/fr7lz56q0tFR/+tOfqlVIWVmZHnvsMd1yyy3q0KGDJCkvL09+fn4KDg52GRsREaG8vDznmJ+GqPL+8r7KZGRkaMaMGdWqEwAAAABMBamIiAht3rxZY8eOVXp6ugzDkCRZLBYlJydr7ty5FUJNVaWlpWnPnj369NNPq/V8M9LT0zVx4kTnY4fDoZiYmFo/LgAAAIBrg+kv5G3evLk+/vhjff/99zp06JAMw1CrVq0UEhJS7SLGjRunFStW6JNPPlHTpk2d7ZGRkSopKVFhYaHLVan8/HxFRkY6x3z++ecu+ytf1a98zKX8/f3l7+9f7XoBAAAA1G+m7pH6qZCQEHXr1k3du3evdogyDEPjxo3TsmXLtGHDBrVo0cKlPyEhQb6+vlq/fr2z7cCBAzp69KjsdrskyW63a/fu3SooKHCOyczMlNVqVXx8fLXqAgAAAICfY/qKVE1KS0vTokWL9OGHH6pRo0bOe5psNpsaNmwom82mBx54QBMnTlRoaKisVqvGjx8vu92um266SdKP310VHx+vESNGaM6cOcrLy9OUKVOUlpbGVScAAAAAtcKtQWrevHmSpN69e7u0L1iwQCNHjpQkvfDCC/Ly8tKQIUNUXFys5ORkvfbaa86x3t7eWrFihcaOHSu73a7AwEClpqZq5syZV+s0AAAAANQzFqN8xYh6zOFwyGazqaioSFar1d3l4BoUO3mlu0sAXByZneLuEgAA8EhVzQbVvkcKAAAAAOorghQAAAAAmOTWe6QAM2r743F81AkAAABVxRUpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATPJxdwGAp4idvNLdJQAAAKCO4IoUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADDJrUHqk08+0R133KHo6GhZLBZ98MEHLv0jR46UxWJx2QYMGOAy5tSpUxo+fLisVquCg4P1wAMP6MyZM1fxLAAAAADUNz7uPPjZs2fVuXNn3X///Ro8eHClYwYMGKAFCxY4H/v7+7v0Dx8+XCdOnFBmZqYuXLigUaNGacyYMVq0aFGt1g4AdVns5JW1tu8js1Nqbd8AAHgKtwapgQMHauDAgT87xt/fX5GRkZX2ffXVV1q9erW2bdumrl27SpJeeeUV3X777XruuecUHR1d4zUDAAAAgMffI7Vp0yaFh4erTZs2Gjt2rL777jtnX3Z2toKDg50hSpKSkpLk5eWlrVu3XnafxcXFcjgcLhsAAAAAVJVHB6kBAwbo73//u9avX69nn31WWVlZGjhwoEpLSyVJeXl5Cg8Pd3mOj4+PQkNDlZeXd9n9ZmRkyGazObeYmJhaPQ8AAAAA1xa3frTvSoYNG+b8uWPHjurUqZPi4uK0adMm9e3bt9r7TU9P18SJE52PHQ4HYQoAAABAlXn0FalLXX/99WrcuLEOHTokSYqMjFRBQYHLmIsXL+rUqVOXva9K+vG+K6vV6rIBAAAAQFXVqSD1v//9T999952ioqIkSXa7XYWFhcrJyXGO2bBhg8rKypSYmOiuMgEAAABc49z60b4zZ844ry5J0uHDh7Vz506FhoYqNDRUM2bM0JAhQxQZGanc3Fw98cQTatmypZKTkyVJ7dq104ABAzR69GjNnz9fFy5c0Lhx4zRs2DBW7AMAAABQa9x6RWr79u3q0qWLunTpIkmaOHGiunTpoqlTp8rb21u7du3Sr3/9a7Vu3VoPPPCAEhIS9O9//9vlu6TeffddtW3bVn379tXtt9+uHj166I033nDXKQEAAACoB9x6Rap3794yDOOy/WvWrLniPkJDQ/nyXQAAAABXVZ26RwoAAAAAPAFBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYJKPuwsAAFxbYievrNX9H5mdUqv7BwCgKrgiBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgEkEKAAAAAExya5D65JNPdMcddyg6OloWi0UffPCBS79hGJo6daqioqLUsGFDJSUl6eDBgy5jTp06peHDh8tqtSo4OFgPPPCAzpw5cxXPAgAAAEB949YgdfbsWXXu3Flz586ttH/OnDl6+eWXNX/+fG3dulWBgYFKTk7W+fPnnWOGDx+uvXv3KjMzUytWrNAnn3yiMWPGXK1TAAAAAFAPufULeQcOHKiBAwdW2mcYhl588UVNmTJFd955pyTp73//uyIiIvTBBx9o2LBh+uqrr7R69Wpt27ZNXbt2lSS98soruv322/Xcc88pOjr6qp0LAAAAgPrDY++ROnz4sPLy8pSUlORss9lsSkxMVHZ2tiQpOztbwcHBzhAlSUlJSfLy8tLWrVsvu+/i4mI5HA6XDQAAAACqymODVF5eniQpIiLCpT0iIsLZl5eXp/DwcJd+Hx8fhYaGOsdUJiMjQzabzbnFxMTUcPUAAAAArmUeG6RqU3p6uoqKipzbsWPH3F0SAAAAgDrEY4NUZGSkJCk/P9+lPT8/39kXGRmpgoICl/6LFy/q1KlTzjGV8ff3l9VqddkAAAAAoKo8Nki1aNFCkZGRWr9+vbPN4XBo69atstvtkiS73a7CwkLl5OQ4x2zYsEFlZWVKTEy86jUDAAAAqB/cumrfmTNndOjQIefjw4cPa+fOnQoNDVWzZs302GOP6amnnlKrVq3UokULPfnkk4qOjtagQYMkSe3atdOAAQM0evRozZ8/XxcuXNC4ceM0bNgwVuwDAAAAUGvcGqS2b9+u2267zfl44sSJkqTU1FQtXLhQTzzxhM6ePasxY8aosLBQPXr00OrVq9WgQQPnc959912NGzdOffv2lZeXl4YMGaKXX375qp8LAAAAgPrDYhiG4e4i3M3hcMhms6moqIj7pTxY7OSV7i4BgAc4MjvF3SUAAK5hVc0GHnuPFAAAAAB4KoIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCQfdxcAAICniJ28slb3f2R2Sq3uHwBw9RCkAAB1Sm2HHQAAqoKP9gEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACY5OPuAn7O9OnTNWPGDJe2Nm3aaP/+/ZKk8+fP6w9/+IMWL16s4uJiJScn67XXXlNERIQ7yoWk2Mkr3V0CAAAAUOs8/opU+/btdeLECef26aefOvsmTJig5cuXa8mSJcrKytLx48c1ePBgN1YLAAAAoD7w6CtSkuTj46PIyMgK7UVFRfrb3/6mRYsWqU+fPpKkBQsWqF27dtqyZYtuuummq10qAAAAgHrC469IHTx4UNHR0br++us1fPhwHT16VJKUk5OjCxcuKCkpyTm2bdu2atasmbKzs392n8XFxXI4HC4bAAAAAFSVRwepxMRELVy4UKtXr9a8efN0+PBh9ezZU6dPn1ZeXp78/PwUHBzs8pyIiAjl5eX97H4zMjJks9mcW0xMTC2eBQAAAIBrjUd/tG/gwIHOnzt16qTExEQ1b95c//znP9WwYcNq7zc9PV0TJ050PnY4HIQpAAAAAFXm0VekLhUcHKzWrVvr0KFDioyMVElJiQoLC13G5OfnV3pP1U/5+/vLarW6bAAAAABQVXUqSJ05c0a5ubmKiopSQkKCfH19tX79emf/gQMHdPToUdntdjdWCQAAAOBa59Ef7fvjH/+oO+64Q82bN9fx48c1bdo0eXt765577pHNZtMDDzygiRMnKjQ0VFarVePHj5fdbmfFPgAAAAC1yqOD1P/+9z/dc889+u6779SkSRP16NFDW7ZsUZMmTSRJL7zwgry8vDRkyBCXL+QFAAAAgNpkMQzDcHcR7uZwOGSz2VRUVMT9Ur9Q7OSV7i4BADzWkdkp7i4BAHAFVc0GdeoeKQAAAADwBAQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMIkgBAAAAgEkEKQAAAAAwiSAFAAAAACYRpAAAAADAJB93FwAAQH0RO3llre37yOyUWts3AKAirkgBAAAAgElckapnavNfQwEAAID6gitSAAAAAGASQQoAAAAATCJIAQAAAIBJBCkAAAAAMIkgBQAAAAAmEaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUgAAAABgko+7CwAAAL9c7OSVtbr/I7NTanX/AFDXEKQAAMAV1WZQI6QBqIv4aB8AAAAAmESQAgAAAACTCFIAAAAAYBJBCgAAAABMYrEJAADgVqw4CKAu4ooUAAAAAJjEFSkPVNv/MgcAQH3C0u0AasM1c0Vq7ty5io2NVYMGDZSYmKjPP//c3SUBAAAAuEZdE0Hqvffe08SJEzVt2jTt2LFDnTt3VnJysgoKCtxdGgAAAIBr0DURpJ5//nmNHj1ao0aNUnx8vObPn6+AgAC99dZb7i4NAAAAwDWozt8jVVJSopycHKWnpzvbvLy8lJSUpOzs7EqfU1xcrOLiYufjoqIiSZLD4ajdYquorPicu0sAAABVUNvvHTpMW1Or+69Ne2Yk19q+eV3cozZfd096Xcr/rg3D+NlxdT5IffvttyotLVVERIRLe0REhPbv31/pczIyMjRjxowK7TExMbVSIwAAuDbZXnR3BZ6L16ZyvC6V88TX5fTp07LZbJftr/NBqjrS09M1ceJE5+OysjKdOnVKYWFhslgsVd6Pw+FQTEyMjh07JqvVWhul4hdgfjwXc+PZmB/Pxvx4LubGszE/nsvT5sYwDJ0+fVrR0dE/O67OB6nGjRvL29tb+fn5Lu35+fmKjIys9Dn+/v7y9/d3aQsODq52DVar1SMmHZVjfjwXc+PZmB/Pxvx4LubGszE/nsuT5ubnrkSVq/OLTfj5+SkhIUHr1693tpWVlWn9+vWy2+1urAwAAADAtarOX5GSpIkTJyo1NVVdu3ZV9+7d9eKLL+rs2bMaNWqUu0sDAAAAcA26JoLU3XffrZMnT2rq1KnKy8vTDTfcoNWrV1dYgKKm+fv7a9q0aRU+JgjPwPx4LubGszE/no358VzMjWdjfjxXXZ0bi3Gldf0AAAAAAC7q/D1SAAAAAHC1EaQAAAAAwCSCFAAAAACYRJACAAAAAJMIUr/A3LlzFRsbqwYNGigxMVGff/65u0uqlz755BPdcccdio6OlsVi0QcffODSbxiGpk6dqqioKDVs2FBJSUk6ePCge4qtZzIyMtStWzc1atRI4eHhGjRokA4cOOAy5vz580pLS1NYWJiCgoI0ZMiQCl+wjZo3b948derUyfnlh3a7XatWrXL2My+eZfbs2bJYLHrsscecbcyR+0yfPl0Wi8Vla9u2rbOfuXGvb775Rvfee6/CwsLUsGFDdezYUdu3b3f2877AfWJjYyv87VgsFqWlpUmqe387BKlqeu+99zRx4kRNmzZNO3bsUOfOnZWcnKyCggJ3l1bvnD17Vp07d9bcuXMr7Z8zZ45efvllzZ8/X1u3blVgYKCSk5N1/vz5q1xp/ZOVlaW0tDRt2bJFmZmZunDhgvr376+zZ886x0yYMEHLly/XkiVLlJWVpePHj2vw4MFurLp+aNq0qWbPnq2cnBxt375dffr00Z133qm9e/dKYl48ybZt2/T666+rU6dOLu3MkXu1b99eJ06ccG6ffvqps4+5cZ/vv/9et9xyi3x9fbVq1Srt27dPf/nLXxQSEuIcw/sC99m2bZvL301mZqYk6be//a2kOvi3Y6BaunfvbqSlpTkfl5aWGtHR0UZGRoYbq4IkY9myZc7HZWVlRmRkpPHnP//Z2VZYWGj4+/sb/+///T83VFi/FRQUGJKMrKwswzB+nAtfX19jyZIlzjFfffWVIcnIzs52V5n1VkhIiPHmm28yLx7k9OnTRqtWrYzMzEzj1ltvNR599FHDMPjbcbdp06YZnTt3rrSPuXGvSZMmGT169LhsP+8LPMujjz5qxMXFGWVlZXXyb4crUtVQUlKinJwcJSUlOdu8vLyUlJSk7OxsN1aGSx0+fFh5eXkuc2Wz2ZSYmMhcuUFRUZEkKTQ0VJKUk5OjCxcuuMxP27Zt1axZM+bnKiotLdXixYt19uxZ2e125sWDpKWlKSUlxWUuJP52PMHBgwcVHR2t66+/XsOHD9fRo0clMTfu9tFHH6lr16767W9/q/DwcHXp0kV//etfnf28L/AcJSUleuedd3T//ffLYrHUyb8dglQ1fPvttyotLVVERIRLe0REhPLy8txUFSpTPh/MlfuVlZXpscce0y233KIOHTpI+nF+/Pz8FBwc7DKW+bk6du/eraCgIPn7++vhhx/WsmXLFB8fz7x4iMWLF2vHjh3KyMio0MccuVdiYqIWLlyo1atXa968eTp8+LB69uyp06dPMzdu9vXXX2vevHlq1aqV1qxZo7Fjx+qRRx7R22+/LYn3BZ7kgw8+UGFhoUaOHCmpbv53zcfdBQCoH9LS0rRnzx6X+wjgXm3atNHOnTtVVFSk999/X6mpqcrKynJ3WZB07NgxPfroo8rMzFSDBg3cXQ4uMXDgQOfPnTp1UmJiopo3b65//vOfatiwoRsrQ1lZmbp27apnnnlGktSlSxft2bNH8+fPV2pqqpurw0/97W9/08CBAxUdHe3uUqqNK1LV0LhxY3l7e1dYRSQ/P1+RkZFuqgqVKZ8P5sq9xo0bpxUrVmjjxo1q2rSpsz0yMlIlJSUqLCx0Gc/8XB1+fn5q2bKlEhISlJGRoc6dO+ull15iXjxATk6OCgoKdOONN8rHx0c+Pj7KysrSyy+/LB8fH0VERDBHHiQ4OFitW7fWoUOH+Ptxs6ioKMXHx7u0tWvXzvnRS94XeIb//ve/WrdunR588EFnW1382yFIVYOfn58SEhK0fv16Z1tZWZnWr18vu93uxspwqRYtWigyMtJlrhwOh7Zu3cpcXQWGYWjcuHFatmyZNmzYoBYtWrj0JyQkyNfX12V+Dhw4oKNHjzI/blBWVqbi4mLmxQP07dtXu3fv1s6dO51b165dNXz4cOfPzJHnOHPmjHJzcxUVFcXfj5vdcsstFb5m4z//+Y+aN28uifcFnmLBggUKDw9XSkqKs61O/u24e7WLumrx4sWGv7+/sXDhQmPfvn3GmDFjjODgYCMvL8/dpdU7p0+fNr744gvjiy++MCQZzz//vPHFF18Y//3vfw3DMIzZs2cbwcHBxocffmjs2rXLuPPOO40WLVoYP/zwg5srv/aNHTvWsNlsxqZNm4wTJ044t3PnzjnHPPzww0azZs2MDRs2GNu3bzfsdrtht9vdWHX9MHnyZCMrK8s4fPiwsWvXLmPy5MmGxWIx1q5daxgG8+KJfrpqn2EwR+70hz/8wdi0aZNx+PBh47PPPjOSkpKMxo0bGwUFBYZhMDfu9Pnnnxs+Pj7G008/bRw8eNB49913jYCAAOOdd95xjuF9gXuVlpYazZo1MyZNmlShr6797RCkfoFXXnnFaNasmeHn52d0797d2LJli7tLqpc2btxoSKqwpaamGobx41KnTz75pBEREWH4+/sbffv2NQ4cOODeouuJyuZFkrFgwQLnmB9++MH4/e9/b4SEhBgBAQHGXXfdZZw4ccJ9RdcT999/v9G8eXPDz8/PaNKkidG3b19niDIM5sUTXRqkmCP3ufvuu42oqCjDz8/PuO6664y7777bOHTokLOfuXGv5cuXGx06dDD8/f2Ntm3bGm+88YZLP+8L3GvNmjWGpEpf87r2t2MxDMNwy6UwAAAAAKijuEcKAAAAAEwiSAEAAACASQQpAAAAADCJIAUAAAAAJhGkAAAAAMAkghQAAAAAmESQAgAAAACTCFIAAAAAYBJBCgCAGjJy5EgNGjTI3WUAAK4CghQAoM5xd2A5cuSILBaLdu7c6bYaAADuRZACAAAAAJMIUgCAa8qePXs0cOBABQUFKSIiQiNGjNC3337r7O/du7ceeeQRPfHEEwoNDVVkZKSmT5/uso/9+/erR48eatCggeLj47Vu3TpZLBZ98MEHkqQWLVpIkrp06SKLxaLevXu7PP+5555TVFSUwsLClJaWpgsXLtTmKQMA3IAgBQC4ZhQWFqpPnz7q0qWLtm/frtWrVys/P19Dhw51Gff2228rMDBQW7du1Zw5czRz5kxlZmZKkkpLSzVo0CAFBARo69ateuONN/SnP/3J5fmff/65JGndunU6ceKEli5d6uzbuHGjcnNztXHjRr399ttauHChFi5cWLsnDgC46nzcXQAAADXl1VdfVZcuXfTMM88429566y3FxMToP//5j1q3bi1J6tSpk6ZNmyZJatWqlV599VWtX79e/fr1U2ZmpnJzc7Vp0yZFRkZKkp5++mn169fPuc8mTZpIksLCwpxjyoWEhOjVV1+Vt7e32rZtq5SUFK1fv16jR4+u1XMHAFxdBCkAwDXjyy+/1MaNGxUUFFShLzc31yVI/VRUVJQKCgokSQcOHFBMTIxLQOrevXuVa2jfvr28vb1d9r17925T5wEA8HwEKQDANePMmTO644479Oyzz1boi4qKcv7s6+vr0mexWFRWVlYjNdTmvgEAnoMgBQC4Ztx4443617/+pdjYWPn4VO9/cW3atNGxY8eUn5+viIgISdK2bdtcxvj5+Un68X4qAED9xGITAIA6qaioSDt37nTZxowZo1OnTumee+7Rtm3blJubqzVr1mjUqFFVDj39+vVTXFycUlNTtWvXLn322WeaMmWKpB+vLklSeHi4GjZs6FzMoqioqNbOEwDgmQhSAIA6adOmTerSpYvLNmvWLH322WcqLS1V//791bFjRz322GMKDg6Wl1fV/pfn7e2tDz74QGfOnFG3bt304IMPOlfta9CggSTJx8dHL7/8sl5//XVFR0frzjvvrLXzBAB4JothGIa7iwAAwJN99tln6tGjhw4dOqS4uDh3lwMA8AAEKQAALrFs2TIFBQWpVatWOnTokB599FGFhITo008/dXdpAAAPwWITAABc4vTp05o0aZKOHj2qxo0bKykpSX/5y1/cXRYAwINwRQoAAAAATGKxCQAAAAAwiSAFAAAAACYRpAAAAADAJIIUAAAAAJhEkAIAAAAAkwhSAAAAAGASQQoAAAAATCJIAQAAAIBJ/x80Qz/r/QUp9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Starting data preprocessing and EDA...\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('GMB_dataset.txt', delimiter='\\t', encoding='latin1')\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# =============================================\n",
    "# Exploratory Data Analysis (EDA)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nPerforming Exploratory Data Analysis...\")\n",
    "\n",
    "# 1. Basic dataset statistics\n",
    "print(\"\\n1. Basic dataset statistics:\")\n",
    "print(f\"- Total rows: {len(data)}\")\n",
    "print(f\"- Columns: {list(data.columns)}\")\n",
    "print(\"- Sample data:\")\n",
    "print(data.head())\n",
    "\n",
    "# 2. Check for missing values\n",
    "print(\"\\n2. Missing values analysis:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Fill NaN values in the 'Sentence #' column\n",
    "data['Sentence #'] = data['Sentence #'].ffill()\n",
    "print(\"\\nFilled NaN values in 'Sentence #' column.\")\n",
    "\n",
    "# 3. Tag distribution analysis\n",
    "print(\"\\n3. Tag distribution:\")\n",
    "tag_counts = data['Tag'].value_counts()\n",
    "print(tag_counts)\n",
    "\n",
    "# 4. Sentence length analysis\n",
    "sent_grouped = data.groupby('Sentence #')\n",
    "sentence_lengths = sent_grouped.size()\n",
    "print(\"\\n4. Sentence length statistics:\")\n",
    "print(f\"- Average length: {sentence_lengths.mean():.2f}\")\n",
    "print(f\"- Min length: {sentence_lengths.min()}\")\n",
    "print(f\"- Max length: {sentence_lengths.max()}\")\n",
    "print(f\"- 90th percentile: {sentence_lengths.quantile(0.9)}\")\n",
    "\n",
    "# 5. Example sentences\n",
    "print(\"\\n5. Example sentences:\")\n",
    "for i, (name, group) in enumerate(sent_grouped):\n",
    "    if i >= 2:  # Show first 2 sentences only\n",
    "        break\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(\"Words:\", \" \".join(group['Word'].tolist()))\n",
    "    print(\"Tags:\", \" \".join(group['Tag'].tolist()))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Tag distribution plot\n",
    "plt.figure(figsize=(12,6))\n",
    "tag_counts.plot(kind='bar')\n",
    "plt.title('Tag Distribution')\n",
    "plt.xlabel('Tag')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Sentence length distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(sentence_lengths, bins=30)\n",
    "plt.title('Sentence Length Distribution')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5087cb",
   "metadata": {
    "papermill": {
     "duration": 0.003761,
     "end_time": "2025-03-25T11:01:28.351017",
     "exception": false,
     "start_time": "2025-03-25T11:01:28.347256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Split data train and test (The last 10% sentence is used for test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f4dd7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T11:01:28.359331Z",
     "iopub.status.busy": "2025-03-25T11:01:28.359089Z",
     "iopub.status.idle": "2025-03-25T11:01:28.672668Z",
     "shell.execute_reply": "2025-03-25T11:01:28.671565Z"
    },
    "papermill": {
     "duration": 0.319335,
     "end_time": "2025-03-25T11:01:28.674080",
     "exception": false,
     "start_time": "2025-03-25T11:01:28.354745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for model with train-only vocab and last 10% test split...\n",
      "Split data into training (2700 sentences) and test (299 sentences) sets - last 10% as test.\n",
      "Extracted unique words from TRAIN ONLY and all tags from FULL dataset.\n",
      "Created word-to-index and tag-to-index mappings.\n",
      "Converted words and tags to indices (with UNK handling for test set).\n",
      "Added padding token to tag2idx.\n",
      "Padded sequences.\n",
      "Converted data to tensors.\n",
      "Separated word indices and tag indices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-9a1b570c24ee>:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sentences = data.groupby('Sentence #').apply(\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Data Preparation for Model (Modified Version)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nPreparing data for model with train-only vocab and last 10% test split...\")\n",
    "\n",
    "# Group by sentence to create list of (word, tag) pairs\n",
    "sentences = data.groupby('Sentence #').apply(\n",
    "    lambda x: list(zip(x['Word'], x['Tag']))\n",
    ").tolist()\n",
    "\n",
    "# Split data - last 10% as test set\n",
    "total_sentences = len(sentences)\n",
    "test_size = int(0.1 * total_sentences)\n",
    "train_sentences = sentences[:-test_size]\n",
    "test_sentences = sentences[-test_size:]\n",
    "print(f\"Split data into training ({len(train_sentences)} sentences) and test ({len(test_sentences)} sentences) sets - last 10% as test.\")\n",
    "\n",
    "# Get unique words from TRAIN SET ONLY\n",
    "words = list(set(word for sentence in train_sentences for word, tag in sentence))\n",
    "words.append('PAD')  # Add padding token\n",
    "words.append('UNK')  # Add unknown token\n",
    "\n",
    "# Get ALL unique tags (from both train and test)\n",
    "tags = list(set(tag for sentence in sentences for word, tag in sentence))\n",
    "print(\"Extracted unique words from TRAIN ONLY and all tags from FULL dataset.\")\n",
    "\n",
    "# Create mappings\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "print(\"Created word-to-index and tag-to-index mappings.\")\n",
    "\n",
    "# Convert sentences to indices (with UNK handling for test set)\n",
    "def convert_sentence(sentence, is_train=True):\n",
    "    if is_train:\n",
    "        return [(word2idx[word], tag2idx[tag]) for word, tag in sentence]\n",
    "    else:\n",
    "        return [(word2idx.get(word, word2idx['UNK']), tag2idx[tag]) for word, tag in sentence]\n",
    "\n",
    "train_sentences = [convert_sentence(s, is_train=True) for s in train_sentences]\n",
    "test_sentences = [convert_sentence(s, is_train=False) for s in test_sentences]\n",
    "print(\"Converted words and tags to indices (with UNK handling for test set).\")\n",
    "\n",
    "# Padding sequences\n",
    "def pad_sequences(sequences, pad_token, pad_tag):\n",
    "    max_len = max(len(s) for s in sequences)\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padded_seq = seq + [(pad_token, pad_tag)] * (max_len - len(seq))\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return padded_sequences\n",
    "\n",
    "# Add padding token to tag2idx\n",
    "tag2idx['PAD'] = len(tag2idx)\n",
    "pad_tag_idx = tag2idx['PAD']\n",
    "print(\"Added padding token to tag2idx.\")\n",
    "\n",
    "# Pad sequences\n",
    "train_sentences = pad_sequences(train_sentences, word2idx['PAD'], pad_tag_idx)\n",
    "test_sentences = pad_sequences(test_sentences, word2idx['PAD'], pad_tag_idx)\n",
    "print(\"Padded sequences.\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_sentences = torch.tensor(train_sentences, dtype=torch.long)\n",
    "test_sentences = torch.tensor(test_sentences, dtype=torch.long)\n",
    "print(\"Converted data to tensors.\")\n",
    "\n",
    "# Separate word indices and tag indices\n",
    "train_words = train_sentences[:, :, 0]  # Word indices\n",
    "train_tags = train_sentences[:, :, 1]   # Tag indices\n",
    "test_words = test_sentences[:, :, 0]    # Word indices\n",
    "test_tags = test_sentences[:, :, 1]     # Tag indices\n",
    "print(\"Separated word indices and tag indices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34885da4",
   "metadata": {
    "papermill": {
     "duration": 0.00374,
     "end_time": "2025-03-25T11:01:28.681968",
     "exception": false,
     "start_time": "2025-03-25T11:01:28.678228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796ef9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T11:01:28.690363Z",
     "iopub.status.busy": "2025-03-25T11:01:28.690132Z",
     "iopub.status.idle": "2025-03-25T11:01:28.701119Z",
     "shell.execute_reply": "2025-03-25T11:01:28.700326Z"
    },
    "papermill": {
     "duration": 0.016495,
     "end_time": "2025-03-25T11:01:28.702297",
     "exception": false,
     "start_time": "2025-03-25T11:01:28.685802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining BiLSTM-CRF model...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the BiLSTM-CRF Model\n",
    "\n",
    "print(\"Defining BiLSTM-CRF model...\")\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.crf = CRF(tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(x), 1, -1))\n",
    "        lstm_out = lstm_out.view(len(x), -1)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        return tag_space\n",
    "\n",
    "# CRF Layer\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags):\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))  # Transition scores\n",
    "\n",
    "    def forward(self, feats, tags):\n",
    "        \"\"\"\n",
    "        Compute the negative log-likelihood loss for a sequence.\n",
    "        \"\"\"\n",
    "        seq_len = feats.shape[0]\n",
    "        score = self._compute_score(feats, tags)  # Score of the gold path\n",
    "        log_z = self._compute_log_partition(feats)  # Log partition function\n",
    "        loss = log_z - score  # Negative log-likelihood\n",
    "        return loss\n",
    "\n",
    "    def _compute_score(self, feats, tags):\n",
    "        \"\"\"\n",
    "        Compute the score of the gold path.\n",
    "        \"\"\"\n",
    "        score = torch.zeros(1)\n",
    "        score += feats[0, tags[0]]  # Start with the first tag\n",
    "        for i in range(1, len(tags)):\n",
    "            score += feats[i, tags[i]] + self.transitions[tags[i - 1], tags[i]]  # Emission + Transition\n",
    "        return score\n",
    "\n",
    "    def _compute_log_partition(self, feats):\n",
    "        \"\"\"\n",
    "        Compute the log partition function using the forward algorithm.\n",
    "        \"\"\"\n",
    "        seq_len, num_tags = feats.shape\n",
    "        alpha = torch.zeros(num_tags)\n",
    "        alpha = feats[0]  # Initialize with the first emission scores\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            alpha_next = torch.zeros(num_tags)\n",
    "            for i in range(num_tags):\n",
    "                emit_score = feats[t, i]\n",
    "                trans_score = self.transitions[:, i]\n",
    "                alpha_next[i] = torch.logsumexp(alpha + trans_score, dim=0) + emit_score\n",
    "            alpha = alpha_next\n",
    "\n",
    "        log_z = torch.logsumexp(alpha, dim=0)\n",
    "        return log_z\n",
    "\n",
    "    def viterbi_decode(self, feats):\n",
    "        \"\"\"\n",
    "        Decode the most likely sequence of tags using the Viterbi algorithm.\n",
    "        \"\"\"\n",
    "        seq_len, num_tags = feats.shape\n",
    "        backpointers = torch.zeros(seq_len, num_tags, dtype=torch.long)\n",
    "\n",
    "        # Initialize the viterbi variables\n",
    "        viterbi = feats[0]\n",
    "\n",
    "        for t in range(1, seq_len):\n",
    "            viterbi_next = torch.zeros(num_tags)\n",
    "            for i in range(num_tags):\n",
    "                trans_score = self.transitions[:, i]\n",
    "                max_score, max_idx = torch.max(viterbi + trans_score, dim=0)\n",
    "                viterbi_next[i] = max_score + feats[t, i]\n",
    "                backpointers[t, i] = max_idx\n",
    "            viterbi = viterbi_next\n",
    "\n",
    "        # Backtrack to find the best path\n",
    "        best_score, best_last_tag = torch.max(viterbi, dim=0)\n",
    "        best_path = [best_last_tag.item()]\n",
    "        for t in reversed(range(1, seq_len)):\n",
    "            best_last_tag = backpointers[t, best_last_tag]\n",
    "            best_path.insert(0, best_last_tag.item())\n",
    "\n",
    "        return best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb347aaf",
   "metadata": {
    "papermill": {
     "duration": 0.00373,
     "end_time": "2025-03-25T11:01:28.709988",
     "exception": false,
     "start_time": "2025-03-25T11:01:28.706258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad92e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T11:01:28.718460Z",
     "iopub.status.busy": "2025-03-25T11:01:28.718244Z",
     "iopub.status.idle": "2025-03-25T12:45:26.369417Z",
     "shell.execute_reply": "2025-03-25T12:45:26.368497Z"
    },
    "papermill": {
     "duration": 6237.65707,
     "end_time": "2025-03-25T12:45:26.370882",
     "exception": false,
     "start_time": "2025-03-25T11:01:28.713812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized BiLSTM-CRF model.\n",
      "Initialized optimizer.\n",
      "Starting model training...\n",
      "Epoch 1, Batch 10/2700 (0.37%), Loss: 25.8842\n",
      "Epoch 1, Batch 20/2700 (0.74%), Loss: 11.6242\n",
      "Epoch 1, Batch 30/2700 (1.11%), Loss: 12.8440\n",
      "Epoch 1, Batch 40/2700 (1.48%), Loss: 15.9344\n",
      "Epoch 1, Batch 50/2700 (1.85%), Loss: 19.7587\n",
      "Epoch 1, Batch 60/2700 (2.22%), Loss: 6.7919\n",
      "Epoch 1, Batch 70/2700 (2.59%), Loss: 12.9040\n",
      "Epoch 1, Batch 80/2700 (2.96%), Loss: 12.1965\n",
      "Epoch 1, Batch 90/2700 (3.33%), Loss: 12.4003\n",
      "Epoch 1, Batch 100/2700 (3.70%), Loss: 31.6235\n",
      "Epoch 1, Batch 110/2700 (4.07%), Loss: 10.7212\n",
      "Epoch 1, Batch 120/2700 (4.44%), Loss: 12.5893\n",
      "Epoch 1, Batch 130/2700 (4.81%), Loss: 11.2956\n",
      "Epoch 1, Batch 140/2700 (5.19%), Loss: 14.8971\n",
      "Epoch 1, Batch 150/2700 (5.56%), Loss: 0.9453\n",
      "Epoch 1, Batch 160/2700 (5.93%), Loss: 18.5746\n",
      "Epoch 1, Batch 170/2700 (6.30%), Loss: 9.2883\n",
      "Epoch 1, Batch 180/2700 (6.67%), Loss: 12.5076\n",
      "Epoch 1, Batch 190/2700 (7.04%), Loss: 23.7749\n",
      "Epoch 1, Batch 200/2700 (7.41%), Loss: 3.9750\n",
      "Epoch 1, Batch 210/2700 (7.78%), Loss: 9.1807\n",
      "Epoch 1, Batch 220/2700 (8.15%), Loss: 11.8857\n",
      "Epoch 1, Batch 230/2700 (8.52%), Loss: 15.2504\n",
      "Epoch 1, Batch 240/2700 (8.89%), Loss: 14.5008\n",
      "Epoch 1, Batch 250/2700 (9.26%), Loss: 3.6552\n",
      "Epoch 1, Batch 260/2700 (9.63%), Loss: 22.3679\n",
      "Epoch 1, Batch 270/2700 (10.00%), Loss: 68.8206\n",
      "Epoch 1, Batch 280/2700 (10.37%), Loss: 3.2078\n",
      "Epoch 1, Batch 290/2700 (10.74%), Loss: 30.5338\n",
      "Epoch 1, Batch 300/2700 (11.11%), Loss: 10.8169\n",
      "Epoch 1, Batch 310/2700 (11.48%), Loss: 38.7551\n",
      "Epoch 1, Batch 320/2700 (11.85%), Loss: 20.8970\n",
      "Epoch 1, Batch 330/2700 (12.22%), Loss: 20.4593\n",
      "Epoch 1, Batch 340/2700 (12.59%), Loss: 34.8765\n",
      "Epoch 1, Batch 350/2700 (12.96%), Loss: 4.7042\n",
      "Epoch 1, Batch 360/2700 (13.33%), Loss: 4.3687\n",
      "Epoch 1, Batch 370/2700 (13.70%), Loss: 11.1835\n",
      "Epoch 1, Batch 380/2700 (14.07%), Loss: 27.4334\n",
      "Epoch 1, Batch 390/2700 (14.44%), Loss: 3.0571\n",
      "Epoch 1, Batch 400/2700 (14.81%), Loss: 6.2527\n",
      "Epoch 1, Batch 410/2700 (15.19%), Loss: 37.5274\n",
      "Epoch 1, Batch 420/2700 (15.56%), Loss: 1.6608\n",
      "Epoch 1, Batch 430/2700 (15.93%), Loss: 31.1577\n",
      "Epoch 1, Batch 440/2700 (16.30%), Loss: 26.3989\n",
      "Epoch 1, Batch 450/2700 (16.67%), Loss: 21.9678\n",
      "Epoch 1, Batch 460/2700 (17.04%), Loss: 9.3673\n",
      "Epoch 1, Batch 470/2700 (17.41%), Loss: 1.9261\n",
      "Epoch 1, Batch 480/2700 (17.78%), Loss: 11.7832\n",
      "Epoch 1, Batch 490/2700 (18.15%), Loss: 4.5389\n",
      "Epoch 1, Batch 500/2700 (18.52%), Loss: 1.2564\n",
      "Epoch 1, Batch 510/2700 (18.89%), Loss: 23.1287\n",
      "Epoch 1, Batch 520/2700 (19.26%), Loss: 13.8234\n",
      "Epoch 1, Batch 530/2700 (19.63%), Loss: 11.0989\n",
      "Epoch 1, Batch 540/2700 (20.00%), Loss: 3.0369\n",
      "Epoch 1, Batch 550/2700 (20.37%), Loss: 4.8442\n",
      "Epoch 1, Batch 560/2700 (20.74%), Loss: 4.7979\n",
      "Epoch 1, Batch 570/2700 (21.11%), Loss: 12.8809\n",
      "Epoch 1, Batch 580/2700 (21.48%), Loss: 13.3309\n",
      "Epoch 1, Batch 590/2700 (21.85%), Loss: 16.6291\n",
      "Epoch 1, Batch 600/2700 (22.22%), Loss: 4.6826\n",
      "Epoch 1, Batch 610/2700 (22.59%), Loss: 1.1588\n",
      "Epoch 1, Batch 620/2700 (22.96%), Loss: 6.5161\n",
      "Epoch 1, Batch 630/2700 (23.33%), Loss: 8.3183\n",
      "Epoch 1, Batch 640/2700 (23.70%), Loss: 9.3173\n",
      "Epoch 1, Batch 650/2700 (24.07%), Loss: 13.5106\n",
      "Epoch 1, Batch 660/2700 (24.44%), Loss: 20.9823\n",
      "Epoch 1, Batch 670/2700 (24.81%), Loss: 1.6517\n",
      "Epoch 1, Batch 680/2700 (25.19%), Loss: 0.8230\n",
      "Epoch 1, Batch 690/2700 (25.56%), Loss: 10.2462\n",
      "Epoch 1, Batch 700/2700 (25.93%), Loss: 0.7230\n",
      "Epoch 1, Batch 710/2700 (26.30%), Loss: 13.8566\n",
      "Epoch 1, Batch 720/2700 (26.67%), Loss: 12.8028\n",
      "Epoch 1, Batch 730/2700 (27.04%), Loss: 23.3665\n",
      "Epoch 1, Batch 740/2700 (27.41%), Loss: 3.0760\n",
      "Epoch 1, Batch 750/2700 (27.78%), Loss: 24.9894\n",
      "Epoch 1, Batch 760/2700 (28.15%), Loss: 18.1412\n",
      "Epoch 1, Batch 770/2700 (28.52%), Loss: 19.3293\n",
      "Epoch 1, Batch 780/2700 (28.89%), Loss: 8.4810\n",
      "Epoch 1, Batch 790/2700 (29.26%), Loss: 41.7704\n",
      "Epoch 1, Batch 800/2700 (29.63%), Loss: 17.4264\n",
      "Epoch 1, Batch 810/2700 (30.00%), Loss: 6.0516\n",
      "Epoch 1, Batch 820/2700 (30.37%), Loss: 17.4330\n",
      "Epoch 1, Batch 830/2700 (30.74%), Loss: 10.7386\n",
      "Epoch 1, Batch 840/2700 (31.11%), Loss: 0.7872\n",
      "Epoch 1, Batch 850/2700 (31.48%), Loss: 17.4940\n",
      "Epoch 1, Batch 860/2700 (31.85%), Loss: 9.2277\n",
      "Epoch 1, Batch 870/2700 (32.22%), Loss: 28.4508\n",
      "Epoch 1, Batch 880/2700 (32.59%), Loss: 19.2048\n",
      "Epoch 1, Batch 890/2700 (32.96%), Loss: 15.4503\n",
      "Epoch 1, Batch 900/2700 (33.33%), Loss: 8.2186\n",
      "Epoch 1, Batch 910/2700 (33.70%), Loss: 2.7723\n",
      "Epoch 1, Batch 920/2700 (34.07%), Loss: 6.7636\n",
      "Epoch 1, Batch 930/2700 (34.44%), Loss: 4.3113\n",
      "Epoch 1, Batch 940/2700 (34.81%), Loss: 12.6624\n",
      "Epoch 1, Batch 950/2700 (35.19%), Loss: 15.1280\n",
      "Epoch 1, Batch 960/2700 (35.56%), Loss: 24.3994\n",
      "Epoch 1, Batch 970/2700 (35.93%), Loss: 6.5699\n",
      "Epoch 1, Batch 980/2700 (36.30%), Loss: 5.0775\n",
      "Epoch 1, Batch 990/2700 (36.67%), Loss: 14.0775\n",
      "Epoch 1, Batch 1000/2700 (37.04%), Loss: 1.1040\n",
      "Epoch 1, Batch 1010/2700 (37.41%), Loss: 0.4369\n",
      "Epoch 1, Batch 1020/2700 (37.78%), Loss: 10.9450\n",
      "Epoch 1, Batch 1030/2700 (38.15%), Loss: 5.1258\n",
      "Epoch 1, Batch 1040/2700 (38.52%), Loss: 12.0943\n",
      "Epoch 1, Batch 1050/2700 (38.89%), Loss: 2.9624\n",
      "Epoch 1, Batch 1060/2700 (39.26%), Loss: 15.4586\n",
      "Epoch 1, Batch 1070/2700 (39.63%), Loss: 13.7812\n",
      "Epoch 1, Batch 1080/2700 (40.00%), Loss: 3.1071\n",
      "Epoch 1, Batch 1090/2700 (40.37%), Loss: 3.1833\n",
      "Epoch 1, Batch 1100/2700 (40.74%), Loss: 11.4872\n",
      "Epoch 1, Batch 1110/2700 (41.11%), Loss: 4.4653\n",
      "Epoch 1, Batch 1120/2700 (41.48%), Loss: 21.5455\n",
      "Epoch 1, Batch 1130/2700 (41.85%), Loss: 1.1466\n",
      "Epoch 1, Batch 1140/2700 (42.22%), Loss: 4.3148\n",
      "Epoch 1, Batch 1150/2700 (42.59%), Loss: 35.5346\n",
      "Epoch 1, Batch 1160/2700 (42.96%), Loss: 0.5752\n",
      "Epoch 1, Batch 1170/2700 (43.33%), Loss: 24.0833\n",
      "Epoch 1, Batch 1180/2700 (43.70%), Loss: 15.4387\n",
      "Epoch 1, Batch 1190/2700 (44.07%), Loss: 4.7938\n",
      "Epoch 1, Batch 1200/2700 (44.44%), Loss: 2.7897\n",
      "Epoch 1, Batch 1210/2700 (44.81%), Loss: 6.4687\n",
      "Epoch 1, Batch 1220/2700 (45.19%), Loss: 6.8413\n",
      "Epoch 1, Batch 1230/2700 (45.56%), Loss: 1.9297\n",
      "Epoch 1, Batch 1240/2700 (45.93%), Loss: 0.5529\n",
      "Epoch 1, Batch 1250/2700 (46.30%), Loss: 6.6717\n",
      "Epoch 1, Batch 1260/2700 (46.67%), Loss: 1.7377\n",
      "Epoch 1, Batch 1270/2700 (47.04%), Loss: 0.4990\n",
      "Epoch 1, Batch 1280/2700 (47.41%), Loss: 2.8156\n",
      "Epoch 1, Batch 1290/2700 (47.78%), Loss: 9.6625\n",
      "Epoch 1, Batch 1300/2700 (48.15%), Loss: 19.7498\n",
      "Epoch 1, Batch 1310/2700 (48.52%), Loss: 15.7648\n",
      "Epoch 1, Batch 1320/2700 (48.89%), Loss: 8.3798\n",
      "Epoch 1, Batch 1330/2700 (49.26%), Loss: 0.7789\n",
      "Epoch 1, Batch 1340/2700 (49.63%), Loss: 5.8344\n",
      "Epoch 1, Batch 1350/2700 (50.00%), Loss: 2.6258\n",
      "Epoch 1, Batch 1360/2700 (50.37%), Loss: 10.8363\n",
      "Epoch 1, Batch 1370/2700 (50.74%), Loss: 6.2438\n",
      "Epoch 1, Batch 1380/2700 (51.11%), Loss: 3.3138\n",
      "Epoch 1, Batch 1390/2700 (51.48%), Loss: 14.7615\n",
      "Epoch 1, Batch 1400/2700 (51.85%), Loss: 3.8896\n",
      "Epoch 1, Batch 1410/2700 (52.22%), Loss: 31.8755\n",
      "Epoch 1, Batch 1420/2700 (52.59%), Loss: 12.0349\n",
      "Epoch 1, Batch 1430/2700 (52.96%), Loss: 3.6860\n",
      "Epoch 1, Batch 1440/2700 (53.33%), Loss: 12.6212\n",
      "Epoch 1, Batch 1450/2700 (53.70%), Loss: 21.4517\n",
      "Epoch 1, Batch 1460/2700 (54.07%), Loss: 13.1726\n",
      "Epoch 1, Batch 1470/2700 (54.44%), Loss: 3.4440\n",
      "Epoch 1, Batch 1480/2700 (54.81%), Loss: 2.7349\n",
      "Epoch 1, Batch 1490/2700 (55.19%), Loss: 6.6171\n",
      "Epoch 1, Batch 1500/2700 (55.56%), Loss: 4.3206\n",
      "Epoch 1, Batch 1510/2700 (55.93%), Loss: 11.0591\n",
      "Epoch 1, Batch 1520/2700 (56.30%), Loss: 6.5387\n",
      "Epoch 1, Batch 1530/2700 (56.67%), Loss: 7.4087\n",
      "Epoch 1, Batch 1540/2700 (57.04%), Loss: 3.0674\n",
      "Epoch 1, Batch 1550/2700 (57.41%), Loss: 0.9467\n",
      "Epoch 1, Batch 1560/2700 (57.78%), Loss: 7.2928\n",
      "Epoch 1, Batch 1570/2700 (58.15%), Loss: 7.4075\n",
      "Epoch 1, Batch 1580/2700 (58.52%), Loss: 9.9637\n",
      "Epoch 1, Batch 1590/2700 (58.89%), Loss: 1.8370\n",
      "Epoch 1, Batch 1600/2700 (59.26%), Loss: 4.6053\n",
      "Epoch 1, Batch 1610/2700 (59.63%), Loss: 11.1898\n",
      "Epoch 1, Batch 1620/2700 (60.00%), Loss: 8.9265\n",
      "Epoch 1, Batch 1630/2700 (60.37%), Loss: 3.8881\n",
      "Epoch 1, Batch 1640/2700 (60.74%), Loss: 8.3096\n",
      "Epoch 1, Batch 1650/2700 (61.11%), Loss: 3.9492\n",
      "Epoch 1, Batch 1660/2700 (61.48%), Loss: 5.7694\n",
      "Epoch 1, Batch 1670/2700 (61.85%), Loss: 0.5024\n",
      "Epoch 1, Batch 1680/2700 (62.22%), Loss: 2.9323\n",
      "Epoch 1, Batch 1690/2700 (62.59%), Loss: 1.7636\n",
      "Epoch 1, Batch 1700/2700 (62.96%), Loss: 3.8837\n",
      "Epoch 1, Batch 1710/2700 (63.33%), Loss: 1.3545\n",
      "Epoch 1, Batch 1720/2700 (63.70%), Loss: 12.7537\n",
      "Epoch 1, Batch 1730/2700 (64.07%), Loss: 2.0972\n",
      "Epoch 1, Batch 1740/2700 (64.44%), Loss: 1.3583\n",
      "Epoch 1, Batch 1750/2700 (64.81%), Loss: 5.0991\n",
      "Epoch 1, Batch 1760/2700 (65.19%), Loss: 5.0748\n",
      "Epoch 1, Batch 1770/2700 (65.56%), Loss: 8.6522\n",
      "Epoch 1, Batch 1780/2700 (65.93%), Loss: 12.3221\n",
      "Epoch 1, Batch 1790/2700 (66.30%), Loss: 2.5543\n",
      "Epoch 1, Batch 1800/2700 (66.67%), Loss: 0.3354\n",
      "Epoch 1, Batch 1810/2700 (67.04%), Loss: 21.5669\n",
      "Epoch 1, Batch 1820/2700 (67.41%), Loss: 5.2205\n",
      "Epoch 1, Batch 1830/2700 (67.78%), Loss: 0.1655\n",
      "Epoch 1, Batch 1840/2700 (68.15%), Loss: 8.3666\n",
      "Epoch 1, Batch 1850/2700 (68.52%), Loss: 3.3486\n",
      "Epoch 1, Batch 1860/2700 (68.89%), Loss: 16.1738\n",
      "Epoch 1, Batch 1870/2700 (69.26%), Loss: 7.5284\n",
      "Epoch 1, Batch 1880/2700 (69.63%), Loss: 0.3090\n",
      "Epoch 1, Batch 1890/2700 (70.00%), Loss: 16.1988\n",
      "Epoch 1, Batch 1900/2700 (70.37%), Loss: 10.7618\n",
      "Epoch 1, Batch 1910/2700 (70.74%), Loss: 3.7561\n",
      "Epoch 1, Batch 1920/2700 (71.11%), Loss: 15.6223\n",
      "Epoch 1, Batch 1930/2700 (71.48%), Loss: 11.5582\n",
      "Epoch 1, Batch 1940/2700 (71.85%), Loss: 0.6746\n",
      "Epoch 1, Batch 1950/2700 (72.22%), Loss: 11.2117\n",
      "Epoch 1, Batch 1960/2700 (72.59%), Loss: 0.8516\n",
      "Epoch 1, Batch 1970/2700 (72.96%), Loss: 0.7975\n",
      "Epoch 1, Batch 1980/2700 (73.33%), Loss: 8.1240\n",
      "Epoch 1, Batch 1990/2700 (73.70%), Loss: 0.1819\n",
      "Epoch 1, Batch 2000/2700 (74.07%), Loss: 22.2569\n",
      "Epoch 1, Batch 2010/2700 (74.44%), Loss: 4.2238\n",
      "Epoch 1, Batch 2020/2700 (74.81%), Loss: 3.5816\n",
      "Epoch 1, Batch 2030/2700 (75.19%), Loss: 3.6878\n",
      "Epoch 1, Batch 2040/2700 (75.56%), Loss: 1.0537\n",
      "Epoch 1, Batch 2050/2700 (75.93%), Loss: 8.7735\n",
      "Epoch 1, Batch 2060/2700 (76.30%), Loss: 9.3170\n",
      "Epoch 1, Batch 2070/2700 (76.67%), Loss: 4.7457\n",
      "Epoch 1, Batch 2080/2700 (77.04%), Loss: 1.9292\n",
      "Epoch 1, Batch 2090/2700 (77.41%), Loss: 0.2368\n",
      "Epoch 1, Batch 2100/2700 (77.78%), Loss: 0.7574\n",
      "Epoch 1, Batch 2110/2700 (78.15%), Loss: 12.2952\n",
      "Epoch 1, Batch 2120/2700 (78.52%), Loss: 1.2437\n",
      "Epoch 1, Batch 2130/2700 (78.89%), Loss: 4.3699\n",
      "Epoch 1, Batch 2140/2700 (79.26%), Loss: 8.6270\n",
      "Epoch 1, Batch 2150/2700 (79.63%), Loss: 1.0618\n",
      "Epoch 1, Batch 2160/2700 (80.00%), Loss: 6.8245\n",
      "Epoch 1, Batch 2170/2700 (80.37%), Loss: 0.5170\n",
      "Epoch 1, Batch 2180/2700 (80.74%), Loss: 5.8961\n",
      "Epoch 1, Batch 2190/2700 (81.11%), Loss: 4.3632\n",
      "Epoch 1, Batch 2200/2700 (81.48%), Loss: 0.2026\n",
      "Epoch 1, Batch 2210/2700 (81.85%), Loss: 14.5194\n",
      "Epoch 1, Batch 2220/2700 (82.22%), Loss: 4.7836\n",
      "Epoch 1, Batch 2230/2700 (82.59%), Loss: 0.2321\n",
      "Epoch 1, Batch 2240/2700 (82.96%), Loss: 0.6804\n",
      "Epoch 1, Batch 2250/2700 (83.33%), Loss: 2.7830\n",
      "Epoch 1, Batch 2260/2700 (83.70%), Loss: 5.4579\n",
      "Epoch 1, Batch 2270/2700 (84.07%), Loss: 3.5346\n",
      "Epoch 1, Batch 2280/2700 (84.44%), Loss: 8.0999\n",
      "Epoch 1, Batch 2290/2700 (84.81%), Loss: 4.8483\n",
      "Epoch 1, Batch 2300/2700 (85.19%), Loss: 4.7683\n",
      "Epoch 1, Batch 2310/2700 (85.56%), Loss: 12.3135\n",
      "Epoch 1, Batch 2320/2700 (85.93%), Loss: 16.6744\n",
      "Epoch 1, Batch 2330/2700 (86.30%), Loss: 15.3400\n",
      "Epoch 1, Batch 2340/2700 (86.67%), Loss: 8.0325\n",
      "Epoch 1, Batch 2350/2700 (87.04%), Loss: 0.9923\n",
      "Epoch 1, Batch 2360/2700 (87.41%), Loss: 2.7410\n",
      "Epoch 1, Batch 2370/2700 (87.78%), Loss: 5.3645\n",
      "Epoch 1, Batch 2380/2700 (88.15%), Loss: 65.4277\n",
      "Epoch 1, Batch 2390/2700 (88.52%), Loss: 30.3216\n",
      "Epoch 1, Batch 2400/2700 (88.89%), Loss: 9.2708\n",
      "Epoch 1, Batch 2410/2700 (89.26%), Loss: 10.3320\n",
      "Epoch 1, Batch 2420/2700 (89.63%), Loss: 14.7133\n",
      "Epoch 1, Batch 2430/2700 (90.00%), Loss: 7.1516\n",
      "Epoch 1, Batch 2440/2700 (90.37%), Loss: 1.7363\n",
      "Epoch 1, Batch 2450/2700 (90.74%), Loss: 9.4504\n",
      "Epoch 1, Batch 2460/2700 (91.11%), Loss: 3.1114\n",
      "Epoch 1, Batch 2470/2700 (91.48%), Loss: 3.7479\n",
      "Epoch 1, Batch 2480/2700 (91.85%), Loss: 2.1872\n",
      "Epoch 1, Batch 2490/2700 (92.22%), Loss: 15.4222\n",
      "Epoch 1, Batch 2500/2700 (92.59%), Loss: 5.1904\n",
      "Epoch 1, Batch 2510/2700 (92.96%), Loss: 2.2734\n",
      "Epoch 1, Batch 2520/2700 (93.33%), Loss: 40.1441\n",
      "Epoch 1, Batch 2530/2700 (93.70%), Loss: 8.3212\n",
      "Epoch 1, Batch 2540/2700 (94.07%), Loss: 0.0847\n",
      "Epoch 1, Batch 2550/2700 (94.44%), Loss: 0.1205\n",
      "Epoch 1, Batch 2560/2700 (94.81%), Loss: 8.0893\n",
      "Epoch 1, Batch 2570/2700 (95.19%), Loss: 3.5165\n",
      "Epoch 1, Batch 2580/2700 (95.56%), Loss: 4.3516\n",
      "Epoch 1, Batch 2590/2700 (95.93%), Loss: 17.0925\n",
      "Epoch 1, Batch 2600/2700 (96.30%), Loss: 2.4832\n",
      "Epoch 1, Batch 2610/2700 (96.67%), Loss: 2.7476\n",
      "Epoch 1, Batch 2620/2700 (97.04%), Loss: 8.9139\n",
      "Epoch 1, Batch 2630/2700 (97.41%), Loss: 4.0040\n",
      "Epoch 1, Batch 2640/2700 (97.78%), Loss: 8.8069\n",
      "Epoch 1, Batch 2650/2700 (98.15%), Loss: 3.0771\n",
      "Epoch 1, Batch 2660/2700 (98.52%), Loss: 16.7589\n",
      "Epoch 1, Batch 2670/2700 (98.89%), Loss: 4.3295\n",
      "Epoch 1, Batch 2680/2700 (99.26%), Loss: 7.5450\n",
      "Epoch 1, Batch 2690/2700 (99.63%), Loss: 6.4058\n",
      "Epoch 1, Batch 2700/2700 (100.00%), Loss: 10.2413\n",
      "Epoch 1 completed. Average loss: 9.7942\n",
      "Epoch 2, Batch 10/2700 (0.37%), Loss: 4.4692\n",
      "Epoch 2, Batch 20/2700 (0.74%), Loss: 1.0347\n",
      "Epoch 2, Batch 30/2700 (1.11%), Loss: 1.8587\n",
      "Epoch 2, Batch 40/2700 (1.48%), Loss: 1.2225\n",
      "Epoch 2, Batch 50/2700 (1.85%), Loss: 2.6946\n",
      "Epoch 2, Batch 60/2700 (2.22%), Loss: 2.9361\n",
      "Epoch 2, Batch 70/2700 (2.59%), Loss: 7.3835\n",
      "Epoch 2, Batch 80/2700 (2.96%), Loss: 1.0509\n",
      "Epoch 2, Batch 90/2700 (3.33%), Loss: 4.6675\n",
      "Epoch 2, Batch 100/2700 (3.70%), Loss: 10.9081\n",
      "Epoch 2, Batch 110/2700 (4.07%), Loss: 2.0072\n",
      "Epoch 2, Batch 120/2700 (4.44%), Loss: 7.9373\n",
      "Epoch 2, Batch 130/2700 (4.81%), Loss: 7.8318\n",
      "Epoch 2, Batch 140/2700 (5.19%), Loss: 1.4990\n",
      "Epoch 2, Batch 150/2700 (5.56%), Loss: 1.0670\n",
      "Epoch 2, Batch 160/2700 (5.93%), Loss: 1.2068\n",
      "Epoch 2, Batch 170/2700 (6.30%), Loss: 1.4031\n",
      "Epoch 2, Batch 180/2700 (6.67%), Loss: 7.6340\n",
      "Epoch 2, Batch 190/2700 (7.04%), Loss: 7.5546\n",
      "Epoch 2, Batch 200/2700 (7.41%), Loss: 0.9720\n",
      "Epoch 2, Batch 210/2700 (7.78%), Loss: 7.1622\n",
      "Epoch 2, Batch 220/2700 (8.15%), Loss: 12.3635\n",
      "Epoch 2, Batch 230/2700 (8.52%), Loss: 2.7329\n",
      "Epoch 2, Batch 240/2700 (8.89%), Loss: 11.9158\n",
      "Epoch 2, Batch 250/2700 (9.26%), Loss: 1.2546\n",
      "Epoch 2, Batch 260/2700 (9.63%), Loss: 10.6823\n",
      "Epoch 2, Batch 270/2700 (10.00%), Loss: 49.9783\n",
      "Epoch 2, Batch 280/2700 (10.37%), Loss: 0.7528\n",
      "Epoch 2, Batch 290/2700 (10.74%), Loss: 6.7035\n",
      "Epoch 2, Batch 300/2700 (11.11%), Loss: 2.2295\n",
      "Epoch 2, Batch 310/2700 (11.48%), Loss: 38.0692\n",
      "Epoch 2, Batch 320/2700 (11.85%), Loss: 6.6807\n",
      "Epoch 2, Batch 330/2700 (12.22%), Loss: 7.1896\n",
      "Epoch 2, Batch 340/2700 (12.59%), Loss: 10.8917\n",
      "Epoch 2, Batch 350/2700 (12.96%), Loss: 1.4661\n",
      "Epoch 2, Batch 360/2700 (13.33%), Loss: 2.1697\n",
      "Epoch 2, Batch 370/2700 (13.70%), Loss: 6.9606\n",
      "Epoch 2, Batch 380/2700 (14.07%), Loss: 10.3903\n",
      "Epoch 2, Batch 390/2700 (14.44%), Loss: 0.1764\n",
      "Epoch 2, Batch 400/2700 (14.81%), Loss: 4.9307\n",
      "Epoch 2, Batch 410/2700 (15.19%), Loss: 15.2826\n",
      "Epoch 2, Batch 420/2700 (15.56%), Loss: 0.4805\n",
      "Epoch 2, Batch 430/2700 (15.93%), Loss: 4.0687\n",
      "Epoch 2, Batch 440/2700 (16.30%), Loss: 5.1451\n",
      "Epoch 2, Batch 450/2700 (16.67%), Loss: 9.3638\n",
      "Epoch 2, Batch 460/2700 (17.04%), Loss: 4.2075\n",
      "Epoch 2, Batch 470/2700 (17.41%), Loss: 0.3092\n",
      "Epoch 2, Batch 480/2700 (17.78%), Loss: 3.7012\n",
      "Epoch 2, Batch 490/2700 (18.15%), Loss: 0.7930\n",
      "Epoch 2, Batch 500/2700 (18.52%), Loss: 0.5240\n",
      "Epoch 2, Batch 510/2700 (18.89%), Loss: 4.2548\n",
      "Epoch 2, Batch 520/2700 (19.26%), Loss: 2.1234\n",
      "Epoch 2, Batch 530/2700 (19.63%), Loss: 5.9409\n",
      "Epoch 2, Batch 540/2700 (20.00%), Loss: 0.6174\n",
      "Epoch 2, Batch 550/2700 (20.37%), Loss: 1.6789\n",
      "Epoch 2, Batch 560/2700 (20.74%), Loss: 2.3345\n",
      "Epoch 2, Batch 570/2700 (21.11%), Loss: 4.0127\n",
      "Epoch 2, Batch 580/2700 (21.48%), Loss: 4.8984\n",
      "Epoch 2, Batch 590/2700 (21.85%), Loss: 8.7263\n",
      "Epoch 2, Batch 600/2700 (22.22%), Loss: 4.5041\n",
      "Epoch 2, Batch 610/2700 (22.59%), Loss: 0.8001\n",
      "Epoch 2, Batch 620/2700 (22.96%), Loss: 1.4341\n",
      "Epoch 2, Batch 630/2700 (23.33%), Loss: 5.0718\n",
      "Epoch 2, Batch 640/2700 (23.70%), Loss: 4.8339\n",
      "Epoch 2, Batch 650/2700 (24.07%), Loss: 7.8588\n",
      "Epoch 2, Batch 660/2700 (24.44%), Loss: 12.8790\n",
      "Epoch 2, Batch 670/2700 (24.81%), Loss: 0.3550\n",
      "Epoch 2, Batch 680/2700 (25.19%), Loss: 0.5035\n",
      "Epoch 2, Batch 690/2700 (25.56%), Loss: 4.4817\n",
      "Epoch 2, Batch 700/2700 (25.93%), Loss: 0.1157\n",
      "Epoch 2, Batch 710/2700 (26.30%), Loss: 2.7034\n",
      "Epoch 2, Batch 720/2700 (26.67%), Loss: 7.9607\n",
      "Epoch 2, Batch 730/2700 (27.04%), Loss: 14.7045\n",
      "Epoch 2, Batch 740/2700 (27.41%), Loss: 0.4643\n",
      "Epoch 2, Batch 750/2700 (27.78%), Loss: 10.6827\n",
      "Epoch 2, Batch 760/2700 (28.15%), Loss: 5.4852\n",
      "Epoch 2, Batch 770/2700 (28.52%), Loss: 8.4313\n",
      "Epoch 2, Batch 780/2700 (28.89%), Loss: 2.7446\n",
      "Epoch 2, Batch 790/2700 (29.26%), Loss: 22.9979\n",
      "Epoch 2, Batch 800/2700 (29.63%), Loss: 7.4157\n",
      "Epoch 2, Batch 810/2700 (30.00%), Loss: 1.3911\n",
      "Epoch 2, Batch 820/2700 (30.37%), Loss: 14.6911\n",
      "Epoch 2, Batch 830/2700 (30.74%), Loss: 2.0694\n",
      "Epoch 2, Batch 840/2700 (31.11%), Loss: 0.0810\n",
      "Epoch 2, Batch 850/2700 (31.48%), Loss: 14.0026\n",
      "Epoch 2, Batch 860/2700 (31.85%), Loss: 3.3859\n",
      "Epoch 2, Batch 870/2700 (32.22%), Loss: 17.7892\n",
      "Epoch 2, Batch 880/2700 (32.59%), Loss: 12.8389\n",
      "Epoch 2, Batch 890/2700 (32.96%), Loss: 9.2666\n",
      "Epoch 2, Batch 900/2700 (33.33%), Loss: 5.5925\n",
      "Epoch 2, Batch 910/2700 (33.70%), Loss: 0.6433\n",
      "Epoch 2, Batch 920/2700 (34.07%), Loss: 1.0674\n",
      "Epoch 2, Batch 930/2700 (34.44%), Loss: 2.1573\n",
      "Epoch 2, Batch 940/2700 (34.81%), Loss: 6.1574\n",
      "Epoch 2, Batch 950/2700 (35.19%), Loss: 5.7153\n",
      "Epoch 2, Batch 960/2700 (35.56%), Loss: 13.0887\n",
      "Epoch 2, Batch 970/2700 (35.93%), Loss: 1.7431\n",
      "Epoch 2, Batch 980/2700 (36.30%), Loss: 2.4166\n",
      "Epoch 2, Batch 990/2700 (36.67%), Loss: 8.3041\n",
      "Epoch 2, Batch 1000/2700 (37.04%), Loss: 0.6891\n",
      "Epoch 2, Batch 1010/2700 (37.41%), Loss: 0.0543\n",
      "Epoch 2, Batch 1020/2700 (37.78%), Loss: 7.3496\n",
      "Epoch 2, Batch 1030/2700 (38.15%), Loss: 1.0754\n",
      "Epoch 2, Batch 1040/2700 (38.52%), Loss: 9.9091\n",
      "Epoch 2, Batch 1050/2700 (38.89%), Loss: 1.1056\n",
      "Epoch 2, Batch 1060/2700 (39.26%), Loss: 8.2476\n",
      "Epoch 2, Batch 1070/2700 (39.63%), Loss: 3.5827\n",
      "Epoch 2, Batch 1080/2700 (40.00%), Loss: 0.4113\n",
      "Epoch 2, Batch 1090/2700 (40.37%), Loss: 1.5768\n",
      "Epoch 2, Batch 1100/2700 (40.74%), Loss: 4.3869\n",
      "Epoch 2, Batch 1110/2700 (41.11%), Loss: 0.7696\n",
      "Epoch 2, Batch 1120/2700 (41.48%), Loss: 10.0522\n",
      "Epoch 2, Batch 1130/2700 (41.85%), Loss: 0.1520\n",
      "Epoch 2, Batch 1140/2700 (42.22%), Loss: 1.5488\n",
      "Epoch 2, Batch 1150/2700 (42.59%), Loss: 16.3063\n",
      "Epoch 2, Batch 1160/2700 (42.96%), Loss: 0.1647\n",
      "Epoch 2, Batch 1170/2700 (43.33%), Loss: 13.2704\n",
      "Epoch 2, Batch 1180/2700 (43.70%), Loss: 10.6474\n",
      "Epoch 2, Batch 1190/2700 (44.07%), Loss: 2.3886\n",
      "Epoch 2, Batch 1200/2700 (44.44%), Loss: 1.5023\n",
      "Epoch 2, Batch 1210/2700 (44.81%), Loss: 4.2861\n",
      "Epoch 2, Batch 1220/2700 (45.19%), Loss: 3.9224\n",
      "Epoch 2, Batch 1230/2700 (45.56%), Loss: 1.1051\n",
      "Epoch 2, Batch 1240/2700 (45.93%), Loss: 0.0460\n",
      "Epoch 2, Batch 1250/2700 (46.30%), Loss: 2.2191\n",
      "Epoch 2, Batch 1260/2700 (46.67%), Loss: 0.7848\n",
      "Epoch 2, Batch 1270/2700 (47.04%), Loss: 0.2147\n",
      "Epoch 2, Batch 1280/2700 (47.41%), Loss: 2.2636\n",
      "Epoch 2, Batch 1290/2700 (47.78%), Loss: 3.8828\n",
      "Epoch 2, Batch 1300/2700 (48.15%), Loss: 11.6088\n",
      "Epoch 2, Batch 1310/2700 (48.52%), Loss: 10.2544\n",
      "Epoch 2, Batch 1320/2700 (48.89%), Loss: 2.3652\n",
      "Epoch 2, Batch 1330/2700 (49.26%), Loss: 0.3058\n",
      "Epoch 2, Batch 1340/2700 (49.63%), Loss: 2.9363\n",
      "Epoch 2, Batch 1350/2700 (50.00%), Loss: 0.9127\n",
      "Epoch 2, Batch 1360/2700 (50.37%), Loss: 4.7656\n",
      "Epoch 2, Batch 1370/2700 (50.74%), Loss: 4.0047\n",
      "Epoch 2, Batch 1380/2700 (51.11%), Loss: 1.2778\n",
      "Epoch 2, Batch 1390/2700 (51.48%), Loss: 5.6255\n",
      "Epoch 2, Batch 1400/2700 (51.85%), Loss: 3.0915\n",
      "Epoch 2, Batch 1410/2700 (52.22%), Loss: 16.5095\n",
      "Epoch 2, Batch 1420/2700 (52.59%), Loss: 9.1408\n",
      "Epoch 2, Batch 1430/2700 (52.96%), Loss: 1.3355\n",
      "Epoch 2, Batch 1440/2700 (53.33%), Loss: 7.5715\n",
      "Epoch 2, Batch 1450/2700 (53.70%), Loss: 15.7596\n",
      "Epoch 2, Batch 1460/2700 (54.07%), Loss: 4.9620\n",
      "Epoch 2, Batch 1470/2700 (54.44%), Loss: 0.3004\n",
      "Epoch 2, Batch 1480/2700 (54.81%), Loss: 0.8729\n",
      "Epoch 2, Batch 1490/2700 (55.19%), Loss: 2.2392\n",
      "Epoch 2, Batch 1500/2700 (55.56%), Loss: 2.9141\n",
      "Epoch 2, Batch 1510/2700 (55.93%), Loss: 2.9725\n",
      "Epoch 2, Batch 1520/2700 (56.30%), Loss: 1.4968\n",
      "Epoch 2, Batch 1530/2700 (56.67%), Loss: 5.4284\n",
      "Epoch 2, Batch 1540/2700 (57.04%), Loss: 1.1749\n",
      "Epoch 2, Batch 1550/2700 (57.41%), Loss: 0.5236\n",
      "Epoch 2, Batch 1560/2700 (57.78%), Loss: 4.2792\n",
      "Epoch 2, Batch 1570/2700 (58.15%), Loss: 2.8120\n",
      "Epoch 2, Batch 1580/2700 (58.52%), Loss: 7.3002\n",
      "Epoch 2, Batch 1590/2700 (58.89%), Loss: 1.2762\n",
      "Epoch 2, Batch 1600/2700 (59.26%), Loss: 1.9231\n",
      "Epoch 2, Batch 1610/2700 (59.63%), Loss: 5.8475\n",
      "Epoch 2, Batch 1620/2700 (60.00%), Loss: 2.6406\n",
      "Epoch 2, Batch 1630/2700 (60.37%), Loss: 2.5331\n",
      "Epoch 2, Batch 1640/2700 (60.74%), Loss: 9.4167\n",
      "Epoch 2, Batch 1650/2700 (61.11%), Loss: 1.4448\n",
      "Epoch 2, Batch 1660/2700 (61.48%), Loss: 5.5381\n",
      "Epoch 2, Batch 1670/2700 (61.85%), Loss: 0.0688\n",
      "Epoch 2, Batch 1680/2700 (62.22%), Loss: 1.5374\n",
      "Epoch 2, Batch 1690/2700 (62.59%), Loss: 0.5253\n",
      "Epoch 2, Batch 1700/2700 (62.96%), Loss: 3.6241\n",
      "Epoch 2, Batch 1710/2700 (63.33%), Loss: 1.1566\n",
      "Epoch 2, Batch 1720/2700 (63.70%), Loss: 9.3007\n",
      "Epoch 2, Batch 1730/2700 (64.07%), Loss: 1.3889\n",
      "Epoch 2, Batch 1740/2700 (64.44%), Loss: 0.3544\n",
      "Epoch 2, Batch 1750/2700 (64.81%), Loss: 2.5848\n",
      "Epoch 2, Batch 1760/2700 (65.19%), Loss: 2.6470\n",
      "Epoch 2, Batch 1770/2700 (65.56%), Loss: 3.7409\n",
      "Epoch 2, Batch 1780/2700 (65.93%), Loss: 8.5851\n",
      "Epoch 2, Batch 1790/2700 (66.30%), Loss: 1.6141\n",
      "Epoch 2, Batch 1800/2700 (66.67%), Loss: 0.4407\n",
      "Epoch 2, Batch 1810/2700 (67.04%), Loss: 15.3724\n",
      "Epoch 2, Batch 1820/2700 (67.41%), Loss: 2.2307\n",
      "Epoch 2, Batch 1830/2700 (67.78%), Loss: 0.0464\n",
      "Epoch 2, Batch 1840/2700 (68.15%), Loss: 7.3151\n",
      "Epoch 2, Batch 1850/2700 (68.52%), Loss: 2.8170\n",
      "Epoch 2, Batch 1860/2700 (68.89%), Loss: 8.1021\n",
      "Epoch 2, Batch 1870/2700 (69.26%), Loss: 6.1632\n",
      "Epoch 2, Batch 1880/2700 (69.63%), Loss: 0.1592\n",
      "Epoch 2, Batch 1890/2700 (70.00%), Loss: 11.3724\n",
      "Epoch 2, Batch 1900/2700 (70.37%), Loss: 5.9565\n",
      "Epoch 2, Batch 1910/2700 (70.74%), Loss: 1.0187\n",
      "Epoch 2, Batch 1920/2700 (71.11%), Loss: 6.9539\n",
      "Epoch 2, Batch 1930/2700 (71.48%), Loss: 6.3350\n",
      "Epoch 2, Batch 1940/2700 (71.85%), Loss: 0.3112\n",
      "Epoch 2, Batch 1950/2700 (72.22%), Loss: 10.3760\n",
      "Epoch 2, Batch 1960/2700 (72.59%), Loss: 0.2095\n",
      "Epoch 2, Batch 1970/2700 (72.96%), Loss: 0.1664\n",
      "Epoch 2, Batch 1980/2700 (73.33%), Loss: 3.8423\n",
      "Epoch 2, Batch 1990/2700 (73.70%), Loss: 0.0587\n",
      "Epoch 2, Batch 2000/2700 (74.07%), Loss: 11.4399\n",
      "Epoch 2, Batch 2010/2700 (74.44%), Loss: 2.2489\n",
      "Epoch 2, Batch 2020/2700 (74.81%), Loss: 1.5805\n",
      "Epoch 2, Batch 2030/2700 (75.19%), Loss: 1.3967\n",
      "Epoch 2, Batch 2040/2700 (75.56%), Loss: 0.4460\n",
      "Epoch 2, Batch 2050/2700 (75.93%), Loss: 8.5925\n",
      "Epoch 2, Batch 2060/2700 (76.30%), Loss: 2.4752\n",
      "Epoch 2, Batch 2070/2700 (76.67%), Loss: 4.0889\n",
      "Epoch 2, Batch 2080/2700 (77.04%), Loss: 1.0814\n",
      "Epoch 2, Batch 2090/2700 (77.41%), Loss: 0.0488\n",
      "Epoch 2, Batch 2100/2700 (77.78%), Loss: 0.1163\n",
      "Epoch 2, Batch 2110/2700 (78.15%), Loss: 6.5291\n",
      "Epoch 2, Batch 2120/2700 (78.52%), Loss: 0.2753\n",
      "Epoch 2, Batch 2130/2700 (78.89%), Loss: 1.2589\n",
      "Epoch 2, Batch 2140/2700 (79.26%), Loss: 6.7086\n",
      "Epoch 2, Batch 2150/2700 (79.63%), Loss: 0.4304\n",
      "Epoch 2, Batch 2160/2700 (80.00%), Loss: 4.4005\n",
      "Epoch 2, Batch 2170/2700 (80.37%), Loss: 0.0842\n",
      "Epoch 2, Batch 2180/2700 (80.74%), Loss: 5.1761\n",
      "Epoch 2, Batch 2190/2700 (81.11%), Loss: 2.3372\n",
      "Epoch 2, Batch 2200/2700 (81.48%), Loss: 0.1111\n",
      "Epoch 2, Batch 2210/2700 (81.85%), Loss: 5.8167\n",
      "Epoch 2, Batch 2220/2700 (82.22%), Loss: 2.3834\n",
      "Epoch 2, Batch 2230/2700 (82.59%), Loss: 0.1364\n",
      "Epoch 2, Batch 2240/2700 (82.96%), Loss: 0.2888\n",
      "Epoch 2, Batch 2250/2700 (83.33%), Loss: 2.0260\n",
      "Epoch 2, Batch 2260/2700 (83.70%), Loss: 1.0571\n",
      "Epoch 2, Batch 2270/2700 (84.07%), Loss: 1.4828\n",
      "Epoch 2, Batch 2280/2700 (84.44%), Loss: 5.6513\n",
      "Epoch 2, Batch 2290/2700 (84.81%), Loss: 2.0834\n",
      "Epoch 2, Batch 2300/2700 (85.19%), Loss: 2.6638\n",
      "Epoch 2, Batch 2310/2700 (85.56%), Loss: 4.0440\n",
      "Epoch 2, Batch 2320/2700 (85.93%), Loss: 12.0288\n",
      "Epoch 2, Batch 2330/2700 (86.30%), Loss: 8.0684\n",
      "Epoch 2, Batch 2340/2700 (86.67%), Loss: 4.4025\n",
      "Epoch 2, Batch 2350/2700 (87.04%), Loss: 0.8544\n",
      "Epoch 2, Batch 2360/2700 (87.41%), Loss: 0.8580\n",
      "Epoch 2, Batch 2370/2700 (87.78%), Loss: 3.6432\n",
      "Epoch 2, Batch 2380/2700 (88.15%), Loss: 57.9351\n",
      "Epoch 2, Batch 2390/2700 (88.52%), Loss: 20.2576\n",
      "Epoch 2, Batch 2400/2700 (88.89%), Loss: 6.4387\n",
      "Epoch 2, Batch 2410/2700 (89.26%), Loss: 8.4833\n",
      "Epoch 2, Batch 2420/2700 (89.63%), Loss: 10.3969\n",
      "Epoch 2, Batch 2430/2700 (90.00%), Loss: 6.0970\n",
      "Epoch 2, Batch 2440/2700 (90.37%), Loss: 1.8434\n",
      "Epoch 2, Batch 2450/2700 (90.74%), Loss: 4.1121\n",
      "Epoch 2, Batch 2460/2700 (91.11%), Loss: 1.3409\n",
      "Epoch 2, Batch 2470/2700 (91.48%), Loss: 2.2336\n",
      "Epoch 2, Batch 2480/2700 (91.85%), Loss: 0.6964\n",
      "Epoch 2, Batch 2490/2700 (92.22%), Loss: 9.4549\n",
      "Epoch 2, Batch 2500/2700 (92.59%), Loss: 3.2923\n",
      "Epoch 2, Batch 2510/2700 (92.96%), Loss: 1.2382\n",
      "Epoch 2, Batch 2520/2700 (93.33%), Loss: 22.7007\n",
      "Epoch 2, Batch 2530/2700 (93.70%), Loss: 7.8517\n",
      "Epoch 2, Batch 2540/2700 (94.07%), Loss: 0.0455\n",
      "Epoch 2, Batch 2550/2700 (94.44%), Loss: 0.0525\n",
      "Epoch 2, Batch 2560/2700 (94.81%), Loss: 4.5748\n",
      "Epoch 2, Batch 2570/2700 (95.19%), Loss: 1.4041\n",
      "Epoch 2, Batch 2580/2700 (95.56%), Loss: 2.6115\n",
      "Epoch 2, Batch 2590/2700 (95.93%), Loss: 16.3538\n",
      "Epoch 2, Batch 2600/2700 (96.30%), Loss: 1.2983\n",
      "Epoch 2, Batch 2610/2700 (96.67%), Loss: 1.4103\n",
      "Epoch 2, Batch 2620/2700 (97.04%), Loss: 7.4354\n",
      "Epoch 2, Batch 2630/2700 (97.41%), Loss: 1.3741\n",
      "Epoch 2, Batch 2640/2700 (97.78%), Loss: 6.9859\n",
      "Epoch 2, Batch 2650/2700 (98.15%), Loss: 1.3465\n",
      "Epoch 2, Batch 2660/2700 (98.52%), Loss: 14.8834\n",
      "Epoch 2, Batch 2670/2700 (98.89%), Loss: 2.1832\n",
      "Epoch 2, Batch 2680/2700 (99.26%), Loss: 2.3196\n",
      "Epoch 2, Batch 2690/2700 (99.63%), Loss: 3.1960\n",
      "Epoch 2, Batch 2700/2700 (100.00%), Loss: 5.4009\n",
      "Epoch 2 completed. Average loss: 4.7285\n",
      "Epoch 3, Batch 10/2700 (0.37%), Loss: 2.4667\n",
      "Epoch 3, Batch 20/2700 (0.74%), Loss: 0.3094\n",
      "Epoch 3, Batch 30/2700 (1.11%), Loss: 2.9265\n",
      "Epoch 3, Batch 40/2700 (1.48%), Loss: 0.4777\n",
      "Epoch 3, Batch 50/2700 (1.85%), Loss: 0.8498\n",
      "Epoch 3, Batch 60/2700 (2.22%), Loss: 1.8998\n",
      "Epoch 3, Batch 70/2700 (2.59%), Loss: 3.9127\n",
      "Epoch 3, Batch 80/2700 (2.96%), Loss: 0.2711\n",
      "Epoch 3, Batch 90/2700 (3.33%), Loss: 2.3187\n",
      "Epoch 3, Batch 100/2700 (3.70%), Loss: 6.8635\n",
      "Epoch 3, Batch 110/2700 (4.07%), Loss: 1.2767\n",
      "Epoch 3, Batch 120/2700 (4.44%), Loss: 5.2202\n",
      "Epoch 3, Batch 130/2700 (4.81%), Loss: 6.9422\n",
      "Epoch 3, Batch 140/2700 (5.19%), Loss: 0.5815\n",
      "Epoch 3, Batch 150/2700 (5.56%), Loss: 0.6615\n",
      "Epoch 3, Batch 160/2700 (5.93%), Loss: 0.3506\n",
      "Epoch 3, Batch 170/2700 (6.30%), Loss: 1.5712\n",
      "Epoch 3, Batch 180/2700 (6.67%), Loss: 5.3692\n",
      "Epoch 3, Batch 190/2700 (7.04%), Loss: 4.3182\n",
      "Epoch 3, Batch 200/2700 (7.41%), Loss: 0.4516\n",
      "Epoch 3, Batch 210/2700 (7.78%), Loss: 4.1823\n",
      "Epoch 3, Batch 220/2700 (8.15%), Loss: 9.6593\n",
      "Epoch 3, Batch 230/2700 (8.52%), Loss: 1.6222\n",
      "Epoch 3, Batch 240/2700 (8.89%), Loss: 7.7161\n",
      "Epoch 3, Batch 250/2700 (9.26%), Loss: 0.3945\n",
      "Epoch 3, Batch 260/2700 (9.63%), Loss: 8.1546\n",
      "Epoch 3, Batch 270/2700 (10.00%), Loss: 44.9399\n",
      "Epoch 3, Batch 280/2700 (10.37%), Loss: 0.3348\n",
      "Epoch 3, Batch 290/2700 (10.74%), Loss: 3.0389\n",
      "Epoch 3, Batch 300/2700 (11.11%), Loss: 1.0139\n",
      "Epoch 3, Batch 310/2700 (11.48%), Loss: 31.6740\n",
      "Epoch 3, Batch 320/2700 (11.85%), Loss: 3.0471\n",
      "Epoch 3, Batch 330/2700 (12.22%), Loss: 5.8795\n",
      "Epoch 3, Batch 340/2700 (12.59%), Loss: 7.2471\n",
      "Epoch 3, Batch 350/2700 (12.96%), Loss: 0.9375\n",
      "Epoch 3, Batch 360/2700 (13.33%), Loss: 1.0820\n",
      "Epoch 3, Batch 370/2700 (13.70%), Loss: 4.0006\n",
      "Epoch 3, Batch 380/2700 (14.07%), Loss: 5.5306\n",
      "Epoch 3, Batch 390/2700 (14.44%), Loss: 0.1475\n",
      "Epoch 3, Batch 400/2700 (14.81%), Loss: 3.8693\n",
      "Epoch 3, Batch 410/2700 (15.19%), Loss: 9.6978\n",
      "Epoch 3, Batch 420/2700 (15.56%), Loss: 0.1298\n",
      "Epoch 3, Batch 430/2700 (15.93%), Loss: 2.2838\n",
      "Epoch 3, Batch 440/2700 (16.30%), Loss: 2.8865\n",
      "Epoch 3, Batch 450/2700 (16.67%), Loss: 5.9312\n",
      "Epoch 3, Batch 460/2700 (17.04%), Loss: 2.3031\n",
      "Epoch 3, Batch 470/2700 (17.41%), Loss: 0.2488\n",
      "Epoch 3, Batch 480/2700 (17.78%), Loss: 2.2592\n",
      "Epoch 3, Batch 490/2700 (18.15%), Loss: 0.2277\n",
      "Epoch 3, Batch 500/2700 (18.52%), Loss: 0.2546\n",
      "Epoch 3, Batch 510/2700 (18.89%), Loss: 3.9667\n",
      "Epoch 3, Batch 520/2700 (19.26%), Loss: 0.9077\n",
      "Epoch 3, Batch 530/2700 (19.63%), Loss: 5.2974\n",
      "Epoch 3, Batch 540/2700 (20.00%), Loss: 0.3443\n",
      "Epoch 3, Batch 550/2700 (20.37%), Loss: 0.8101\n",
      "Epoch 3, Batch 560/2700 (20.74%), Loss: 2.2378\n",
      "Epoch 3, Batch 570/2700 (21.11%), Loss: 1.7827\n",
      "Epoch 3, Batch 580/2700 (21.48%), Loss: 2.8657\n",
      "Epoch 3, Batch 590/2700 (21.85%), Loss: 4.7763\n",
      "Epoch 3, Batch 600/2700 (22.22%), Loss: 3.2811\n",
      "Epoch 3, Batch 610/2700 (22.59%), Loss: 1.0110\n",
      "Epoch 3, Batch 620/2700 (22.96%), Loss: 0.3973\n",
      "Epoch 3, Batch 630/2700 (23.33%), Loss: 2.4729\n",
      "Epoch 3, Batch 640/2700 (23.70%), Loss: 4.0072\n",
      "Epoch 3, Batch 650/2700 (24.07%), Loss: 5.1465\n",
      "Epoch 3, Batch 660/2700 (24.44%), Loss: 9.8807\n",
      "Epoch 3, Batch 670/2700 (24.81%), Loss: 0.2600\n",
      "Epoch 3, Batch 680/2700 (25.19%), Loss: 0.1871\n",
      "Epoch 3, Batch 690/2700 (25.56%), Loss: 3.0060\n",
      "Epoch 3, Batch 700/2700 (25.93%), Loss: 0.0852\n",
      "Epoch 3, Batch 710/2700 (26.30%), Loss: 1.5180\n",
      "Epoch 3, Batch 720/2700 (26.67%), Loss: 7.0966\n",
      "Epoch 3, Batch 730/2700 (27.04%), Loss: 13.7401\n",
      "Epoch 3, Batch 740/2700 (27.41%), Loss: 0.1586\n",
      "Epoch 3, Batch 750/2700 (27.78%), Loss: 5.1718\n",
      "Epoch 3, Batch 760/2700 (28.15%), Loss: 2.4474\n",
      "Epoch 3, Batch 770/2700 (28.52%), Loss: 6.9874\n",
      "Epoch 3, Batch 780/2700 (28.89%), Loss: 0.8841\n",
      "Epoch 3, Batch 790/2700 (29.26%), Loss: 15.9457\n",
      "Epoch 3, Batch 800/2700 (29.63%), Loss: 4.4175\n",
      "Epoch 3, Batch 810/2700 (30.00%), Loss: 0.4193\n",
      "Epoch 3, Batch 820/2700 (30.37%), Loss: 11.2083\n",
      "Epoch 3, Batch 830/2700 (30.74%), Loss: 1.0776\n",
      "Epoch 3, Batch 840/2700 (31.11%), Loss: 0.0254\n",
      "Epoch 3, Batch 850/2700 (31.48%), Loss: 11.4662\n",
      "Epoch 3, Batch 860/2700 (31.85%), Loss: 1.4819\n",
      "Epoch 3, Batch 870/2700 (32.22%), Loss: 11.4520\n",
      "Epoch 3, Batch 880/2700 (32.59%), Loss: 10.4150\n",
      "Epoch 3, Batch 890/2700 (32.96%), Loss: 9.9366\n",
      "Epoch 3, Batch 900/2700 (33.33%), Loss: 4.4229\n",
      "Epoch 3, Batch 910/2700 (33.70%), Loss: 0.3321\n",
      "Epoch 3, Batch 920/2700 (34.07%), Loss: 0.5233\n",
      "Epoch 3, Batch 930/2700 (34.44%), Loss: 1.5841\n",
      "Epoch 3, Batch 940/2700 (34.81%), Loss: 3.6276\n",
      "Epoch 3, Batch 950/2700 (35.19%), Loss: 2.8467\n",
      "Epoch 3, Batch 960/2700 (35.56%), Loss: 11.6607\n",
      "Epoch 3, Batch 970/2700 (35.93%), Loss: 2.3593\n",
      "Epoch 3, Batch 980/2700 (36.30%), Loss: 0.8833\n",
      "Epoch 3, Batch 990/2700 (36.67%), Loss: 5.9037\n",
      "Epoch 3, Batch 1000/2700 (37.04%), Loss: 0.3469\n",
      "Epoch 3, Batch 1010/2700 (37.41%), Loss: 0.0184\n",
      "Epoch 3, Batch 1020/2700 (37.78%), Loss: 5.3151\n",
      "Epoch 3, Batch 1030/2700 (38.15%), Loss: 0.3878\n",
      "Epoch 3, Batch 1040/2700 (38.52%), Loss: 7.4738\n",
      "Epoch 3, Batch 1050/2700 (38.89%), Loss: 0.5734\n",
      "Epoch 3, Batch 1060/2700 (39.26%), Loss: 5.6572\n",
      "Epoch 3, Batch 1070/2700 (39.63%), Loss: 1.8906\n",
      "Epoch 3, Batch 1080/2700 (40.00%), Loss: 0.1739\n",
      "Epoch 3, Batch 1090/2700 (40.37%), Loss: 0.6263\n",
      "Epoch 3, Batch 1100/2700 (40.74%), Loss: 3.2941\n",
      "Epoch 3, Batch 1110/2700 (41.11%), Loss: 0.3951\n",
      "Epoch 3, Batch 1120/2700 (41.48%), Loss: 4.2425\n",
      "Epoch 3, Batch 1130/2700 (41.85%), Loss: 0.0388\n",
      "Epoch 3, Batch 1140/2700 (42.22%), Loss: 0.5687\n",
      "Epoch 3, Batch 1150/2700 (42.59%), Loss: 9.8303\n",
      "Epoch 3, Batch 1160/2700 (42.96%), Loss: 0.1112\n",
      "Epoch 3, Batch 1170/2700 (43.33%), Loss: 8.7607\n",
      "Epoch 3, Batch 1180/2700 (43.70%), Loss: 7.3688\n",
      "Epoch 3, Batch 1190/2700 (44.07%), Loss: 2.2108\n",
      "Epoch 3, Batch 1200/2700 (44.44%), Loss: 0.9943\n",
      "Epoch 3, Batch 1210/2700 (44.81%), Loss: 3.7655\n",
      "Epoch 3, Batch 1220/2700 (45.19%), Loss: 2.6546\n",
      "Epoch 3, Batch 1230/2700 (45.56%), Loss: 1.1184\n",
      "Epoch 3, Batch 1240/2700 (45.93%), Loss: 0.0195\n",
      "Epoch 3, Batch 1250/2700 (46.30%), Loss: 1.4292\n",
      "Epoch 3, Batch 1260/2700 (46.67%), Loss: 0.7632\n",
      "Epoch 3, Batch 1270/2700 (47.04%), Loss: 0.1066\n",
      "Epoch 3, Batch 1280/2700 (47.41%), Loss: 1.6548\n",
      "Epoch 3, Batch 1290/2700 (47.78%), Loss: 1.1412\n",
      "Epoch 3, Batch 1300/2700 (48.15%), Loss: 7.7066\n",
      "Epoch 3, Batch 1310/2700 (48.52%), Loss: 7.2564\n",
      "Epoch 3, Batch 1320/2700 (48.89%), Loss: 1.3816\n",
      "Epoch 3, Batch 1330/2700 (49.26%), Loss: 0.1439\n",
      "Epoch 3, Batch 1340/2700 (49.63%), Loss: 1.8438\n",
      "Epoch 3, Batch 1350/2700 (50.00%), Loss: 0.6512\n",
      "Epoch 3, Batch 1360/2700 (50.37%), Loss: 2.8591\n",
      "Epoch 3, Batch 1370/2700 (50.74%), Loss: 2.7816\n",
      "Epoch 3, Batch 1380/2700 (51.11%), Loss: 0.7919\n",
      "Epoch 3, Batch 1390/2700 (51.48%), Loss: 4.0058\n",
      "Epoch 3, Batch 1400/2700 (51.85%), Loss: 2.0714\n",
      "Epoch 3, Batch 1410/2700 (52.22%), Loss: 9.2615\n",
      "Epoch 3, Batch 1420/2700 (52.59%), Loss: 6.0067\n",
      "Epoch 3, Batch 1430/2700 (52.96%), Loss: 0.3674\n",
      "Epoch 3, Batch 1440/2700 (53.33%), Loss: 3.8494\n",
      "Epoch 3, Batch 1450/2700 (53.70%), Loss: 12.5554\n",
      "Epoch 3, Batch 1460/2700 (54.07%), Loss: 2.0967\n",
      "Epoch 3, Batch 1470/2700 (54.44%), Loss: 0.1216\n",
      "Epoch 3, Batch 1480/2700 (54.81%), Loss: 0.9532\n",
      "Epoch 3, Batch 1490/2700 (55.19%), Loss: 1.7321\n",
      "Epoch 3, Batch 1500/2700 (55.56%), Loss: 2.5430\n",
      "Epoch 3, Batch 1510/2700 (55.93%), Loss: 1.1384\n",
      "Epoch 3, Batch 1520/2700 (56.30%), Loss: 0.8487\n",
      "Epoch 3, Batch 1530/2700 (56.67%), Loss: 4.4121\n",
      "Epoch 3, Batch 1540/2700 (57.04%), Loss: 0.8565\n",
      "Epoch 3, Batch 1550/2700 (57.41%), Loss: 0.3461\n",
      "Epoch 3, Batch 1560/2700 (57.78%), Loss: 2.8011\n",
      "Epoch 3, Batch 1570/2700 (58.15%), Loss: 1.8054\n",
      "Epoch 3, Batch 1580/2700 (58.52%), Loss: 7.0125\n",
      "Epoch 3, Batch 1590/2700 (58.89%), Loss: 0.3510\n",
      "Epoch 3, Batch 1600/2700 (59.26%), Loss: 1.0582\n",
      "Epoch 3, Batch 1610/2700 (59.63%), Loss: 3.3810\n",
      "Epoch 3, Batch 1620/2700 (60.00%), Loss: 0.7697\n",
      "Epoch 3, Batch 1630/2700 (60.37%), Loss: 1.2811\n",
      "Epoch 3, Batch 1640/2700 (60.74%), Loss: 9.7582\n",
      "Epoch 3, Batch 1650/2700 (61.11%), Loss: 1.1504\n",
      "Epoch 3, Batch 1660/2700 (61.48%), Loss: 4.2694\n",
      "Epoch 3, Batch 1670/2700 (61.85%), Loss: 0.0720\n",
      "Epoch 3, Batch 1680/2700 (62.22%), Loss: 0.9598\n",
      "Epoch 3, Batch 1690/2700 (62.59%), Loss: 0.2593\n",
      "Epoch 3, Batch 1700/2700 (62.96%), Loss: 2.3544\n",
      "Epoch 3, Batch 1710/2700 (63.33%), Loss: 1.1304\n",
      "Epoch 3, Batch 1720/2700 (63.70%), Loss: 7.2590\n",
      "Epoch 3, Batch 1730/2700 (64.07%), Loss: 1.2733\n",
      "Epoch 3, Batch 1740/2700 (64.44%), Loss: 0.1715\n",
      "Epoch 3, Batch 1750/2700 (64.81%), Loss: 1.6954\n",
      "Epoch 3, Batch 1760/2700 (65.19%), Loss: 1.6030\n",
      "Epoch 3, Batch 1770/2700 (65.56%), Loss: 3.0977\n",
      "Epoch 3, Batch 1780/2700 (65.93%), Loss: 6.5500\n",
      "Epoch 3, Batch 1790/2700 (66.30%), Loss: 0.8871\n",
      "Epoch 3, Batch 1800/2700 (66.67%), Loss: 0.1918\n",
      "Epoch 3, Batch 1810/2700 (67.04%), Loss: 11.6321\n",
      "Epoch 3, Batch 1820/2700 (67.41%), Loss: 1.2746\n",
      "Epoch 3, Batch 1830/2700 (67.78%), Loss: 0.0292\n",
      "Epoch 3, Batch 1840/2700 (68.15%), Loss: 6.0921\n",
      "Epoch 3, Batch 1850/2700 (68.52%), Loss: 1.8503\n",
      "Epoch 3, Batch 1860/2700 (68.89%), Loss: 4.7479\n",
      "Epoch 3, Batch 1870/2700 (69.26%), Loss: 5.0496\n",
      "Epoch 3, Batch 1880/2700 (69.63%), Loss: 0.0596\n",
      "Epoch 3, Batch 1890/2700 (70.00%), Loss: 7.4348\n",
      "Epoch 3, Batch 1900/2700 (70.37%), Loss: 4.6315\n",
      "Epoch 3, Batch 1910/2700 (70.74%), Loss: 0.4706\n",
      "Epoch 3, Batch 1920/2700 (71.11%), Loss: 4.8389\n",
      "Epoch 3, Batch 1930/2700 (71.48%), Loss: 3.8322\n",
      "Epoch 3, Batch 1940/2700 (71.85%), Loss: 0.1822\n",
      "Epoch 3, Batch 1950/2700 (72.22%), Loss: 7.6528\n",
      "Epoch 3, Batch 1960/2700 (72.59%), Loss: 0.1062\n",
      "Epoch 3, Batch 1970/2700 (72.96%), Loss: 0.0838\n",
      "Epoch 3, Batch 1980/2700 (73.33%), Loss: 2.4921\n",
      "Epoch 3, Batch 1990/2700 (73.70%), Loss: 0.0269\n",
      "Epoch 3, Batch 2000/2700 (74.07%), Loss: 4.7430\n",
      "Epoch 3, Batch 2010/2700 (74.44%), Loss: 1.8159\n",
      "Epoch 3, Batch 2020/2700 (74.81%), Loss: 1.1609\n",
      "Epoch 3, Batch 2030/2700 (75.19%), Loss: 0.4510\n",
      "Epoch 3, Batch 2040/2700 (75.56%), Loss: 0.1870\n",
      "Epoch 3, Batch 2050/2700 (75.93%), Loss: 7.3657\n",
      "Epoch 3, Batch 2060/2700 (76.30%), Loss: 1.4210\n",
      "Epoch 3, Batch 2070/2700 (76.67%), Loss: 3.5286\n",
      "Epoch 3, Batch 2080/2700 (77.04%), Loss: 1.2313\n",
      "Epoch 3, Batch 2090/2700 (77.41%), Loss: 0.0225\n",
      "Epoch 3, Batch 2100/2700 (77.78%), Loss: 0.0692\n",
      "Epoch 3, Batch 2110/2700 (78.15%), Loss: 3.2092\n",
      "Epoch 3, Batch 2120/2700 (78.52%), Loss: 0.1093\n",
      "Epoch 3, Batch 2130/2700 (78.89%), Loss: 0.4743\n",
      "Epoch 3, Batch 2140/2700 (79.26%), Loss: 5.9367\n",
      "Epoch 3, Batch 2150/2700 (79.63%), Loss: 0.3167\n",
      "Epoch 3, Batch 2160/2700 (80.00%), Loss: 3.8456\n",
      "Epoch 3, Batch 2170/2700 (80.37%), Loss: 0.0535\n",
      "Epoch 3, Batch 2180/2700 (80.74%), Loss: 4.4035\n",
      "Epoch 3, Batch 2190/2700 (81.11%), Loss: 1.3300\n",
      "Epoch 3, Batch 2200/2700 (81.48%), Loss: 0.0297\n",
      "Epoch 3, Batch 2210/2700 (81.85%), Loss: 2.0896\n",
      "Epoch 3, Batch 2220/2700 (82.22%), Loss: 1.3371\n",
      "Epoch 3, Batch 2230/2700 (82.59%), Loss: 0.0908\n",
      "Epoch 3, Batch 2240/2700 (82.96%), Loss: 0.1024\n",
      "Epoch 3, Batch 2250/2700 (83.33%), Loss: 1.2532\n",
      "Epoch 3, Batch 2260/2700 (83.70%), Loss: 0.4493\n",
      "Epoch 3, Batch 2270/2700 (84.07%), Loss: 0.8033\n",
      "Epoch 3, Batch 2280/2700 (84.44%), Loss: 2.5999\n",
      "Epoch 3, Batch 2290/2700 (84.81%), Loss: 0.2533\n",
      "Epoch 3, Batch 2300/2700 (85.19%), Loss: 0.8394\n",
      "Epoch 3, Batch 2310/2700 (85.56%), Loss: 1.7762\n",
      "Epoch 3, Batch 2320/2700 (85.93%), Loss: 9.2438\n",
      "Epoch 3, Batch 2330/2700 (86.30%), Loss: 5.2366\n",
      "Epoch 3, Batch 2340/2700 (86.67%), Loss: 2.8193\n",
      "Epoch 3, Batch 2350/2700 (87.04%), Loss: 0.4509\n",
      "Epoch 3, Batch 2360/2700 (87.41%), Loss: 0.4194\n",
      "Epoch 3, Batch 2370/2700 (87.78%), Loss: 1.7681\n",
      "Epoch 3, Batch 2380/2700 (88.15%), Loss: 51.3602\n",
      "Epoch 3, Batch 2390/2700 (88.52%), Loss: 11.1060\n",
      "Epoch 3, Batch 2400/2700 (88.89%), Loss: 2.7686\n",
      "Epoch 3, Batch 2410/2700 (89.26%), Loss: 7.9755\n",
      "Epoch 3, Batch 2420/2700 (89.63%), Loss: 8.9819\n",
      "Epoch 3, Batch 2430/2700 (90.00%), Loss: 5.1505\n",
      "Epoch 3, Batch 2440/2700 (90.37%), Loss: 1.4141\n",
      "Epoch 3, Batch 2450/2700 (90.74%), Loss: 1.6777\n",
      "Epoch 3, Batch 2460/2700 (91.11%), Loss: 0.9212\n",
      "Epoch 3, Batch 2470/2700 (91.48%), Loss: 1.3289\n",
      "Epoch 3, Batch 2480/2700 (91.85%), Loss: 0.5664\n",
      "Epoch 3, Batch 2490/2700 (92.22%), Loss: 4.8744\n",
      "Epoch 3, Batch 2500/2700 (92.59%), Loss: 1.6661\n",
      "Epoch 3, Batch 2510/2700 (92.96%), Loss: 0.5881\n",
      "Epoch 3, Batch 2520/2700 (93.33%), Loss: 12.7472\n",
      "Epoch 3, Batch 2530/2700 (93.70%), Loss: 4.8804\n",
      "Epoch 3, Batch 2540/2700 (94.07%), Loss: 0.0606\n",
      "Epoch 3, Batch 2550/2700 (94.44%), Loss: 0.0363\n",
      "Epoch 3, Batch 2560/2700 (94.81%), Loss: 2.0695\n",
      "Epoch 3, Batch 2570/2700 (95.19%), Loss: 1.4324\n",
      "Epoch 3, Batch 2580/2700 (95.56%), Loss: 0.7736\n",
      "Epoch 3, Batch 2590/2700 (95.93%), Loss: 13.3674\n",
      "Epoch 3, Batch 2600/2700 (96.30%), Loss: 1.0659\n",
      "Epoch 3, Batch 2610/2700 (96.67%), Loss: 0.8730\n",
      "Epoch 3, Batch 2620/2700 (97.04%), Loss: 6.6454\n",
      "Epoch 3, Batch 2630/2700 (97.41%), Loss: 0.7120\n",
      "Epoch 3, Batch 2640/2700 (97.78%), Loss: 5.3261\n",
      "Epoch 3, Batch 2650/2700 (98.15%), Loss: 0.7864\n",
      "Epoch 3, Batch 2660/2700 (98.52%), Loss: 11.5571\n",
      "Epoch 3, Batch 2670/2700 (98.89%), Loss: 1.4083\n",
      "Epoch 3, Batch 2680/2700 (99.26%), Loss: 0.9669\n",
      "Epoch 3, Batch 2690/2700 (99.63%), Loss: 1.7355\n",
      "Epoch 3, Batch 2700/2700 (100.00%), Loss: 3.8967\n",
      "Epoch 3 completed. Average loss: 3.1541\n",
      "Epoch 4, Batch 10/2700 (0.37%), Loss: 1.2883\n",
      "Epoch 4, Batch 20/2700 (0.74%), Loss: 0.1530\n",
      "Epoch 4, Batch 30/2700 (1.11%), Loss: 2.9111\n",
      "Epoch 4, Batch 40/2700 (1.48%), Loss: 0.3031\n",
      "Epoch 4, Batch 50/2700 (1.85%), Loss: 0.4650\n",
      "Epoch 4, Batch 60/2700 (2.22%), Loss: 1.7813\n",
      "Epoch 4, Batch 70/2700 (2.59%), Loss: 1.7453\n",
      "Epoch 4, Batch 80/2700 (2.96%), Loss: 0.1436\n",
      "Epoch 4, Batch 90/2700 (3.33%), Loss: 1.1985\n",
      "Epoch 4, Batch 100/2700 (3.70%), Loss: 4.4905\n",
      "Epoch 4, Batch 110/2700 (4.07%), Loss: 0.7124\n",
      "Epoch 4, Batch 120/2700 (4.44%), Loss: 2.3232\n",
      "Epoch 4, Batch 130/2700 (4.81%), Loss: 5.6409\n",
      "Epoch 4, Batch 140/2700 (5.19%), Loss: 0.3111\n",
      "Epoch 4, Batch 150/2700 (5.56%), Loss: 0.3044\n",
      "Epoch 4, Batch 160/2700 (5.93%), Loss: 0.1988\n",
      "Epoch 4, Batch 170/2700 (6.30%), Loss: 1.3384\n",
      "Epoch 4, Batch 180/2700 (6.67%), Loss: 3.5421\n",
      "Epoch 4, Batch 190/2700 (7.04%), Loss: 2.7878\n",
      "Epoch 4, Batch 200/2700 (7.41%), Loss: 0.2164\n",
      "Epoch 4, Batch 210/2700 (7.78%), Loss: 2.5855\n",
      "Epoch 4, Batch 220/2700 (8.15%), Loss: 6.1494\n",
      "Epoch 4, Batch 230/2700 (8.52%), Loss: 1.2009\n",
      "Epoch 4, Batch 240/2700 (8.89%), Loss: 4.5452\n",
      "Epoch 4, Batch 250/2700 (9.26%), Loss: 0.1192\n",
      "Epoch 4, Batch 260/2700 (9.63%), Loss: 6.9070\n",
      "Epoch 4, Batch 270/2700 (10.00%), Loss: 38.6474\n",
      "Epoch 4, Batch 280/2700 (10.37%), Loss: 0.1600\n",
      "Epoch 4, Batch 290/2700 (10.74%), Loss: 1.0253\n",
      "Epoch 4, Batch 300/2700 (11.11%), Loss: 0.5020\n",
      "Epoch 4, Batch 310/2700 (11.48%), Loss: 27.5055\n",
      "Epoch 4, Batch 320/2700 (11.85%), Loss: 1.1348\n",
      "Epoch 4, Batch 330/2700 (12.22%), Loss: 4.6508\n",
      "Epoch 4, Batch 340/2700 (12.59%), Loss: 5.0489\n",
      "Epoch 4, Batch 350/2700 (12.96%), Loss: 0.6929\n",
      "Epoch 4, Batch 360/2700 (13.33%), Loss: 0.8223\n",
      "Epoch 4, Batch 370/2700 (13.70%), Loss: 2.2383\n",
      "Epoch 4, Batch 380/2700 (14.07%), Loss: 4.8144\n",
      "Epoch 4, Batch 390/2700 (14.44%), Loss: 0.1217\n",
      "Epoch 4, Batch 400/2700 (14.81%), Loss: 2.3713\n",
      "Epoch 4, Batch 410/2700 (15.19%), Loss: 7.5443\n",
      "Epoch 4, Batch 420/2700 (15.56%), Loss: 0.0550\n",
      "Epoch 4, Batch 430/2700 (15.93%), Loss: 1.4509\n",
      "Epoch 4, Batch 440/2700 (16.30%), Loss: 1.6823\n",
      "Epoch 4, Batch 450/2700 (16.67%), Loss: 3.7086\n",
      "Epoch 4, Batch 460/2700 (17.04%), Loss: 1.6333\n",
      "Epoch 4, Batch 470/2700 (17.41%), Loss: 0.1549\n",
      "Epoch 4, Batch 480/2700 (17.78%), Loss: 1.6312\n",
      "Epoch 4, Batch 490/2700 (18.15%), Loss: 0.1108\n",
      "Epoch 4, Batch 500/2700 (18.52%), Loss: 0.1963\n",
      "Epoch 4, Batch 510/2700 (18.89%), Loss: 3.3809\n",
      "Epoch 4, Batch 520/2700 (19.26%), Loss: 0.5901\n",
      "Epoch 4, Batch 530/2700 (19.63%), Loss: 4.4690\n",
      "Epoch 4, Batch 540/2700 (20.00%), Loss: 0.2537\n",
      "Epoch 4, Batch 550/2700 (20.37%), Loss: 0.3583\n",
      "Epoch 4, Batch 560/2700 (20.74%), Loss: 1.6381\n",
      "Epoch 4, Batch 570/2700 (21.11%), Loss: 1.0283\n",
      "Epoch 4, Batch 580/2700 (21.48%), Loss: 1.7509\n",
      "Epoch 4, Batch 590/2700 (21.85%), Loss: 2.3674\n",
      "Epoch 4, Batch 600/2700 (22.22%), Loss: 2.2636\n",
      "Epoch 4, Batch 610/2700 (22.59%), Loss: 0.8576\n",
      "Epoch 4, Batch 620/2700 (22.96%), Loss: 0.1851\n",
      "Epoch 4, Batch 630/2700 (23.33%), Loss: 0.9680\n",
      "Epoch 4, Batch 640/2700 (23.70%), Loss: 3.4921\n",
      "Epoch 4, Batch 650/2700 (24.07%), Loss: 2.6839\n",
      "Epoch 4, Batch 660/2700 (24.44%), Loss: 8.4632\n",
      "Epoch 4, Batch 670/2700 (24.81%), Loss: 0.1337\n",
      "Epoch 4, Batch 680/2700 (25.19%), Loss: 0.1046\n",
      "Epoch 4, Batch 690/2700 (25.56%), Loss: 2.2746\n",
      "Epoch 4, Batch 700/2700 (25.93%), Loss: 0.0601\n",
      "Epoch 4, Batch 710/2700 (26.30%), Loss: 0.8968\n",
      "Epoch 4, Batch 720/2700 (26.67%), Loss: 5.6434\n",
      "Epoch 4, Batch 730/2700 (27.04%), Loss: 10.4084\n",
      "Epoch 4, Batch 740/2700 (27.41%), Loss: 0.0660\n",
      "Epoch 4, Batch 750/2700 (27.78%), Loss: 2.1577\n",
      "Epoch 4, Batch 760/2700 (28.15%), Loss: 1.4179\n",
      "Epoch 4, Batch 770/2700 (28.52%), Loss: 4.9741\n",
      "Epoch 4, Batch 780/2700 (28.89%), Loss: 0.3461\n",
      "Epoch 4, Batch 790/2700 (29.26%), Loss: 9.3839\n",
      "Epoch 4, Batch 800/2700 (29.63%), Loss: 2.7690\n",
      "Epoch 4, Batch 810/2700 (30.00%), Loss: 0.2189\n",
      "Epoch 4, Batch 820/2700 (30.37%), Loss: 7.3020\n",
      "Epoch 4, Batch 830/2700 (30.74%), Loss: 1.1455\n",
      "Epoch 4, Batch 840/2700 (31.11%), Loss: 0.0095\n",
      "Epoch 4, Batch 850/2700 (31.48%), Loss: 10.1525\n",
      "Epoch 4, Batch 860/2700 (31.85%), Loss: 0.6053\n",
      "Epoch 4, Batch 870/2700 (32.22%), Loss: 6.5855\n",
      "Epoch 4, Batch 880/2700 (32.59%), Loss: 10.0383\n",
      "Epoch 4, Batch 890/2700 (32.96%), Loss: 10.3083\n",
      "Epoch 4, Batch 900/2700 (33.33%), Loss: 1.9443\n",
      "Epoch 4, Batch 910/2700 (33.70%), Loss: 0.1743\n",
      "Epoch 4, Batch 920/2700 (34.07%), Loss: 0.3730\n",
      "Epoch 4, Batch 930/2700 (34.44%), Loss: 1.2983\n",
      "Epoch 4, Batch 940/2700 (34.81%), Loss: 3.1960\n",
      "Epoch 4, Batch 950/2700 (35.19%), Loss: 2.0286\n",
      "Epoch 4, Batch 960/2700 (35.56%), Loss: 10.5347\n",
      "Epoch 4, Batch 970/2700 (35.93%), Loss: 2.0767\n",
      "Epoch 4, Batch 980/2700 (36.30%), Loss: 0.3275\n",
      "Epoch 4, Batch 990/2700 (36.67%), Loss: 3.5149\n",
      "Epoch 4, Batch 1000/2700 (37.04%), Loss: 0.2159\n",
      "Epoch 4, Batch 1010/2700 (37.41%), Loss: 0.0108\n",
      "Epoch 4, Batch 1020/2700 (37.78%), Loss: 4.2167\n",
      "Epoch 4, Batch 1030/2700 (38.15%), Loss: 0.3311\n",
      "Epoch 4, Batch 1040/2700 (38.52%), Loss: 5.3797\n",
      "Epoch 4, Batch 1050/2700 (38.89%), Loss: 0.5184\n",
      "Epoch 4, Batch 1060/2700 (39.26%), Loss: 3.9529\n",
      "Epoch 4, Batch 1070/2700 (39.63%), Loss: 1.2551\n",
      "Epoch 4, Batch 1080/2700 (40.00%), Loss: 0.0935\n",
      "Epoch 4, Batch 1090/2700 (40.37%), Loss: 0.5152\n",
      "Epoch 4, Batch 1100/2700 (40.74%), Loss: 3.3710\n",
      "Epoch 4, Batch 1110/2700 (41.11%), Loss: 0.2535\n",
      "Epoch 4, Batch 1120/2700 (41.48%), Loss: 2.8934\n",
      "Epoch 4, Batch 1130/2700 (41.85%), Loss: 0.0175\n",
      "Epoch 4, Batch 1140/2700 (42.22%), Loss: 0.2896\n",
      "Epoch 4, Batch 1150/2700 (42.59%), Loss: 5.9579\n",
      "Epoch 4, Batch 1160/2700 (42.96%), Loss: 0.0250\n",
      "Epoch 4, Batch 1170/2700 (43.33%), Loss: 5.5956\n",
      "Epoch 4, Batch 1180/2700 (43.70%), Loss: 4.6973\n",
      "Epoch 4, Batch 1190/2700 (44.07%), Loss: 1.9012\n",
      "Epoch 4, Batch 1200/2700 (44.44%), Loss: 0.8605\n",
      "Epoch 4, Batch 1210/2700 (44.81%), Loss: 2.8480\n",
      "Epoch 4, Batch 1220/2700 (45.19%), Loss: 2.1459\n",
      "Epoch 4, Batch 1230/2700 (45.56%), Loss: 0.8780\n",
      "Epoch 4, Batch 1240/2700 (45.93%), Loss: 0.0088\n",
      "Epoch 4, Batch 1250/2700 (46.30%), Loss: 0.6550\n",
      "Epoch 4, Batch 1260/2700 (46.67%), Loss: 0.8152\n",
      "Epoch 4, Batch 1270/2700 (47.04%), Loss: 0.0536\n",
      "Epoch 4, Batch 1280/2700 (47.41%), Loss: 1.1700\n",
      "Epoch 4, Batch 1290/2700 (47.78%), Loss: 0.5707\n",
      "Epoch 4, Batch 1300/2700 (48.15%), Loss: 4.8887\n",
      "Epoch 4, Batch 1310/2700 (48.52%), Loss: 6.3246\n",
      "Epoch 4, Batch 1320/2700 (48.89%), Loss: 1.2630\n",
      "Epoch 4, Batch 1330/2700 (49.26%), Loss: 0.0769\n",
      "Epoch 4, Batch 1340/2700 (49.63%), Loss: 1.1346\n",
      "Epoch 4, Batch 1350/2700 (50.00%), Loss: 0.3538\n",
      "Epoch 4, Batch 1360/2700 (50.37%), Loss: 2.0100\n",
      "Epoch 4, Batch 1370/2700 (50.74%), Loss: 1.7320\n",
      "Epoch 4, Batch 1380/2700 (51.11%), Loss: 0.6190\n",
      "Epoch 4, Batch 1390/2700 (51.48%), Loss: 3.2001\n",
      "Epoch 4, Batch 1400/2700 (51.85%), Loss: 1.3527\n",
      "Epoch 4, Batch 1410/2700 (52.22%), Loss: 6.3604\n",
      "Epoch 4, Batch 1420/2700 (52.59%), Loss: 4.0090\n",
      "Epoch 4, Batch 1430/2700 (52.96%), Loss: 0.1579\n",
      "Epoch 4, Batch 1440/2700 (53.33%), Loss: 3.2385\n",
      "Epoch 4, Batch 1450/2700 (53.70%), Loss: 10.2280\n",
      "Epoch 4, Batch 1460/2700 (54.07%), Loss: 1.4189\n",
      "Epoch 4, Batch 1470/2700 (54.44%), Loss: 0.0529\n",
      "Epoch 4, Batch 1480/2700 (54.81%), Loss: 0.5330\n",
      "Epoch 4, Batch 1490/2700 (55.19%), Loss: 1.4077\n",
      "Epoch 4, Batch 1500/2700 (55.56%), Loss: 1.8026\n",
      "Epoch 4, Batch 1510/2700 (55.93%), Loss: 0.7788\n",
      "Epoch 4, Batch 1520/2700 (56.30%), Loss: 0.5609\n",
      "Epoch 4, Batch 1530/2700 (56.67%), Loss: 2.8354\n",
      "Epoch 4, Batch 1540/2700 (57.04%), Loss: 0.6979\n",
      "Epoch 4, Batch 1550/2700 (57.41%), Loss: 0.1554\n",
      "Epoch 4, Batch 1560/2700 (57.78%), Loss: 1.8542\n",
      "Epoch 4, Batch 1570/2700 (58.15%), Loss: 0.9514\n",
      "Epoch 4, Batch 1580/2700 (58.52%), Loss: 6.2576\n",
      "Epoch 4, Batch 1590/2700 (58.89%), Loss: 0.2365\n",
      "Epoch 4, Batch 1600/2700 (59.26%), Loss: 0.6425\n",
      "Epoch 4, Batch 1610/2700 (59.63%), Loss: 2.0302\n",
      "Epoch 4, Batch 1620/2700 (60.00%), Loss: 0.3813\n",
      "Epoch 4, Batch 1630/2700 (60.37%), Loss: 0.6588\n",
      "Epoch 4, Batch 1640/2700 (60.74%), Loss: 8.2699\n",
      "Epoch 4, Batch 1650/2700 (61.11%), Loss: 0.9155\n",
      "Epoch 4, Batch 1660/2700 (61.48%), Loss: 2.4633\n",
      "Epoch 4, Batch 1670/2700 (61.85%), Loss: 0.0635\n",
      "Epoch 4, Batch 1680/2700 (62.22%), Loss: 0.6044\n",
      "Epoch 4, Batch 1690/2700 (62.59%), Loss: 0.1105\n",
      "Epoch 4, Batch 1700/2700 (62.96%), Loss: 1.2849\n",
      "Epoch 4, Batch 1710/2700 (63.33%), Loss: 0.4285\n",
      "Epoch 4, Batch 1720/2700 (63.70%), Loss: 5.6256\n",
      "Epoch 4, Batch 1730/2700 (64.07%), Loss: 0.9370\n",
      "Epoch 4, Batch 1740/2700 (64.44%), Loss: 0.0959\n",
      "Epoch 4, Batch 1750/2700 (64.81%), Loss: 1.2669\n",
      "Epoch 4, Batch 1760/2700 (65.19%), Loss: 0.7711\n",
      "Epoch 4, Batch 1770/2700 (65.56%), Loss: 2.6156\n",
      "Epoch 4, Batch 1780/2700 (65.93%), Loss: 4.7335\n",
      "Epoch 4, Batch 1790/2700 (66.30%), Loss: 0.4010\n",
      "Epoch 4, Batch 1800/2700 (66.67%), Loss: 0.1075\n",
      "Epoch 4, Batch 1810/2700 (67.04%), Loss: 9.1348\n",
      "Epoch 4, Batch 1820/2700 (67.41%), Loss: 0.7587\n",
      "Epoch 4, Batch 1830/2700 (67.78%), Loss: 0.0250\n",
      "Epoch 4, Batch 1840/2700 (68.15%), Loss: 4.7520\n",
      "Epoch 4, Batch 1850/2700 (68.52%), Loss: 1.0564\n",
      "Epoch 4, Batch 1860/2700 (68.89%), Loss: 2.1128\n",
      "Epoch 4, Batch 1870/2700 (69.26%), Loss: 3.1874\n",
      "Epoch 4, Batch 1880/2700 (69.63%), Loss: 0.0200\n",
      "Epoch 4, Batch 1890/2700 (70.00%), Loss: 4.6286\n",
      "Epoch 4, Batch 1900/2700 (70.37%), Loss: 3.4820\n",
      "Epoch 4, Batch 1910/2700 (70.74%), Loss: 0.2670\n",
      "Epoch 4, Batch 1920/2700 (71.11%), Loss: 3.9183\n",
      "Epoch 4, Batch 1930/2700 (71.48%), Loss: 3.1799\n",
      "Epoch 4, Batch 1940/2700 (71.85%), Loss: 0.0848\n",
      "Epoch 4, Batch 1950/2700 (72.22%), Loss: 3.7444\n",
      "Epoch 4, Batch 1960/2700 (72.59%), Loss: 0.0695\n",
      "Epoch 4, Batch 1970/2700 (72.96%), Loss: 0.0634\n",
      "Epoch 4, Batch 1980/2700 (73.33%), Loss: 1.7290\n",
      "Epoch 4, Batch 1990/2700 (73.70%), Loss: 0.0117\n",
      "Epoch 4, Batch 2000/2700 (74.07%), Loss: 3.0172\n",
      "Epoch 4, Batch 2010/2700 (74.44%), Loss: 0.4641\n",
      "Epoch 4, Batch 2020/2700 (74.81%), Loss: 0.5544\n",
      "Epoch 4, Batch 2030/2700 (75.19%), Loss: 0.2390\n",
      "Epoch 4, Batch 2040/2700 (75.56%), Loss: 0.1136\n",
      "Epoch 4, Batch 2050/2700 (75.93%), Loss: 5.4440\n",
      "Epoch 4, Batch 2060/2700 (76.30%), Loss: 0.9411\n",
      "Epoch 4, Batch 2070/2700 (76.67%), Loss: 2.3585\n",
      "Epoch 4, Batch 2080/2700 (77.04%), Loss: 1.0049\n",
      "Epoch 4, Batch 2090/2700 (77.41%), Loss: 0.0166\n",
      "Epoch 4, Batch 2100/2700 (77.78%), Loss: 0.1057\n",
      "Epoch 4, Batch 2110/2700 (78.15%), Loss: 1.7336\n",
      "Epoch 4, Batch 2120/2700 (78.52%), Loss: 0.0352\n",
      "Epoch 4, Batch 2130/2700 (78.89%), Loss: 0.2985\n",
      "Epoch 4, Batch 2140/2700 (79.26%), Loss: 4.9318\n",
      "Epoch 4, Batch 2150/2700 (79.63%), Loss: 0.2644\n",
      "Epoch 4, Batch 2160/2700 (80.00%), Loss: 2.9333\n",
      "Epoch 4, Batch 2170/2700 (80.37%), Loss: 0.0230\n",
      "Epoch 4, Batch 2180/2700 (80.74%), Loss: 2.5186\n",
      "Epoch 4, Batch 2190/2700 (81.11%), Loss: 0.5714\n",
      "Epoch 4, Batch 2200/2700 (81.48%), Loss: 0.0069\n",
      "Epoch 4, Batch 2210/2700 (81.85%), Loss: 0.9970\n",
      "Epoch 4, Batch 2220/2700 (82.22%), Loss: 0.6412\n",
      "Epoch 4, Batch 2230/2700 (82.59%), Loss: 0.0832\n",
      "Epoch 4, Batch 2240/2700 (82.96%), Loss: 0.0488\n",
      "Epoch 4, Batch 2250/2700 (83.33%), Loss: 0.6661\n",
      "Epoch 4, Batch 2260/2700 (83.70%), Loss: 0.3007\n",
      "Epoch 4, Batch 2270/2700 (84.07%), Loss: 0.5092\n",
      "Epoch 4, Batch 2280/2700 (84.44%), Loss: 1.2610\n",
      "Epoch 4, Batch 2290/2700 (84.81%), Loss: 0.1374\n",
      "Epoch 4, Batch 2300/2700 (85.19%), Loss: 0.3657\n",
      "Epoch 4, Batch 2310/2700 (85.56%), Loss: 0.8771\n",
      "Epoch 4, Batch 2320/2700 (85.93%), Loss: 7.4594\n",
      "Epoch 4, Batch 2330/2700 (86.30%), Loss: 3.6989\n",
      "Epoch 4, Batch 2340/2700 (86.67%), Loss: 1.3751\n",
      "Epoch 4, Batch 2350/2700 (87.04%), Loss: 0.3118\n",
      "Epoch 4, Batch 2360/2700 (87.41%), Loss: 0.2180\n",
      "Epoch 4, Batch 2370/2700 (87.78%), Loss: 0.8309\n",
      "Epoch 4, Batch 2380/2700 (88.15%), Loss: 43.3145\n",
      "Epoch 4, Batch 2390/2700 (88.52%), Loss: 5.8064\n",
      "Epoch 4, Batch 2400/2700 (88.89%), Loss: 1.0282\n",
      "Epoch 4, Batch 2410/2700 (89.26%), Loss: 7.1589\n",
      "Epoch 4, Batch 2420/2700 (89.63%), Loss: 6.3738\n",
      "Epoch 4, Batch 2430/2700 (90.00%), Loss: 2.6125\n",
      "Epoch 4, Batch 2440/2700 (90.37%), Loss: 0.8807\n",
      "Epoch 4, Batch 2450/2700 (90.74%), Loss: 0.9645\n",
      "Epoch 4, Batch 2460/2700 (91.11%), Loss: 0.6125\n",
      "Epoch 4, Batch 2470/2700 (91.48%), Loss: 0.7238\n",
      "Epoch 4, Batch 2480/2700 (91.85%), Loss: 0.4887\n",
      "Epoch 4, Batch 2490/2700 (92.22%), Loss: 3.1134\n",
      "Epoch 4, Batch 2500/2700 (92.59%), Loss: 0.7578\n",
      "Epoch 4, Batch 2510/2700 (92.96%), Loss: 0.4029\n",
      "Epoch 4, Batch 2520/2700 (93.33%), Loss: 8.5014\n",
      "Epoch 4, Batch 2530/2700 (93.70%), Loss: 2.6982\n",
      "Epoch 4, Batch 2540/2700 (94.07%), Loss: 0.0607\n",
      "Epoch 4, Batch 2550/2700 (94.44%), Loss: 0.0238\n",
      "Epoch 4, Batch 2560/2700 (94.81%), Loss: 1.6012\n",
      "Epoch 4, Batch 2570/2700 (95.19%), Loss: 1.6336\n",
      "Epoch 4, Batch 2580/2700 (95.56%), Loss: 0.2704\n",
      "Epoch 4, Batch 2590/2700 (95.93%), Loss: 9.5714\n",
      "Epoch 4, Batch 2600/2700 (96.30%), Loss: 0.5115\n",
      "Epoch 4, Batch 2610/2700 (96.67%), Loss: 0.5045\n",
      "Epoch 4, Batch 2620/2700 (97.04%), Loss: 3.9033\n",
      "Epoch 4, Batch 2630/2700 (97.41%), Loss: 0.3215\n",
      "Epoch 4, Batch 2640/2700 (97.78%), Loss: 3.7000\n",
      "Epoch 4, Batch 2650/2700 (98.15%), Loss: 0.5928\n",
      "Epoch 4, Batch 2660/2700 (98.52%), Loss: 8.3572\n",
      "Epoch 4, Batch 2670/2700 (98.89%), Loss: 0.9116\n",
      "Epoch 4, Batch 2680/2700 (99.26%), Loss: 0.5126\n",
      "Epoch 4, Batch 2690/2700 (99.63%), Loss: 0.9490\n",
      "Epoch 4, Batch 2700/2700 (100.00%), Loss: 2.2827\n",
      "Epoch 4 completed. Average loss: 2.1820\n",
      "Epoch 5, Batch 10/2700 (0.37%), Loss: 0.6821\n",
      "Epoch 5, Batch 20/2700 (0.74%), Loss: 0.0948\n",
      "Epoch 5, Batch 30/2700 (1.11%), Loss: 1.9432\n",
      "Epoch 5, Batch 40/2700 (1.48%), Loss: 0.2016\n",
      "Epoch 5, Batch 50/2700 (1.85%), Loss: 0.2306\n",
      "Epoch 5, Batch 60/2700 (2.22%), Loss: 1.8818\n",
      "Epoch 5, Batch 70/2700 (2.59%), Loss: 1.2186\n",
      "Epoch 5, Batch 80/2700 (2.96%), Loss: 0.1145\n",
      "Epoch 5, Batch 90/2700 (3.33%), Loss: 0.9533\n",
      "Epoch 5, Batch 100/2700 (3.70%), Loss: 2.8821\n",
      "Epoch 5, Batch 110/2700 (4.07%), Loss: 0.3433\n",
      "Epoch 5, Batch 120/2700 (4.44%), Loss: 0.8106\n",
      "Epoch 5, Batch 130/2700 (4.81%), Loss: 4.1050\n",
      "Epoch 5, Batch 140/2700 (5.19%), Loss: 0.2316\n",
      "Epoch 5, Batch 150/2700 (5.56%), Loss: 0.1187\n",
      "Epoch 5, Batch 160/2700 (5.93%), Loss: 0.1683\n",
      "Epoch 5, Batch 170/2700 (6.30%), Loss: 0.7545\n",
      "Epoch 5, Batch 180/2700 (6.67%), Loss: 1.9998\n",
      "Epoch 5, Batch 190/2700 (7.04%), Loss: 1.9365\n",
      "Epoch 5, Batch 200/2700 (7.41%), Loss: 0.0817\n",
      "Epoch 5, Batch 210/2700 (7.78%), Loss: 1.4277\n",
      "Epoch 5, Batch 220/2700 (8.15%), Loss: 3.6168\n",
      "Epoch 5, Batch 230/2700 (8.52%), Loss: 0.7664\n",
      "Epoch 5, Batch 240/2700 (8.89%), Loss: 2.9836\n",
      "Epoch 5, Batch 250/2700 (9.26%), Loss: 0.0314\n",
      "Epoch 5, Batch 260/2700 (9.63%), Loss: 5.6584\n",
      "Epoch 5, Batch 270/2700 (10.00%), Loss: 32.6977\n",
      "Epoch 5, Batch 280/2700 (10.37%), Loss: 0.1365\n",
      "Epoch 5, Batch 290/2700 (10.74%), Loss: 0.5272\n",
      "Epoch 5, Batch 300/2700 (11.11%), Loss: 0.2673\n",
      "Epoch 5, Batch 310/2700 (11.48%), Loss: 23.0415\n",
      "Epoch 5, Batch 320/2700 (11.85%), Loss: 0.6738\n",
      "Epoch 5, Batch 330/2700 (12.22%), Loss: 3.8692\n",
      "Epoch 5, Batch 340/2700 (12.59%), Loss: 3.2460\n",
      "Epoch 5, Batch 350/2700 (12.96%), Loss: 0.6470\n",
      "Epoch 5, Batch 360/2700 (13.33%), Loss: 0.6000\n",
      "Epoch 5, Batch 370/2700 (13.70%), Loss: 1.3279\n",
      "Epoch 5, Batch 380/2700 (14.07%), Loss: 4.6331\n",
      "Epoch 5, Batch 390/2700 (14.44%), Loss: 0.1384\n",
      "Epoch 5, Batch 400/2700 (14.81%), Loss: 1.2185\n",
      "Epoch 5, Batch 410/2700 (15.19%), Loss: 6.4031\n",
      "Epoch 5, Batch 420/2700 (15.56%), Loss: 0.0227\n",
      "Epoch 5, Batch 430/2700 (15.93%), Loss: 1.1478\n",
      "Epoch 5, Batch 440/2700 (16.30%), Loss: 0.9938\n",
      "Epoch 5, Batch 450/2700 (16.67%), Loss: 2.1056\n",
      "Epoch 5, Batch 460/2700 (17.04%), Loss: 1.0839\n",
      "Epoch 5, Batch 470/2700 (17.41%), Loss: 0.1221\n",
      "Epoch 5, Batch 480/2700 (17.78%), Loss: 1.1233\n",
      "Epoch 5, Batch 490/2700 (18.15%), Loss: 0.0970\n",
      "Epoch 5, Batch 500/2700 (18.52%), Loss: 0.1342\n",
      "Epoch 5, Batch 510/2700 (18.89%), Loss: 2.5995\n",
      "Epoch 5, Batch 520/2700 (19.26%), Loss: 0.3600\n",
      "Epoch 5, Batch 530/2700 (19.63%), Loss: 3.7753\n",
      "Epoch 5, Batch 540/2700 (20.00%), Loss: 0.1924\n",
      "Epoch 5, Batch 550/2700 (20.37%), Loss: 0.2087\n",
      "Epoch 5, Batch 560/2700 (20.74%), Loss: 1.2368\n",
      "Epoch 5, Batch 570/2700 (21.11%), Loss: 0.5794\n",
      "Epoch 5, Batch 580/2700 (21.48%), Loss: 1.0898\n",
      "Epoch 5, Batch 590/2700 (21.85%), Loss: 1.4823\n",
      "Epoch 5, Batch 600/2700 (22.22%), Loss: 1.4498\n",
      "Epoch 5, Batch 610/2700 (22.59%), Loss: 0.5749\n",
      "Epoch 5, Batch 620/2700 (22.96%), Loss: 0.1069\n",
      "Epoch 5, Batch 630/2700 (23.33%), Loss: 0.5790\n",
      "Epoch 5, Batch 640/2700 (23.70%), Loss: 3.0229\n",
      "Epoch 5, Batch 650/2700 (24.07%), Loss: 1.3688\n",
      "Epoch 5, Batch 660/2700 (24.44%), Loss: 7.5286\n",
      "Epoch 5, Batch 670/2700 (24.81%), Loss: 0.0610\n",
      "Epoch 5, Batch 680/2700 (25.19%), Loss: 0.0472\n",
      "Epoch 5, Batch 690/2700 (25.56%), Loss: 1.3808\n",
      "Epoch 5, Batch 700/2700 (25.93%), Loss: 0.0273\n",
      "Epoch 5, Batch 710/2700 (26.30%), Loss: 0.9515\n",
      "Epoch 5, Batch 720/2700 (26.67%), Loss: 4.3879\n",
      "Epoch 5, Batch 730/2700 (27.04%), Loss: 6.4965\n",
      "Epoch 5, Batch 740/2700 (27.41%), Loss: 0.0383\n",
      "Epoch 5, Batch 750/2700 (27.78%), Loss: 0.8736\n",
      "Epoch 5, Batch 760/2700 (28.15%), Loss: 0.6371\n",
      "Epoch 5, Batch 770/2700 (28.52%), Loss: 2.9772\n",
      "Epoch 5, Batch 780/2700 (28.89%), Loss: 0.1775\n",
      "Epoch 5, Batch 790/2700 (29.26%), Loss: 3.3512\n",
      "Epoch 5, Batch 800/2700 (29.63%), Loss: 1.9957\n",
      "Epoch 5, Batch 810/2700 (30.00%), Loss: 0.3737\n",
      "Epoch 5, Batch 820/2700 (30.37%), Loss: 5.3578\n",
      "Epoch 5, Batch 830/2700 (30.74%), Loss: 0.8685\n",
      "Epoch 5, Batch 840/2700 (31.11%), Loss: 0.0045\n",
      "Epoch 5, Batch 850/2700 (31.48%), Loss: 8.6597\n",
      "Epoch 5, Batch 860/2700 (31.85%), Loss: 0.1870\n",
      "Epoch 5, Batch 870/2700 (32.22%), Loss: 3.1469\n",
      "Epoch 5, Batch 880/2700 (32.59%), Loss: 10.3172\n",
      "Epoch 5, Batch 890/2700 (32.96%), Loss: 8.4556\n",
      "Epoch 5, Batch 900/2700 (33.33%), Loss: 0.9797\n",
      "Epoch 5, Batch 910/2700 (33.70%), Loss: 0.0565\n",
      "Epoch 5, Batch 920/2700 (34.07%), Loss: 0.2644\n",
      "Epoch 5, Batch 930/2700 (34.44%), Loss: 1.3375\n",
      "Epoch 5, Batch 940/2700 (34.81%), Loss: 2.8593\n",
      "Epoch 5, Batch 950/2700 (35.19%), Loss: 1.5699\n",
      "Epoch 5, Batch 960/2700 (35.56%), Loss: 9.3561\n",
      "Epoch 5, Batch 970/2700 (35.93%), Loss: 1.4126\n",
      "Epoch 5, Batch 980/2700 (36.30%), Loss: 0.1931\n",
      "Epoch 5, Batch 990/2700 (36.67%), Loss: 2.3226\n",
      "Epoch 5, Batch 1000/2700 (37.04%), Loss: 0.1011\n",
      "Epoch 5, Batch 1010/2700 (37.41%), Loss: 0.0048\n",
      "Epoch 5, Batch 1020/2700 (37.78%), Loss: 3.5440\n",
      "Epoch 5, Batch 1030/2700 (38.15%), Loss: 0.3503\n",
      "Epoch 5, Batch 1040/2700 (38.52%), Loss: 3.1203\n",
      "Epoch 5, Batch 1050/2700 (38.89%), Loss: 0.2245\n",
      "Epoch 5, Batch 1060/2700 (39.26%), Loss: 2.8856\n",
      "Epoch 5, Batch 1070/2700 (39.63%), Loss: 0.7432\n",
      "Epoch 5, Batch 1080/2700 (40.00%), Loss: 0.0505\n",
      "Epoch 5, Batch 1090/2700 (40.37%), Loss: 0.5458\n",
      "Epoch 5, Batch 1100/2700 (40.74%), Loss: 2.9265\n",
      "Epoch 5, Batch 1110/2700 (41.11%), Loss: 0.2214\n",
      "Epoch 5, Batch 1120/2700 (41.48%), Loss: 2.2440\n",
      "Epoch 5, Batch 1130/2700 (41.85%), Loss: 0.0089\n",
      "Epoch 5, Batch 1140/2700 (42.22%), Loss: 0.2073\n",
      "Epoch 5, Batch 1150/2700 (42.59%), Loss: 4.4338\n",
      "Epoch 5, Batch 1160/2700 (42.96%), Loss: 0.0056\n",
      "Epoch 5, Batch 1170/2700 (43.33%), Loss: 3.1926\n",
      "Epoch 5, Batch 1180/2700 (43.70%), Loss: 2.8120\n",
      "Epoch 5, Batch 1190/2700 (44.07%), Loss: 1.0378\n",
      "Epoch 5, Batch 1200/2700 (44.44%), Loss: 0.5711\n",
      "Epoch 5, Batch 1210/2700 (44.81%), Loss: 2.6078\n",
      "Epoch 5, Batch 1220/2700 (45.19%), Loss: 1.2654\n",
      "Epoch 5, Batch 1230/2700 (45.56%), Loss: 0.5209\n",
      "Epoch 5, Batch 1240/2700 (45.93%), Loss: 0.0057\n",
      "Epoch 5, Batch 1250/2700 (46.30%), Loss: 0.4141\n",
      "Epoch 5, Batch 1260/2700 (46.67%), Loss: 0.5854\n",
      "Epoch 5, Batch 1270/2700 (47.04%), Loss: 0.0458\n",
      "Epoch 5, Batch 1280/2700 (47.41%), Loss: 1.0850\n",
      "Epoch 5, Batch 1290/2700 (47.78%), Loss: 0.3177\n",
      "Epoch 5, Batch 1300/2700 (48.15%), Loss: 3.1973\n",
      "Epoch 5, Batch 1310/2700 (48.52%), Loss: 5.4250\n",
      "Epoch 5, Batch 1320/2700 (48.89%), Loss: 0.8503\n",
      "Epoch 5, Batch 1330/2700 (49.26%), Loss: 0.0345\n",
      "Epoch 5, Batch 1340/2700 (49.63%), Loss: 0.8183\n",
      "Epoch 5, Batch 1350/2700 (50.00%), Loss: 0.2225\n",
      "Epoch 5, Batch 1360/2700 (50.37%), Loss: 1.4993\n",
      "Epoch 5, Batch 1370/2700 (50.74%), Loss: 0.9348\n",
      "Epoch 5, Batch 1380/2700 (51.11%), Loss: 0.5067\n",
      "Epoch 5, Batch 1390/2700 (51.48%), Loss: 3.1924\n",
      "Epoch 5, Batch 1400/2700 (51.85%), Loss: 1.1142\n",
      "Epoch 5, Batch 1410/2700 (52.22%), Loss: 5.3127\n",
      "Epoch 5, Batch 1420/2700 (52.59%), Loss: 2.3701\n",
      "Epoch 5, Batch 1430/2700 (52.96%), Loss: 0.0924\n",
      "Epoch 5, Batch 1440/2700 (53.33%), Loss: 1.8814\n",
      "Epoch 5, Batch 1450/2700 (53.70%), Loss: 8.2172\n",
      "Epoch 5, Batch 1460/2700 (54.07%), Loss: 0.7075\n",
      "Epoch 5, Batch 1470/2700 (54.44%), Loss: 0.0192\n",
      "Epoch 5, Batch 1480/2700 (54.81%), Loss: 0.1708\n",
      "Epoch 5, Batch 1490/2700 (55.19%), Loss: 1.0187\n",
      "Epoch 5, Batch 1500/2700 (55.56%), Loss: 1.0685\n",
      "Epoch 5, Batch 1510/2700 (55.93%), Loss: 0.6418\n",
      "Epoch 5, Batch 1520/2700 (56.30%), Loss: 0.4573\n",
      "Epoch 5, Batch 1530/2700 (56.67%), Loss: 0.9664\n",
      "Epoch 5, Batch 1540/2700 (57.04%), Loss: 0.4512\n",
      "Epoch 5, Batch 1550/2700 (57.41%), Loss: 0.0630\n",
      "Epoch 5, Batch 1560/2700 (57.78%), Loss: 1.2427\n",
      "Epoch 5, Batch 1570/2700 (58.15%), Loss: 0.5092\n",
      "Epoch 5, Batch 1580/2700 (58.52%), Loss: 5.5432\n",
      "Epoch 5, Batch 1590/2700 (58.89%), Loss: 0.1792\n",
      "Epoch 5, Batch 1600/2700 (59.26%), Loss: 0.3525\n",
      "Epoch 5, Batch 1610/2700 (59.63%), Loss: 1.0829\n",
      "Epoch 5, Batch 1620/2700 (60.00%), Loss: 0.3501\n",
      "Epoch 5, Batch 1630/2700 (60.37%), Loss: 0.2292\n",
      "Epoch 5, Batch 1640/2700 (60.74%), Loss: 6.6140\n",
      "Epoch 5, Batch 1650/2700 (61.11%), Loss: 0.4301\n",
      "Epoch 5, Batch 1660/2700 (61.48%), Loss: 1.5790\n",
      "Epoch 5, Batch 1670/2700 (61.85%), Loss: 0.0352\n",
      "Epoch 5, Batch 1680/2700 (62.22%), Loss: 0.4729\n",
      "Epoch 5, Batch 1690/2700 (62.59%), Loss: 0.0634\n",
      "Epoch 5, Batch 1700/2700 (62.96%), Loss: 0.6029\n",
      "Epoch 5, Batch 1710/2700 (63.33%), Loss: 0.1443\n",
      "Epoch 5, Batch 1720/2700 (63.70%), Loss: 4.5132\n",
      "Epoch 5, Batch 1730/2700 (64.07%), Loss: 0.8354\n",
      "Epoch 5, Batch 1740/2700 (64.44%), Loss: 0.0705\n",
      "Epoch 5, Batch 1750/2700 (64.81%), Loss: 1.1773\n",
      "Epoch 5, Batch 1760/2700 (65.19%), Loss: 0.5805\n",
      "Epoch 5, Batch 1770/2700 (65.56%), Loss: 1.7020\n",
      "Epoch 5, Batch 1780/2700 (65.93%), Loss: 2.8237\n",
      "Epoch 5, Batch 1790/2700 (66.30%), Loss: 0.1585\n",
      "Epoch 5, Batch 1800/2700 (66.67%), Loss: 0.0662\n",
      "Epoch 5, Batch 1810/2700 (67.04%), Loss: 6.5323\n",
      "Epoch 5, Batch 1820/2700 (67.41%), Loss: 0.5732\n",
      "Epoch 5, Batch 1830/2700 (67.78%), Loss: 0.0282\n",
      "Epoch 5, Batch 1840/2700 (68.15%), Loss: 3.4819\n",
      "Epoch 5, Batch 1850/2700 (68.52%), Loss: 0.6733\n",
      "Epoch 5, Batch 1860/2700 (68.89%), Loss: 1.0360\n",
      "Epoch 5, Batch 1870/2700 (69.26%), Loss: 1.3742\n",
      "Epoch 5, Batch 1880/2700 (69.63%), Loss: 0.0108\n",
      "Epoch 5, Batch 1890/2700 (70.00%), Loss: 2.5487\n",
      "Epoch 5, Batch 1900/2700 (70.37%), Loss: 2.5430\n",
      "Epoch 5, Batch 1910/2700 (70.74%), Loss: 0.1833\n",
      "Epoch 5, Batch 1920/2700 (71.11%), Loss: 3.5880\n",
      "Epoch 5, Batch 1930/2700 (71.48%), Loss: 2.4971\n",
      "Epoch 5, Batch 1940/2700 (71.85%), Loss: 0.0796\n",
      "Epoch 5, Batch 1950/2700 (72.22%), Loss: 1.1660\n",
      "Epoch 5, Batch 1960/2700 (72.59%), Loss: 0.0440\n",
      "Epoch 5, Batch 1970/2700 (72.96%), Loss: 0.0420\n",
      "Epoch 5, Batch 1980/2700 (73.33%), Loss: 1.0979\n",
      "Epoch 5, Batch 1990/2700 (73.70%), Loss: 0.0044\n",
      "Epoch 5, Batch 2000/2700 (74.07%), Loss: 1.6684\n",
      "Epoch 5, Batch 2010/2700 (74.44%), Loss: 0.2775\n",
      "Epoch 5, Batch 2020/2700 (74.81%), Loss: 0.3599\n",
      "Epoch 5, Batch 2030/2700 (75.19%), Loss: 0.1521\n",
      "Epoch 5, Batch 2040/2700 (75.56%), Loss: 0.0661\n",
      "Epoch 5, Batch 2050/2700 (75.93%), Loss: 3.2130\n",
      "Epoch 5, Batch 2060/2700 (76.30%), Loss: 0.2422\n",
      "Epoch 5, Batch 2070/2700 (76.67%), Loss: 1.1973\n",
      "Epoch 5, Batch 2080/2700 (77.04%), Loss: 0.6456\n",
      "Epoch 5, Batch 2090/2700 (77.41%), Loss: 0.0076\n",
      "Epoch 5, Batch 2100/2700 (77.78%), Loss: 0.0550\n",
      "Epoch 5, Batch 2110/2700 (78.15%), Loss: 1.0346\n",
      "Epoch 5, Batch 2120/2700 (78.52%), Loss: 0.0151\n",
      "Epoch 5, Batch 2130/2700 (78.89%), Loss: 0.2557\n",
      "Epoch 5, Batch 2140/2700 (79.26%), Loss: 1.8193\n",
      "Epoch 5, Batch 2150/2700 (79.63%), Loss: 0.2418\n",
      "Epoch 5, Batch 2160/2700 (80.00%), Loss: 1.9714\n",
      "Epoch 5, Batch 2170/2700 (80.37%), Loss: 0.0140\n",
      "Epoch 5, Batch 2180/2700 (80.74%), Loss: 1.7220\n",
      "Epoch 5, Batch 2190/2700 (81.11%), Loss: 0.3303\n",
      "Epoch 5, Batch 2200/2700 (81.48%), Loss: 0.0059\n",
      "Epoch 5, Batch 2210/2700 (81.85%), Loss: 0.7062\n",
      "Epoch 5, Batch 2220/2700 (82.22%), Loss: 0.2814\n",
      "Epoch 5, Batch 2230/2700 (82.59%), Loss: 0.0925\n",
      "Epoch 5, Batch 2240/2700 (82.96%), Loss: 0.0338\n",
      "Epoch 5, Batch 2250/2700 (83.33%), Loss: 0.4675\n",
      "Epoch 5, Batch 2260/2700 (83.70%), Loss: 0.2313\n",
      "Epoch 5, Batch 2270/2700 (84.07%), Loss: 0.3159\n",
      "Epoch 5, Batch 2280/2700 (84.44%), Loss: 0.7644\n",
      "Epoch 5, Batch 2290/2700 (84.81%), Loss: 0.2747\n",
      "Epoch 5, Batch 2300/2700 (85.19%), Loss: 0.2116\n",
      "Epoch 5, Batch 2310/2700 (85.56%), Loss: 0.5277\n",
      "Epoch 5, Batch 2320/2700 (85.93%), Loss: 6.0472\n",
      "Epoch 5, Batch 2330/2700 (86.30%), Loss: 3.6064\n",
      "Epoch 5, Batch 2340/2700 (86.67%), Loss: 0.8549\n",
      "Epoch 5, Batch 2350/2700 (87.04%), Loss: 0.2733\n",
      "Epoch 5, Batch 2360/2700 (87.41%), Loss: 0.1536\n",
      "Epoch 5, Batch 2370/2700 (87.78%), Loss: 0.2442\n",
      "Epoch 5, Batch 2380/2700 (88.15%), Loss: 34.2695\n",
      "Epoch 5, Batch 2390/2700 (88.52%), Loss: 4.7289\n",
      "Epoch 5, Batch 2400/2700 (88.89%), Loss: 0.7471\n",
      "Epoch 5, Batch 2410/2700 (89.26%), Loss: 7.3766\n",
      "Epoch 5, Batch 2420/2700 (89.63%), Loss: 3.1534\n",
      "Epoch 5, Batch 2430/2700 (90.00%), Loss: 0.9087\n",
      "Epoch 5, Batch 2440/2700 (90.37%), Loss: 0.2053\n",
      "Epoch 5, Batch 2450/2700 (90.74%), Loss: 0.4731\n",
      "Epoch 5, Batch 2460/2700 (91.11%), Loss: 0.4678\n",
      "Epoch 5, Batch 2470/2700 (91.48%), Loss: 0.3639\n",
      "Epoch 5, Batch 2480/2700 (91.85%), Loss: 0.2579\n",
      "Epoch 5, Batch 2490/2700 (92.22%), Loss: 1.5746\n",
      "Epoch 5, Batch 2500/2700 (92.59%), Loss: 0.4071\n",
      "Epoch 5, Batch 2510/2700 (92.96%), Loss: 0.2065\n",
      "Epoch 5, Batch 2520/2700 (93.33%), Loss: 5.3421\n",
      "Epoch 5, Batch 2530/2700 (93.70%), Loss: 0.9257\n",
      "Epoch 5, Batch 2540/2700 (94.07%), Loss: 0.0418\n",
      "Epoch 5, Batch 2550/2700 (94.44%), Loss: 0.0132\n",
      "Epoch 5, Batch 2560/2700 (94.81%), Loss: 1.4410\n",
      "Epoch 5, Batch 2570/2700 (95.19%), Loss: 1.2137\n",
      "Epoch 5, Batch 2580/2700 (95.56%), Loss: 0.0901\n",
      "Epoch 5, Batch 2590/2700 (95.93%), Loss: 6.5702\n",
      "Epoch 5, Batch 2600/2700 (96.30%), Loss: 0.3244\n",
      "Epoch 5, Batch 2610/2700 (96.67%), Loss: 0.4368\n",
      "Epoch 5, Batch 2620/2700 (97.04%), Loss: 1.2710\n",
      "Epoch 5, Batch 2630/2700 (97.41%), Loss: 0.0712\n",
      "Epoch 5, Batch 2640/2700 (97.78%), Loss: 2.7015\n",
      "Epoch 5, Batch 2650/2700 (98.15%), Loss: 0.5020\n",
      "Epoch 5, Batch 2660/2700 (98.52%), Loss: 5.7789\n",
      "Epoch 5, Batch 2670/2700 (98.89%), Loss: 0.5907\n",
      "Epoch 5, Batch 2680/2700 (99.26%), Loss: 0.3393\n",
      "Epoch 5, Batch 2690/2700 (99.63%), Loss: 0.4463\n",
      "Epoch 5, Batch 2700/2700 (100.00%), Loss: 1.0551\n",
      "Epoch 5 completed. Average loss: 1.5175\n",
      "Epoch 6, Batch 10/2700 (0.37%), Loss: 0.4570\n",
      "Epoch 6, Batch 20/2700 (0.74%), Loss: 0.0453\n",
      "Epoch 6, Batch 30/2700 (1.11%), Loss: 0.7763\n",
      "Epoch 6, Batch 40/2700 (1.48%), Loss: 0.1102\n",
      "Epoch 6, Batch 50/2700 (1.85%), Loss: 0.1548\n",
      "Epoch 6, Batch 60/2700 (2.22%), Loss: 1.0690\n",
      "Epoch 6, Batch 70/2700 (2.59%), Loss: 0.6439\n",
      "Epoch 6, Batch 80/2700 (2.96%), Loss: 0.1191\n",
      "Epoch 6, Batch 90/2700 (3.33%), Loss: 0.6892\n",
      "Epoch 6, Batch 100/2700 (3.70%), Loss: 1.5233\n",
      "Epoch 6, Batch 110/2700 (4.07%), Loss: 0.1940\n",
      "Epoch 6, Batch 120/2700 (4.44%), Loss: 0.4308\n",
      "Epoch 6, Batch 130/2700 (4.81%), Loss: 3.0394\n",
      "Epoch 6, Batch 140/2700 (5.19%), Loss: 0.1827\n",
      "Epoch 6, Batch 150/2700 (5.56%), Loss: 0.0667\n",
      "Epoch 6, Batch 160/2700 (5.93%), Loss: 0.1440\n",
      "Epoch 6, Batch 170/2700 (6.30%), Loss: 0.5959\n",
      "Epoch 6, Batch 180/2700 (6.67%), Loss: 0.9248\n",
      "Epoch 6, Batch 190/2700 (7.04%), Loss: 2.4149\n",
      "Epoch 6, Batch 200/2700 (7.41%), Loss: 0.0445\n",
      "Epoch 6, Batch 210/2700 (7.78%), Loss: 0.9110\n",
      "Epoch 6, Batch 220/2700 (8.15%), Loss: 2.3383\n",
      "Epoch 6, Batch 230/2700 (8.52%), Loss: 0.8933\n",
      "Epoch 6, Batch 240/2700 (8.89%), Loss: 2.2380\n",
      "Epoch 6, Batch 250/2700 (9.26%), Loss: 0.0131\n",
      "Epoch 6, Batch 260/2700 (9.63%), Loss: 4.8579\n",
      "Epoch 6, Batch 270/2700 (10.00%), Loss: 26.2220\n",
      "Epoch 6, Batch 280/2700 (10.37%), Loss: 0.1289\n",
      "Epoch 6, Batch 290/2700 (10.74%), Loss: 0.3719\n",
      "Epoch 6, Batch 300/2700 (11.11%), Loss: 0.1345\n",
      "Epoch 6, Batch 310/2700 (11.48%), Loss: 18.0765\n",
      "Epoch 6, Batch 320/2700 (11.85%), Loss: 0.4562\n",
      "Epoch 6, Batch 330/2700 (12.22%), Loss: 2.8666\n",
      "Epoch 6, Batch 340/2700 (12.59%), Loss: 1.5935\n",
      "Epoch 6, Batch 350/2700 (12.96%), Loss: 0.5474\n",
      "Epoch 6, Batch 360/2700 (13.33%), Loss: 0.4150\n",
      "Epoch 6, Batch 370/2700 (13.70%), Loss: 0.8585\n",
      "Epoch 6, Batch 380/2700 (14.07%), Loss: 3.3041\n",
      "Epoch 6, Batch 390/2700 (14.44%), Loss: 0.0796\n",
      "Epoch 6, Batch 400/2700 (14.81%), Loss: 0.9653\n",
      "Epoch 6, Batch 410/2700 (15.19%), Loss: 5.5127\n",
      "Epoch 6, Batch 420/2700 (15.56%), Loss: 0.0186\n",
      "Epoch 6, Batch 430/2700 (15.93%), Loss: 1.0226\n",
      "Epoch 6, Batch 440/2700 (16.30%), Loss: 0.7638\n",
      "Epoch 6, Batch 450/2700 (16.67%), Loss: 1.4571\n",
      "Epoch 6, Batch 460/2700 (17.04%), Loss: 1.1531\n",
      "Epoch 6, Batch 470/2700 (17.41%), Loss: 0.0842\n",
      "Epoch 6, Batch 480/2700 (17.78%), Loss: 0.8047\n",
      "Epoch 6, Batch 490/2700 (18.15%), Loss: 0.0787\n",
      "Epoch 6, Batch 500/2700 (18.52%), Loss: 0.0737\n",
      "Epoch 6, Batch 510/2700 (18.89%), Loss: 1.9282\n",
      "Epoch 6, Batch 520/2700 (19.26%), Loss: 0.2451\n",
      "Epoch 6, Batch 530/2700 (19.63%), Loss: 3.1709\n",
      "Epoch 6, Batch 540/2700 (20.00%), Loss: 0.1327\n",
      "Epoch 6, Batch 550/2700 (20.37%), Loss: 0.1839\n",
      "Epoch 6, Batch 560/2700 (20.74%), Loss: 0.8680\n",
      "Epoch 6, Batch 570/2700 (21.11%), Loss: 0.3016\n",
      "Epoch 6, Batch 580/2700 (21.48%), Loss: 1.0534\n",
      "Epoch 6, Batch 590/2700 (21.85%), Loss: 0.7926\n",
      "Epoch 6, Batch 600/2700 (22.22%), Loss: 0.9377\n",
      "Epoch 6, Batch 610/2700 (22.59%), Loss: 0.3944\n",
      "Epoch 6, Batch 620/2700 (22.96%), Loss: 0.0829\n",
      "Epoch 6, Batch 630/2700 (23.33%), Loss: 0.3938\n",
      "Epoch 6, Batch 640/2700 (23.70%), Loss: 2.5661\n",
      "Epoch 6, Batch 650/2700 (24.07%), Loss: 0.7392\n",
      "Epoch 6, Batch 660/2700 (24.44%), Loss: 7.1666\n",
      "Epoch 6, Batch 670/2700 (24.81%), Loss: 0.0432\n",
      "Epoch 6, Batch 680/2700 (25.19%), Loss: 0.0259\n",
      "Epoch 6, Batch 690/2700 (25.56%), Loss: 0.8504\n",
      "Epoch 6, Batch 700/2700 (25.93%), Loss: 0.0165\n",
      "Epoch 6, Batch 710/2700 (26.30%), Loss: 0.9135\n",
      "Epoch 6, Batch 720/2700 (26.67%), Loss: 3.4718\n",
      "Epoch 6, Batch 730/2700 (27.04%), Loss: 3.7292\n",
      "Epoch 6, Batch 740/2700 (27.41%), Loss: 0.0248\n",
      "Epoch 6, Batch 750/2700 (27.78%), Loss: 0.5520\n",
      "Epoch 6, Batch 760/2700 (28.15%), Loss: 0.4075\n",
      "Epoch 6, Batch 770/2700 (28.52%), Loss: 1.5239\n",
      "Epoch 6, Batch 780/2700 (28.89%), Loss: 0.1767\n",
      "Epoch 6, Batch 790/2700 (29.26%), Loss: 1.9752\n",
      "Epoch 6, Batch 800/2700 (29.63%), Loss: 1.6962\n",
      "Epoch 6, Batch 810/2700 (30.00%), Loss: 0.1946\n",
      "Epoch 6, Batch 820/2700 (30.37%), Loss: 2.7440\n",
      "Epoch 6, Batch 830/2700 (30.74%), Loss: 0.4520\n",
      "Epoch 6, Batch 840/2700 (31.11%), Loss: 0.0049\n",
      "Epoch 6, Batch 850/2700 (31.48%), Loss: 7.4501\n",
      "Epoch 6, Batch 860/2700 (31.85%), Loss: 0.1218\n",
      "Epoch 6, Batch 870/2700 (32.22%), Loss: 1.4728\n",
      "Epoch 6, Batch 880/2700 (32.59%), Loss: 9.4973\n",
      "Epoch 6, Batch 890/2700 (32.96%), Loss: 5.8880\n",
      "Epoch 6, Batch 900/2700 (33.33%), Loss: 0.4267\n",
      "Epoch 6, Batch 910/2700 (33.70%), Loss: 0.0244\n",
      "Epoch 6, Batch 920/2700 (34.07%), Loss: 0.1955\n",
      "Epoch 6, Batch 930/2700 (34.44%), Loss: 0.6751\n",
      "Epoch 6, Batch 940/2700 (34.81%), Loss: 2.7669\n",
      "Epoch 6, Batch 950/2700 (35.19%), Loss: 1.5150\n",
      "Epoch 6, Batch 960/2700 (35.56%), Loss: 8.4169\n",
      "Epoch 6, Batch 970/2700 (35.93%), Loss: 1.2133\n",
      "Epoch 6, Batch 980/2700 (36.30%), Loss: 0.1113\n",
      "Epoch 6, Batch 990/2700 (36.67%), Loss: 1.9249\n",
      "Epoch 6, Batch 1000/2700 (37.04%), Loss: 0.0245\n",
      "Epoch 6, Batch 1010/2700 (37.41%), Loss: 0.0039\n",
      "Epoch 6, Batch 1020/2700 (37.78%), Loss: 2.9375\n",
      "Epoch 6, Batch 1030/2700 (38.15%), Loss: 0.4149\n",
      "Epoch 6, Batch 1040/2700 (38.52%), Loss: 1.7618\n",
      "Epoch 6, Batch 1050/2700 (38.89%), Loss: 0.1485\n",
      "Epoch 6, Batch 1060/2700 (39.26%), Loss: 2.0777\n",
      "Epoch 6, Batch 1070/2700 (39.63%), Loss: 0.2820\n",
      "Epoch 6, Batch 1080/2700 (40.00%), Loss: 0.0364\n",
      "Epoch 6, Batch 1090/2700 (40.37%), Loss: 0.4134\n",
      "Epoch 6, Batch 1100/2700 (40.74%), Loss: 2.4758\n",
      "Epoch 6, Batch 1110/2700 (41.11%), Loss: 0.1805\n",
      "Epoch 6, Batch 1120/2700 (41.48%), Loss: 1.6766\n",
      "Epoch 6, Batch 1130/2700 (41.85%), Loss: 0.0079\n",
      "Epoch 6, Batch 1140/2700 (42.22%), Loss: 0.1221\n",
      "Epoch 6, Batch 1150/2700 (42.59%), Loss: 3.2817\n",
      "Epoch 6, Batch 1160/2700 (42.96%), Loss: 0.0048\n",
      "Epoch 6, Batch 1170/2700 (43.33%), Loss: 1.8606\n",
      "Epoch 6, Batch 1180/2700 (43.70%), Loss: 1.7138\n",
      "Epoch 6, Batch 1190/2700 (44.07%), Loss: 0.6995\n",
      "Epoch 6, Batch 1200/2700 (44.44%), Loss: 0.2601\n",
      "Epoch 6, Batch 1210/2700 (44.81%), Loss: 1.8204\n",
      "Epoch 6, Batch 1220/2700 (45.19%), Loss: 0.6111\n",
      "Epoch 6, Batch 1230/2700 (45.56%), Loss: 0.3293\n",
      "Epoch 6, Batch 1240/2700 (45.93%), Loss: 0.0025\n",
      "Epoch 6, Batch 1250/2700 (46.30%), Loss: 0.7520\n",
      "Epoch 6, Batch 1260/2700 (46.67%), Loss: 0.4278\n",
      "Epoch 6, Batch 1270/2700 (47.04%), Loss: 0.0395\n",
      "Epoch 6, Batch 1280/2700 (47.41%), Loss: 0.9152\n",
      "Epoch 6, Batch 1290/2700 (47.78%), Loss: 0.1490\n",
      "Epoch 6, Batch 1300/2700 (48.15%), Loss: 2.0225\n",
      "Epoch 6, Batch 1310/2700 (48.52%), Loss: 4.2529\n",
      "Epoch 6, Batch 1320/2700 (48.89%), Loss: 0.6059\n",
      "Epoch 6, Batch 1330/2700 (49.26%), Loss: 0.0255\n",
      "Epoch 6, Batch 1340/2700 (49.63%), Loss: 0.6431\n",
      "Epoch 6, Batch 1350/2700 (50.00%), Loss: 0.1737\n",
      "Epoch 6, Batch 1360/2700 (50.37%), Loss: 1.1644\n",
      "Epoch 6, Batch 1370/2700 (50.74%), Loss: 0.5504\n",
      "Epoch 6, Batch 1380/2700 (51.11%), Loss: 0.4101\n",
      "Epoch 6, Batch 1390/2700 (51.48%), Loss: 2.9346\n",
      "Epoch 6, Batch 1400/2700 (51.85%), Loss: 0.8500\n",
      "Epoch 6, Batch 1410/2700 (52.22%), Loss: 3.6118\n",
      "Epoch 6, Batch 1420/2700 (52.59%), Loss: 1.3280\n",
      "Epoch 6, Batch 1430/2700 (52.96%), Loss: 0.1095\n",
      "Epoch 6, Batch 1440/2700 (53.33%), Loss: 1.1220\n",
      "Epoch 6, Batch 1450/2700 (53.70%), Loss: 6.1564\n",
      "Epoch 6, Batch 1460/2700 (54.07%), Loss: 0.3980\n",
      "Epoch 6, Batch 1470/2700 (54.44%), Loss: 0.0117\n",
      "Epoch 6, Batch 1480/2700 (54.81%), Loss: 0.0970\n",
      "Epoch 6, Batch 1490/2700 (55.19%), Loss: 0.7192\n",
      "Epoch 6, Batch 1500/2700 (55.56%), Loss: 0.4527\n",
      "Epoch 6, Batch 1510/2700 (55.93%), Loss: 0.5245\n",
      "Epoch 6, Batch 1520/2700 (56.30%), Loss: 0.3166\n",
      "Epoch 6, Batch 1530/2700 (56.67%), Loss: 0.3517\n",
      "Epoch 6, Batch 1540/2700 (57.04%), Loss: 0.2919\n",
      "Epoch 6, Batch 1550/2700 (57.41%), Loss: 0.0454\n",
      "Epoch 6, Batch 1560/2700 (57.78%), Loss: 1.0108\n",
      "Epoch 6, Batch 1570/2700 (58.15%), Loss: 0.2365\n",
      "Epoch 6, Batch 1580/2700 (58.52%), Loss: 4.3303\n",
      "Epoch 6, Batch 1590/2700 (58.89%), Loss: 0.0870\n",
      "Epoch 6, Batch 1600/2700 (59.26%), Loss: 0.2280\n",
      "Epoch 6, Batch 1610/2700 (59.63%), Loss: 0.6024\n",
      "Epoch 6, Batch 1620/2700 (60.00%), Loss: 0.1467\n",
      "Epoch 6, Batch 1630/2700 (60.37%), Loss: 0.1758\n",
      "Epoch 6, Batch 1640/2700 (60.74%), Loss: 4.9305\n",
      "Epoch 6, Batch 1650/2700 (61.11%), Loss: 0.2726\n",
      "Epoch 6, Batch 1660/2700 (61.48%), Loss: 1.0594\n",
      "Epoch 6, Batch 1670/2700 (61.85%), Loss: 0.0292\n",
      "Epoch 6, Batch 1680/2700 (62.22%), Loss: 0.3177\n",
      "Epoch 6, Batch 1690/2700 (62.59%), Loss: 0.0517\n",
      "Epoch 6, Batch 1700/2700 (62.96%), Loss: 0.3311\n",
      "Epoch 6, Batch 1710/2700 (63.33%), Loss: 0.0872\n",
      "Epoch 6, Batch 1720/2700 (63.70%), Loss: 4.0173\n",
      "Epoch 6, Batch 1730/2700 (64.07%), Loss: 0.5522\n",
      "Epoch 6, Batch 1740/2700 (64.44%), Loss: 0.0692\n",
      "Epoch 6, Batch 1750/2700 (64.81%), Loss: 1.0977\n",
      "Epoch 6, Batch 1760/2700 (65.19%), Loss: 0.4227\n",
      "Epoch 6, Batch 1770/2700 (65.56%), Loss: 1.0766\n",
      "Epoch 6, Batch 1780/2700 (65.93%), Loss: 1.9512\n",
      "Epoch 6, Batch 1790/2700 (66.30%), Loss: 0.0586\n",
      "Epoch 6, Batch 1800/2700 (66.67%), Loss: 0.0246\n",
      "Epoch 6, Batch 1810/2700 (67.04%), Loss: 4.2216\n",
      "Epoch 6, Batch 1820/2700 (67.41%), Loss: 0.3254\n",
      "Epoch 6, Batch 1830/2700 (67.78%), Loss: 0.0225\n",
      "Epoch 6, Batch 1840/2700 (68.15%), Loss: 2.4276\n",
      "Epoch 6, Batch 1850/2700 (68.52%), Loss: 0.4377\n",
      "Epoch 6, Batch 1860/2700 (68.89%), Loss: 0.5182\n",
      "Epoch 6, Batch 1870/2700 (69.26%), Loss: 0.6618\n",
      "Epoch 6, Batch 1880/2700 (69.63%), Loss: 0.0086\n",
      "Epoch 6, Batch 1890/2700 (70.00%), Loss: 1.5182\n",
      "Epoch 6, Batch 1900/2700 (70.37%), Loss: 1.4977\n",
      "Epoch 6, Batch 1910/2700 (70.74%), Loss: 0.1444\n",
      "Epoch 6, Batch 1920/2700 (71.11%), Loss: 3.4427\n",
      "Epoch 6, Batch 1930/2700 (71.48%), Loss: 1.8056\n",
      "Epoch 6, Batch 1940/2700 (71.85%), Loss: 0.0782\n",
      "Epoch 6, Batch 1950/2700 (72.22%), Loss: 0.6296\n",
      "Epoch 6, Batch 1960/2700 (72.59%), Loss: 0.0293\n",
      "Epoch 6, Batch 1970/2700 (72.96%), Loss: 0.0370\n",
      "Epoch 6, Batch 1980/2700 (73.33%), Loss: 0.7067\n",
      "Epoch 6, Batch 1990/2700 (73.70%), Loss: 0.0030\n",
      "Epoch 6, Batch 2000/2700 (74.07%), Loss: 0.8924\n",
      "Epoch 6, Batch 2010/2700 (74.44%), Loss: 0.1691\n",
      "Epoch 6, Batch 2020/2700 (74.81%), Loss: 0.2574\n",
      "Epoch 6, Batch 2030/2700 (75.19%), Loss: 0.0891\n",
      "Epoch 6, Batch 2040/2700 (75.56%), Loss: 0.0270\n",
      "Epoch 6, Batch 2050/2700 (75.93%), Loss: 1.4535\n",
      "Epoch 6, Batch 2060/2700 (76.30%), Loss: 0.1423\n",
      "Epoch 6, Batch 2070/2700 (76.67%), Loss: 0.7478\n",
      "Epoch 6, Batch 2080/2700 (77.04%), Loss: 0.4093\n",
      "Epoch 6, Batch 2090/2700 (77.41%), Loss: 0.0052\n",
      "Epoch 6, Batch 2100/2700 (77.78%), Loss: 0.0742\n",
      "Epoch 6, Batch 2110/2700 (78.15%), Loss: 0.7265\n",
      "Epoch 6, Batch 2120/2700 (78.52%), Loss: 0.0118\n",
      "Epoch 6, Batch 2130/2700 (78.89%), Loss: 0.2500\n",
      "Epoch 6, Batch 2140/2700 (79.26%), Loss: 0.5391\n",
      "Epoch 6, Batch 2150/2700 (79.63%), Loss: 0.2450\n",
      "Epoch 6, Batch 2160/2700 (80.00%), Loss: 1.1512\n",
      "Epoch 6, Batch 2170/2700 (80.37%), Loss: 0.0110\n",
      "Epoch 6, Batch 2180/2700 (80.74%), Loss: 1.2276\n",
      "Epoch 6, Batch 2190/2700 (81.11%), Loss: 0.2089\n",
      "Epoch 6, Batch 2200/2700 (81.48%), Loss: 0.0031\n",
      "Epoch 6, Batch 2210/2700 (81.85%), Loss: 0.7383\n",
      "Epoch 6, Batch 2220/2700 (82.22%), Loss: 0.1811\n",
      "Epoch 6, Batch 2230/2700 (82.59%), Loss: 0.0950\n",
      "Epoch 6, Batch 2240/2700 (82.96%), Loss: 0.0162\n",
      "Epoch 6, Batch 2250/2700 (83.33%), Loss: 0.4885\n",
      "Epoch 6, Batch 2260/2700 (83.70%), Loss: 0.1666\n",
      "Epoch 6, Batch 2270/2700 (84.07%), Loss: 0.1671\n",
      "Epoch 6, Batch 2280/2700 (84.44%), Loss: 0.6420\n",
      "Epoch 6, Batch 2290/2700 (84.81%), Loss: 0.1345\n",
      "Epoch 6, Batch 2300/2700 (85.19%), Loss: 0.1119\n",
      "Epoch 6, Batch 2310/2700 (85.56%), Loss: 0.3646\n",
      "Epoch 6, Batch 2320/2700 (85.93%), Loss: 5.3682\n",
      "Epoch 6, Batch 2330/2700 (86.30%), Loss: 2.8973\n",
      "Epoch 6, Batch 2340/2700 (86.67%), Loss: 0.5880\n",
      "Epoch 6, Batch 2350/2700 (87.04%), Loss: 0.2042\n",
      "Epoch 6, Batch 2360/2700 (87.41%), Loss: 0.1546\n",
      "Epoch 6, Batch 2370/2700 (87.78%), Loss: 0.1387\n",
      "Epoch 6, Batch 2380/2700 (88.15%), Loss: 27.0358\n",
      "Epoch 6, Batch 2390/2700 (88.52%), Loss: 4.7972\n",
      "Epoch 6, Batch 2400/2700 (88.89%), Loss: 0.4380\n",
      "Epoch 6, Batch 2410/2700 (89.26%), Loss: 7.5681\n",
      "Epoch 6, Batch 2420/2700 (89.63%), Loss: 1.8251\n",
      "Epoch 6, Batch 2430/2700 (90.00%), Loss: 0.3472\n",
      "Epoch 6, Batch 2440/2700 (90.37%), Loss: 0.1003\n",
      "Epoch 6, Batch 2450/2700 (90.74%), Loss: 0.3186\n",
      "Epoch 6, Batch 2460/2700 (91.11%), Loss: 0.4052\n",
      "Epoch 6, Batch 2470/2700 (91.48%), Loss: 0.2015\n",
      "Epoch 6, Batch 2480/2700 (91.85%), Loss: 0.0601\n",
      "Epoch 6, Batch 2490/2700 (92.22%), Loss: 1.0074\n",
      "Epoch 6, Batch 2500/2700 (92.59%), Loss: 0.2560\n",
      "Epoch 6, Batch 2510/2700 (92.96%), Loss: 0.1170\n",
      "Epoch 6, Batch 2520/2700 (93.33%), Loss: 2.0439\n",
      "Epoch 6, Batch 2530/2700 (93.70%), Loss: 0.5504\n",
      "Epoch 6, Batch 2540/2700 (94.07%), Loss: 0.0244\n",
      "Epoch 6, Batch 2550/2700 (94.44%), Loss: 0.0090\n",
      "Epoch 6, Batch 2560/2700 (94.81%), Loss: 0.8747\n",
      "Epoch 6, Batch 2570/2700 (95.19%), Loss: 0.9841\n",
      "Epoch 6, Batch 2580/2700 (95.56%), Loss: 0.0706\n",
      "Epoch 6, Batch 2590/2700 (95.93%), Loss: 3.9440\n",
      "Epoch 6, Batch 2600/2700 (96.30%), Loss: 0.2784\n",
      "Epoch 6, Batch 2610/2700 (96.67%), Loss: 0.3339\n",
      "Epoch 6, Batch 2620/2700 (97.04%), Loss: 0.7283\n",
      "Epoch 6, Batch 2630/2700 (97.41%), Loss: 0.0316\n",
      "Epoch 6, Batch 2640/2700 (97.78%), Loss: 1.3004\n",
      "Epoch 6, Batch 2650/2700 (98.15%), Loss: 0.5096\n",
      "Epoch 6, Batch 2660/2700 (98.52%), Loss: 3.8055\n",
      "Epoch 6, Batch 2670/2700 (98.89%), Loss: 0.3599\n",
      "Epoch 6, Batch 2680/2700 (99.26%), Loss: 0.1922\n",
      "Epoch 6, Batch 2690/2700 (99.63%), Loss: 0.2781\n",
      "Epoch 6, Batch 2700/2700 (100.00%), Loss: 0.7794\n",
      "Epoch 6 completed. Average loss: 1.0745\n",
      "Epoch 7, Batch 10/2700 (0.37%), Loss: 0.2082\n",
      "Epoch 7, Batch 20/2700 (0.74%), Loss: 0.0322\n",
      "Epoch 7, Batch 30/2700 (1.11%), Loss: 0.3152\n",
      "Epoch 7, Batch 40/2700 (1.48%), Loss: 0.0660\n",
      "Epoch 7, Batch 50/2700 (1.85%), Loss: 0.1557\n",
      "Epoch 7, Batch 60/2700 (2.22%), Loss: 0.4771\n",
      "Epoch 7, Batch 70/2700 (2.59%), Loss: 0.3907\n",
      "Epoch 7, Batch 80/2700 (2.96%), Loss: 0.1168\n",
      "Epoch 7, Batch 90/2700 (3.33%), Loss: 0.4054\n",
      "Epoch 7, Batch 100/2700 (3.70%), Loss: 1.1179\n",
      "Epoch 7, Batch 110/2700 (4.07%), Loss: 0.2046\n",
      "Epoch 7, Batch 120/2700 (4.44%), Loss: 0.2656\n",
      "Epoch 7, Batch 130/2700 (4.81%), Loss: 2.2869\n",
      "Epoch 7, Batch 140/2700 (5.19%), Loss: 0.1492\n",
      "Epoch 7, Batch 150/2700 (5.56%), Loss: 0.0276\n",
      "Epoch 7, Batch 160/2700 (5.93%), Loss: 0.1053\n",
      "Epoch 7, Batch 170/2700 (6.30%), Loss: 0.5779\n",
      "Epoch 7, Batch 180/2700 (6.67%), Loss: 0.4042\n",
      "Epoch 7, Batch 190/2700 (7.04%), Loss: 0.8724\n",
      "Epoch 7, Batch 200/2700 (7.41%), Loss: 0.0211\n",
      "Epoch 7, Batch 210/2700 (7.78%), Loss: 0.5942\n",
      "Epoch 7, Batch 220/2700 (8.15%), Loss: 1.5397\n",
      "Epoch 7, Batch 230/2700 (8.52%), Loss: 0.4569\n",
      "Epoch 7, Batch 240/2700 (8.89%), Loss: 1.9791\n",
      "Epoch 7, Batch 250/2700 (9.26%), Loss: 0.0108\n",
      "Epoch 7, Batch 260/2700 (9.63%), Loss: 3.9208\n",
      "Epoch 7, Batch 270/2700 (10.00%), Loss: 19.4299\n",
      "Epoch 7, Batch 280/2700 (10.37%), Loss: 0.1321\n",
      "Epoch 7, Batch 290/2700 (10.74%), Loss: 0.2439\n",
      "Epoch 7, Batch 300/2700 (11.11%), Loss: 0.0977\n",
      "Epoch 7, Batch 310/2700 (11.48%), Loss: 12.9136\n",
      "Epoch 7, Batch 320/2700 (11.85%), Loss: 0.3835\n",
      "Epoch 7, Batch 330/2700 (12.22%), Loss: 1.3659\n",
      "Epoch 7, Batch 340/2700 (12.59%), Loss: 0.9586\n",
      "Epoch 7, Batch 350/2700 (12.96%), Loss: 0.1551\n",
      "Epoch 7, Batch 360/2700 (13.33%), Loss: 0.2432\n",
      "Epoch 7, Batch 370/2700 (13.70%), Loss: 0.6343\n",
      "Epoch 7, Batch 380/2700 (14.07%), Loss: 1.5062\n",
      "Epoch 7, Batch 390/2700 (14.44%), Loss: 0.0652\n",
      "Epoch 7, Batch 400/2700 (14.81%), Loss: 0.6339\n",
      "Epoch 7, Batch 410/2700 (15.19%), Loss: 4.8610\n",
      "Epoch 7, Batch 420/2700 (15.56%), Loss: 0.0153\n",
      "Epoch 7, Batch 430/2700 (15.93%), Loss: 1.1180\n",
      "Epoch 7, Batch 440/2700 (16.30%), Loss: 0.5471\n",
      "Epoch 7, Batch 450/2700 (16.67%), Loss: 0.9373\n",
      "Epoch 7, Batch 460/2700 (17.04%), Loss: 0.7503\n",
      "Epoch 7, Batch 470/2700 (17.41%), Loss: 0.0474\n",
      "Epoch 7, Batch 480/2700 (17.78%), Loss: 0.7511\n",
      "Epoch 7, Batch 490/2700 (18.15%), Loss: 0.0703\n",
      "Epoch 7, Batch 500/2700 (18.52%), Loss: 0.0570\n",
      "Epoch 7, Batch 510/2700 (18.89%), Loss: 1.2635\n",
      "Epoch 7, Batch 520/2700 (19.26%), Loss: 0.1781\n",
      "Epoch 7, Batch 530/2700 (19.63%), Loss: 2.2439\n",
      "Epoch 7, Batch 540/2700 (20.00%), Loss: 0.0760\n",
      "Epoch 7, Batch 550/2700 (20.37%), Loss: 0.1833\n",
      "Epoch 7, Batch 560/2700 (20.74%), Loss: 0.4614\n",
      "Epoch 7, Batch 570/2700 (21.11%), Loss: 0.3096\n",
      "Epoch 7, Batch 580/2700 (21.48%), Loss: 1.0919\n",
      "Epoch 7, Batch 590/2700 (21.85%), Loss: 0.5803\n",
      "Epoch 7, Batch 600/2700 (22.22%), Loss: 0.6075\n",
      "Epoch 7, Batch 610/2700 (22.59%), Loss: 0.3314\n",
      "Epoch 7, Batch 620/2700 (22.96%), Loss: 0.0832\n",
      "Epoch 7, Batch 630/2700 (23.33%), Loss: 0.2825\n",
      "Epoch 7, Batch 640/2700 (23.70%), Loss: 1.9113\n",
      "Epoch 7, Batch 650/2700 (24.07%), Loss: 0.4693\n",
      "Epoch 7, Batch 660/2700 (24.44%), Loss: 6.6470\n",
      "Epoch 7, Batch 670/2700 (24.81%), Loss: 0.0360\n",
      "Epoch 7, Batch 680/2700 (25.19%), Loss: 0.0107\n",
      "Epoch 7, Batch 690/2700 (25.56%), Loss: 0.7263\n",
      "Epoch 7, Batch 700/2700 (25.93%), Loss: 0.0078\n",
      "Epoch 7, Batch 710/2700 (26.30%), Loss: 0.7863\n",
      "Epoch 7, Batch 720/2700 (26.67%), Loss: 2.6589\n",
      "Epoch 7, Batch 730/2700 (27.04%), Loss: 2.6851\n",
      "Epoch 7, Batch 740/2700 (27.41%), Loss: 0.0140\n",
      "Epoch 7, Batch 750/2700 (27.78%), Loss: 0.5116\n",
      "Epoch 7, Batch 760/2700 (28.15%), Loss: 0.2718\n",
      "Epoch 7, Batch 770/2700 (28.52%), Loss: 1.2913\n",
      "Epoch 7, Batch 780/2700 (28.89%), Loss: 0.0745\n",
      "Epoch 7, Batch 790/2700 (29.26%), Loss: 1.4278\n",
      "Epoch 7, Batch 800/2700 (29.63%), Loss: 1.1926\n",
      "Epoch 7, Batch 810/2700 (30.00%), Loss: 0.2316\n",
      "Epoch 7, Batch 820/2700 (30.37%), Loss: 1.4196\n",
      "Epoch 7, Batch 830/2700 (30.74%), Loss: 0.2930\n",
      "Epoch 7, Batch 840/2700 (31.11%), Loss: 0.0031\n",
      "Epoch 7, Batch 850/2700 (31.48%), Loss: 5.7631\n",
      "Epoch 7, Batch 860/2700 (31.85%), Loss: 0.0680\n",
      "Epoch 7, Batch 870/2700 (32.22%), Loss: 0.9082\n",
      "Epoch 7, Batch 880/2700 (32.59%), Loss: 1.3772\n",
      "Epoch 7, Batch 890/2700 (32.96%), Loss: 2.5646\n",
      "Epoch 7, Batch 900/2700 (33.33%), Loss: 0.3807\n",
      "Epoch 7, Batch 910/2700 (33.70%), Loss: 0.0123\n",
      "Epoch 7, Batch 920/2700 (34.07%), Loss: 0.1641\n",
      "Epoch 7, Batch 930/2700 (34.44%), Loss: 0.4163\n",
      "Epoch 7, Batch 940/2700 (34.81%), Loss: 2.2596\n",
      "Epoch 7, Batch 950/2700 (35.19%), Loss: 1.3889\n",
      "Epoch 7, Batch 960/2700 (35.56%), Loss: 7.0631\n",
      "Epoch 7, Batch 970/2700 (35.93%), Loss: 1.0327\n",
      "Epoch 7, Batch 980/2700 (36.30%), Loss: 0.0627\n",
      "Epoch 7, Batch 990/2700 (36.67%), Loss: 1.6790\n",
      "Epoch 7, Batch 1000/2700 (37.04%), Loss: 0.0195\n",
      "Epoch 7, Batch 1010/2700 (37.41%), Loss: 0.0040\n",
      "Epoch 7, Batch 1020/2700 (37.78%), Loss: 2.1122\n",
      "Epoch 7, Batch 1030/2700 (38.15%), Loss: 0.5270\n",
      "Epoch 7, Batch 1040/2700 (38.52%), Loss: 0.9227\n",
      "Epoch 7, Batch 1050/2700 (38.89%), Loss: 0.1432\n",
      "Epoch 7, Batch 1060/2700 (39.26%), Loss: 1.3708\n",
      "Epoch 7, Batch 1070/2700 (39.63%), Loss: 0.1928\n",
      "Epoch 7, Batch 1080/2700 (40.00%), Loss: 0.0251\n",
      "Epoch 7, Batch 1090/2700 (40.37%), Loss: 0.1887\n",
      "Epoch 7, Batch 1100/2700 (40.74%), Loss: 1.7003\n",
      "Epoch 7, Batch 1110/2700 (41.11%), Loss: 0.1025\n",
      "Epoch 7, Batch 1120/2700 (41.48%), Loss: 1.4425\n",
      "Epoch 7, Batch 1130/2700 (41.85%), Loss: 0.0048\n",
      "Epoch 7, Batch 1140/2700 (42.22%), Loss: 0.0804\n",
      "Epoch 7, Batch 1150/2700 (42.59%), Loss: 1.8471\n",
      "Epoch 7, Batch 1160/2700 (42.96%), Loss: 0.0042\n",
      "Epoch 7, Batch 1170/2700 (43.33%), Loss: 1.0410\n",
      "Epoch 7, Batch 1180/2700 (43.70%), Loss: 1.4948\n",
      "Epoch 7, Batch 1190/2700 (44.07%), Loss: 0.5878\n",
      "Epoch 7, Batch 1200/2700 (44.44%), Loss: 0.1524\n",
      "Epoch 7, Batch 1210/2700 (44.81%), Loss: 1.4893\n",
      "Epoch 7, Batch 1220/2700 (45.19%), Loss: 0.4858\n",
      "Epoch 7, Batch 1230/2700 (45.56%), Loss: 0.2570\n",
      "Epoch 7, Batch 1240/2700 (45.93%), Loss: 0.0023\n",
      "Epoch 7, Batch 1250/2700 (46.30%), Loss: 0.5394\n",
      "Epoch 7, Batch 1260/2700 (46.67%), Loss: 0.3242\n",
      "Epoch 7, Batch 1270/2700 (47.04%), Loss: 0.0697\n",
      "Epoch 7, Batch 1280/2700 (47.41%), Loss: 0.4834\n",
      "Epoch 7, Batch 1290/2700 (47.78%), Loss: 0.0760\n",
      "Epoch 7, Batch 1300/2700 (48.15%), Loss: 1.4419\n",
      "Epoch 7, Batch 1310/2700 (48.52%), Loss: 3.1722\n",
      "Epoch 7, Batch 1320/2700 (48.89%), Loss: 0.3499\n",
      "Epoch 7, Batch 1330/2700 (49.26%), Loss: 0.0207\n",
      "Epoch 7, Batch 1340/2700 (49.63%), Loss: 0.4023\n",
      "Epoch 7, Batch 1350/2700 (50.00%), Loss: 0.1329\n",
      "Epoch 7, Batch 1360/2700 (50.37%), Loss: 0.8586\n",
      "Epoch 7, Batch 1370/2700 (50.74%), Loss: 0.3317\n",
      "Epoch 7, Batch 1380/2700 (51.11%), Loss: 0.2720\n",
      "Epoch 7, Batch 1390/2700 (51.48%), Loss: 2.7071\n",
      "Epoch 7, Batch 1400/2700 (51.85%), Loss: 0.3554\n",
      "Epoch 7, Batch 1410/2700 (52.22%), Loss: 1.9489\n",
      "Epoch 7, Batch 1420/2700 (52.59%), Loss: 0.7192\n",
      "Epoch 7, Batch 1430/2700 (52.96%), Loss: 0.1475\n",
      "Epoch 7, Batch 1440/2700 (53.33%), Loss: 0.8223\n",
      "Epoch 7, Batch 1450/2700 (53.70%), Loss: 4.7245\n",
      "Epoch 7, Batch 1460/2700 (54.07%), Loss: 0.3344\n",
      "Epoch 7, Batch 1470/2700 (54.44%), Loss: 0.0087\n",
      "Epoch 7, Batch 1480/2700 (54.81%), Loss: 0.0387\n",
      "Epoch 7, Batch 1490/2700 (55.19%), Loss: 0.4233\n",
      "Epoch 7, Batch 1500/2700 (55.56%), Loss: 0.3640\n",
      "Epoch 7, Batch 1510/2700 (55.93%), Loss: 0.4725\n",
      "Epoch 7, Batch 1520/2700 (56.30%), Loss: 0.2763\n",
      "Epoch 7, Batch 1530/2700 (56.67%), Loss: 0.2544\n",
      "Epoch 7, Batch 1540/2700 (57.04%), Loss: 0.1890\n",
      "Epoch 7, Batch 1550/2700 (57.41%), Loss: 0.0351\n",
      "Epoch 7, Batch 1560/2700 (57.78%), Loss: 0.4468\n",
      "Epoch 7, Batch 1570/2700 (58.15%), Loss: 0.1754\n",
      "Epoch 7, Batch 1580/2700 (58.52%), Loss: 3.0315\n",
      "Epoch 7, Batch 1590/2700 (58.89%), Loss: 0.0612\n",
      "Epoch 7, Batch 1600/2700 (59.26%), Loss: 0.1803\n",
      "Epoch 7, Batch 1610/2700 (59.63%), Loss: 0.3434\n",
      "Epoch 7, Batch 1620/2700 (60.00%), Loss: 0.0429\n",
      "Epoch 7, Batch 1630/2700 (60.37%), Loss: 0.2023\n",
      "Epoch 7, Batch 1640/2700 (60.74%), Loss: 3.7172\n",
      "Epoch 7, Batch 1650/2700 (61.11%), Loss: 0.1816\n",
      "Epoch 7, Batch 1660/2700 (61.48%), Loss: 0.5750\n",
      "Epoch 7, Batch 1670/2700 (61.85%), Loss: 0.0193\n",
      "Epoch 7, Batch 1680/2700 (62.22%), Loss: 0.2203\n",
      "Epoch 7, Batch 1690/2700 (62.59%), Loss: 0.0508\n",
      "Epoch 7, Batch 1700/2700 (62.96%), Loss: 0.1760\n",
      "Epoch 7, Batch 1710/2700 (63.33%), Loss: 0.0231\n",
      "Epoch 7, Batch 1720/2700 (63.70%), Loss: 2.8167\n",
      "Epoch 7, Batch 1730/2700 (64.07%), Loss: 0.5213\n",
      "Epoch 7, Batch 1740/2700 (64.44%), Loss: 0.0818\n",
      "Epoch 7, Batch 1750/2700 (64.81%), Loss: 0.9998\n",
      "Epoch 7, Batch 1760/2700 (65.19%), Loss: 0.3893\n",
      "Epoch 7, Batch 1770/2700 (65.56%), Loss: 0.8082\n",
      "Epoch 7, Batch 1780/2700 (65.93%), Loss: 1.4991\n",
      "Epoch 7, Batch 1790/2700 (66.30%), Loss: 0.0607\n",
      "Epoch 7, Batch 1800/2700 (66.67%), Loss: 0.0136\n",
      "Epoch 7, Batch 1810/2700 (67.04%), Loss: 2.1409\n",
      "Epoch 7, Batch 1820/2700 (67.41%), Loss: 0.2906\n",
      "Epoch 7, Batch 1830/2700 (67.78%), Loss: 0.0186\n",
      "Epoch 7, Batch 1840/2700 (68.15%), Loss: 1.1653\n",
      "Epoch 7, Batch 1850/2700 (68.52%), Loss: 0.2939\n",
      "Epoch 7, Batch 1860/2700 (68.89%), Loss: 0.4113\n",
      "Epoch 7, Batch 1870/2700 (69.26%), Loss: 0.3591\n",
      "Epoch 7, Batch 1880/2700 (69.63%), Loss: 0.0102\n",
      "Epoch 7, Batch 1890/2700 (70.00%), Loss: 0.9786\n",
      "Epoch 7, Batch 1900/2700 (70.37%), Loss: 0.7839\n",
      "Epoch 7, Batch 1910/2700 (70.74%), Loss: 0.1254\n",
      "Epoch 7, Batch 1920/2700 (71.11%), Loss: 3.3602\n",
      "Epoch 7, Batch 1930/2700 (71.48%), Loss: 1.4908\n",
      "Epoch 7, Batch 1940/2700 (71.85%), Loss: 0.0826\n",
      "Epoch 7, Batch 1950/2700 (72.22%), Loss: 0.3766\n",
      "Epoch 7, Batch 1960/2700 (72.59%), Loss: 0.0295\n",
      "Epoch 7, Batch 1970/2700 (72.96%), Loss: 0.0264\n",
      "Epoch 7, Batch 1980/2700 (73.33%), Loss: 0.4784\n",
      "Epoch 7, Batch 1990/2700 (73.70%), Loss: 0.0023\n",
      "Epoch 7, Batch 2000/2700 (74.07%), Loss: 0.7960\n",
      "Epoch 7, Batch 2010/2700 (74.44%), Loss: 0.1370\n",
      "Epoch 7, Batch 2020/2700 (74.81%), Loss: 0.1957\n",
      "Epoch 7, Batch 2030/2700 (75.19%), Loss: 0.0652\n",
      "Epoch 7, Batch 2040/2700 (75.56%), Loss: 0.0185\n",
      "Epoch 7, Batch 2050/2700 (75.93%), Loss: 1.0270\n",
      "Epoch 7, Batch 2060/2700 (76.30%), Loss: 0.1296\n",
      "Epoch 7, Batch 2070/2700 (76.67%), Loss: 0.4303\n",
      "Epoch 7, Batch 2080/2700 (77.04%), Loss: 0.1511\n",
      "Epoch 7, Batch 2090/2700 (77.41%), Loss: 0.0046\n",
      "Epoch 7, Batch 2100/2700 (77.78%), Loss: 0.1030\n",
      "Epoch 7, Batch 2110/2700 (78.15%), Loss: 0.4095\n",
      "Epoch 7, Batch 2120/2700 (78.52%), Loss: 0.0095\n",
      "Epoch 7, Batch 2130/2700 (78.89%), Loss: 0.2245\n",
      "Epoch 7, Batch 2140/2700 (79.26%), Loss: 0.3685\n",
      "Epoch 7, Batch 2150/2700 (79.63%), Loss: 0.2637\n",
      "Epoch 7, Batch 2160/2700 (80.00%), Loss: 0.6240\n",
      "Epoch 7, Batch 2170/2700 (80.37%), Loss: 0.0098\n",
      "Epoch 7, Batch 2180/2700 (80.74%), Loss: 0.3871\n",
      "Epoch 7, Batch 2190/2700 (81.11%), Loss: 0.1495\n",
      "Epoch 7, Batch 2200/2700 (81.48%), Loss: 0.0030\n",
      "Epoch 7, Batch 2210/2700 (81.85%), Loss: 0.5671\n",
      "Epoch 7, Batch 2220/2700 (82.22%), Loss: 0.1295\n",
      "Epoch 7, Batch 2230/2700 (82.59%), Loss: 0.0566\n",
      "Epoch 7, Batch 2240/2700 (82.96%), Loss: 0.0127\n",
      "Epoch 7, Batch 2250/2700 (83.33%), Loss: 0.3961\n",
      "Epoch 7, Batch 2260/2700 (83.70%), Loss: 0.1749\n",
      "Epoch 7, Batch 2270/2700 (84.07%), Loss: 0.1022\n",
      "Epoch 7, Batch 2280/2700 (84.44%), Loss: 0.5281\n",
      "Epoch 7, Batch 2290/2700 (84.81%), Loss: 0.0550\n",
      "Epoch 7, Batch 2300/2700 (85.19%), Loss: 0.0826\n",
      "Epoch 7, Batch 2310/2700 (85.56%), Loss: 0.2050\n",
      "Epoch 7, Batch 2320/2700 (85.93%), Loss: 5.1296\n",
      "Epoch 7, Batch 2330/2700 (86.30%), Loss: 1.4799\n",
      "Epoch 7, Batch 2340/2700 (86.67%), Loss: 0.3666\n",
      "Epoch 7, Batch 2350/2700 (87.04%), Loss: 0.1227\n",
      "Epoch 7, Batch 2360/2700 (87.41%), Loss: 0.1293\n",
      "Epoch 7, Batch 2370/2700 (87.78%), Loss: 0.0904\n",
      "Epoch 7, Batch 2380/2700 (88.15%), Loss: 19.3911\n",
      "Epoch 7, Batch 2390/2700 (88.52%), Loss: 4.2278\n",
      "Epoch 7, Batch 2400/2700 (88.89%), Loss: 0.3370\n",
      "Epoch 7, Batch 2410/2700 (89.26%), Loss: 7.9540\n",
      "Epoch 7, Batch 2420/2700 (89.63%), Loss: 1.0872\n",
      "Epoch 7, Batch 2430/2700 (90.00%), Loss: 0.1609\n",
      "Epoch 7, Batch 2440/2700 (90.37%), Loss: 0.0410\n",
      "Epoch 7, Batch 2450/2700 (90.74%), Loss: 0.2523\n",
      "Epoch 7, Batch 2460/2700 (91.11%), Loss: 0.2100\n",
      "Epoch 7, Batch 2470/2700 (91.48%), Loss: 0.1119\n",
      "Epoch 7, Batch 2480/2700 (91.85%), Loss: 0.0415\n",
      "Epoch 7, Batch 2490/2700 (92.22%), Loss: 0.5811\n",
      "Epoch 7, Batch 2500/2700 (92.59%), Loss: 0.1256\n",
      "Epoch 7, Batch 2510/2700 (92.96%), Loss: 0.1263\n",
      "Epoch 7, Batch 2520/2700 (93.33%), Loss: 1.0616\n",
      "Epoch 7, Batch 2530/2700 (93.70%), Loss: 0.3442\n",
      "Epoch 7, Batch 2540/2700 (94.07%), Loss: 0.0223\n",
      "Epoch 7, Batch 2550/2700 (94.44%), Loss: 0.0067\n",
      "Epoch 7, Batch 2560/2700 (94.81%), Loss: 0.5815\n",
      "Epoch 7, Batch 2570/2700 (95.19%), Loss: 0.7328\n",
      "Epoch 7, Batch 2580/2700 (95.56%), Loss: 0.0643\n",
      "Epoch 7, Batch 2590/2700 (95.93%), Loss: 2.0881\n",
      "Epoch 7, Batch 2600/2700 (96.30%), Loss: 0.2214\n",
      "Epoch 7, Batch 2610/2700 (96.67%), Loss: 0.1771\n",
      "Epoch 7, Batch 2620/2700 (97.04%), Loss: 0.9926\n",
      "Epoch 7, Batch 2630/2700 (97.41%), Loss: 0.0225\n",
      "Epoch 7, Batch 2640/2700 (97.78%), Loss: 0.5151\n",
      "Epoch 7, Batch 2650/2700 (98.15%), Loss: 0.4698\n",
      "Epoch 7, Batch 2660/2700 (98.52%), Loss: 2.7022\n",
      "Epoch 7, Batch 2670/2700 (98.89%), Loss: 0.2438\n",
      "Epoch 7, Batch 2680/2700 (99.26%), Loss: 0.1812\n",
      "Epoch 7, Batch 2690/2700 (99.63%), Loss: 0.1423\n",
      "Epoch 7, Batch 2700/2700 (100.00%), Loss: 0.5695\n",
      "Epoch 7 completed. Average loss: 0.7673\n",
      "Epoch 8, Batch 10/2700 (0.37%), Loss: 0.1871\n",
      "Epoch 8, Batch 20/2700 (0.74%), Loss: 0.0237\n",
      "Epoch 8, Batch 30/2700 (1.11%), Loss: 0.2286\n",
      "Epoch 8, Batch 40/2700 (1.48%), Loss: 0.0944\n",
      "Epoch 8, Batch 50/2700 (1.85%), Loss: 0.2039\n",
      "Epoch 8, Batch 60/2700 (2.22%), Loss: 0.2715\n",
      "Epoch 8, Batch 70/2700 (2.59%), Loss: 0.2528\n",
      "Epoch 8, Batch 80/2700 (2.96%), Loss: 0.1580\n",
      "Epoch 8, Batch 90/2700 (3.33%), Loss: 0.2739\n",
      "Epoch 8, Batch 100/2700 (3.70%), Loss: 0.9805\n",
      "Epoch 8, Batch 110/2700 (4.07%), Loss: 0.2314\n",
      "Epoch 8, Batch 120/2700 (4.44%), Loss: 0.2015\n",
      "Epoch 8, Batch 130/2700 (4.81%), Loss: 1.5147\n",
      "Epoch 8, Batch 140/2700 (5.19%), Loss: 0.1099\n",
      "Epoch 8, Batch 150/2700 (5.56%), Loss: 0.0185\n",
      "Epoch 8, Batch 160/2700 (5.93%), Loss: 0.0724\n",
      "Epoch 8, Batch 170/2700 (6.30%), Loss: 0.5086\n",
      "Epoch 8, Batch 180/2700 (6.67%), Loss: 0.2329\n",
      "Epoch 8, Batch 190/2700 (7.04%), Loss: 0.8096\n",
      "Epoch 8, Batch 200/2700 (7.41%), Loss: 0.0090\n",
      "Epoch 8, Batch 210/2700 (7.78%), Loss: 0.3325\n",
      "Epoch 8, Batch 220/2700 (8.15%), Loss: 1.0068\n",
      "Epoch 8, Batch 230/2700 (8.52%), Loss: 0.2524\n",
      "Epoch 8, Batch 240/2700 (8.89%), Loss: 1.9020\n",
      "Epoch 8, Batch 250/2700 (9.26%), Loss: 0.0071\n",
      "Epoch 8, Batch 260/2700 (9.63%), Loss: 3.3687\n",
      "Epoch 8, Batch 270/2700 (10.00%), Loss: 13.2991\n",
      "Epoch 8, Batch 280/2700 (10.37%), Loss: 0.1456\n",
      "Epoch 8, Batch 290/2700 (10.74%), Loss: 0.1939\n",
      "Epoch 8, Batch 300/2700 (11.11%), Loss: 0.0671\n",
      "Epoch 8, Batch 310/2700 (11.48%), Loss: 7.5993\n",
      "Epoch 8, Batch 320/2700 (11.85%), Loss: 0.3234\n",
      "Epoch 8, Batch 330/2700 (12.22%), Loss: 0.8009\n",
      "Epoch 8, Batch 340/2700 (12.59%), Loss: 0.7272\n",
      "Epoch 8, Batch 350/2700 (12.96%), Loss: 0.0515\n",
      "Epoch 8, Batch 360/2700 (13.33%), Loss: 0.1647\n",
      "Epoch 8, Batch 370/2700 (13.70%), Loss: 0.4623\n",
      "Epoch 8, Batch 380/2700 (14.07%), Loss: 0.9523\n",
      "Epoch 8, Batch 390/2700 (14.44%), Loss: 0.0488\n",
      "Epoch 8, Batch 400/2700 (14.81%), Loss: 0.3530\n",
      "Epoch 8, Batch 410/2700 (15.19%), Loss: 4.0278\n",
      "Epoch 8, Batch 420/2700 (15.56%), Loss: 0.0104\n",
      "Epoch 8, Batch 430/2700 (15.93%), Loss: 1.2704\n",
      "Epoch 8, Batch 440/2700 (16.30%), Loss: 0.3900\n",
      "Epoch 8, Batch 450/2700 (16.67%), Loss: 0.6456\n",
      "Epoch 8, Batch 460/2700 (17.04%), Loss: 0.3548\n",
      "Epoch 8, Batch 470/2700 (17.41%), Loss: 0.0215\n",
      "Epoch 8, Batch 480/2700 (17.78%), Loss: 0.6574\n",
      "Epoch 8, Batch 490/2700 (18.15%), Loss: 0.0514\n",
      "Epoch 8, Batch 500/2700 (18.52%), Loss: 0.0389\n",
      "Epoch 8, Batch 510/2700 (18.89%), Loss: 0.7458\n",
      "Epoch 8, Batch 520/2700 (19.26%), Loss: 0.1262\n",
      "Epoch 8, Batch 530/2700 (19.63%), Loss: 1.4302\n",
      "Epoch 8, Batch 540/2700 (20.00%), Loss: 0.0565\n",
      "Epoch 8, Batch 550/2700 (20.37%), Loss: 0.1841\n",
      "Epoch 8, Batch 560/2700 (20.74%), Loss: 0.1547\n",
      "Epoch 8, Batch 570/2700 (21.11%), Loss: 0.2853\n",
      "Epoch 8, Batch 580/2700 (21.48%), Loss: 1.1689\n",
      "Epoch 8, Batch 590/2700 (21.85%), Loss: 0.4098\n",
      "Epoch 8, Batch 600/2700 (22.22%), Loss: 0.3660\n",
      "Epoch 8, Batch 610/2700 (22.59%), Loss: 0.2117\n",
      "Epoch 8, Batch 620/2700 (22.96%), Loss: 0.0962\n",
      "Epoch 8, Batch 630/2700 (23.33%), Loss: 0.2230\n",
      "Epoch 8, Batch 640/2700 (23.70%), Loss: 1.2251\n",
      "Epoch 8, Batch 650/2700 (24.07%), Loss: 0.3142\n",
      "Epoch 8, Batch 660/2700 (24.44%), Loss: 6.1032\n",
      "Epoch 8, Batch 670/2700 (24.81%), Loss: 0.0166\n",
      "Epoch 8, Batch 680/2700 (25.19%), Loss: 0.0097\n",
      "Epoch 8, Batch 690/2700 (25.56%), Loss: 0.5108\n",
      "Epoch 8, Batch 700/2700 (25.93%), Loss: 0.0092\n",
      "Epoch 8, Batch 710/2700 (26.30%), Loss: 0.5117\n",
      "Epoch 8, Batch 720/2700 (26.67%), Loss: 2.0655\n",
      "Epoch 8, Batch 730/2700 (27.04%), Loss: 1.4003\n",
      "Epoch 8, Batch 740/2700 (27.41%), Loss: 0.0113\n",
      "Epoch 8, Batch 750/2700 (27.78%), Loss: 0.3882\n",
      "Epoch 8, Batch 760/2700 (28.15%), Loss: 0.2068\n",
      "Epoch 8, Batch 770/2700 (28.52%), Loss: 0.8101\n",
      "Epoch 8, Batch 780/2700 (28.89%), Loss: 0.0735\n",
      "Epoch 8, Batch 790/2700 (29.26%), Loss: 1.1511\n",
      "Epoch 8, Batch 800/2700 (29.63%), Loss: 0.8875\n",
      "Epoch 8, Batch 810/2700 (30.00%), Loss: 0.1656\n",
      "Epoch 8, Batch 820/2700 (30.37%), Loss: 0.5613\n",
      "Epoch 8, Batch 830/2700 (30.74%), Loss: 0.2722\n",
      "Epoch 8, Batch 840/2700 (31.11%), Loss: 0.0036\n",
      "Epoch 8, Batch 850/2700 (31.48%), Loss: 4.1260\n",
      "Epoch 8, Batch 860/2700 (31.85%), Loss: 0.0521\n",
      "Epoch 8, Batch 870/2700 (32.22%), Loss: 0.5240\n",
      "Epoch 8, Batch 880/2700 (32.59%), Loss: 1.1832\n",
      "Epoch 8, Batch 890/2700 (32.96%), Loss: 0.4779\n",
      "Epoch 8, Batch 900/2700 (33.33%), Loss: 0.2700\n",
      "Epoch 8, Batch 910/2700 (33.70%), Loss: 0.0132\n",
      "Epoch 8, Batch 920/2700 (34.07%), Loss: 0.1393\n",
      "Epoch 8, Batch 930/2700 (34.44%), Loss: 0.2408\n",
      "Epoch 8, Batch 940/2700 (34.81%), Loss: 1.9957\n",
      "Epoch 8, Batch 950/2700 (35.19%), Loss: 1.0020\n",
      "Epoch 8, Batch 960/2700 (35.56%), Loss: 6.1313\n",
      "Epoch 8, Batch 970/2700 (35.93%), Loss: 0.8796\n",
      "Epoch 8, Batch 980/2700 (36.30%), Loss: 0.0389\n",
      "Epoch 8, Batch 990/2700 (36.67%), Loss: 1.9761\n",
      "Epoch 8, Batch 1000/2700 (37.04%), Loss: 0.0132\n",
      "Epoch 8, Batch 1010/2700 (37.41%), Loss: 0.0034\n",
      "Epoch 8, Batch 1020/2700 (37.78%), Loss: 1.6631\n",
      "Epoch 8, Batch 1030/2700 (38.15%), Loss: 0.6249\n",
      "Epoch 8, Batch 1040/2700 (38.52%), Loss: 0.6023\n",
      "Epoch 8, Batch 1050/2700 (38.89%), Loss: 0.2273\n",
      "Epoch 8, Batch 1060/2700 (39.26%), Loss: 1.1503\n",
      "Epoch 8, Batch 1070/2700 (39.63%), Loss: 0.1296\n",
      "Epoch 8, Batch 1080/2700 (40.00%), Loss: 0.0198\n",
      "Epoch 8, Batch 1090/2700 (40.37%), Loss: 0.2006\n",
      "Epoch 8, Batch 1100/2700 (40.74%), Loss: 1.4952\n",
      "Epoch 8, Batch 1110/2700 (41.11%), Loss: 0.0627\n",
      "Epoch 8, Batch 1120/2700 (41.48%), Loss: 0.8766\n",
      "Epoch 8, Batch 1130/2700 (41.85%), Loss: 0.0052\n",
      "Epoch 8, Batch 1140/2700 (42.22%), Loss: 0.0591\n",
      "Epoch 8, Batch 1150/2700 (42.59%), Loss: 1.0502\n",
      "Epoch 8, Batch 1160/2700 (42.96%), Loss: 0.0033\n",
      "Epoch 8, Batch 1170/2700 (43.33%), Loss: 0.8336\n",
      "Epoch 8, Batch 1180/2700 (43.70%), Loss: 1.2320\n",
      "Epoch 8, Batch 1190/2700 (44.07%), Loss: 0.2756\n",
      "Epoch 8, Batch 1200/2700 (44.44%), Loss: 0.1003\n",
      "Epoch 8, Batch 1210/2700 (44.81%), Loss: 1.1663\n",
      "Epoch 8, Batch 1220/2700 (45.19%), Loss: 0.3873\n",
      "Epoch 8, Batch 1230/2700 (45.56%), Loss: 0.2964\n",
      "Epoch 8, Batch 1240/2700 (45.93%), Loss: 0.0027\n",
      "Epoch 8, Batch 1250/2700 (46.30%), Loss: 0.5183\n",
      "Epoch 8, Batch 1260/2700 (46.67%), Loss: 0.2297\n",
      "Epoch 8, Batch 1270/2700 (47.04%), Loss: 0.0447\n",
      "Epoch 8, Batch 1280/2700 (47.41%), Loss: 0.3904\n",
      "Epoch 8, Batch 1290/2700 (47.78%), Loss: 0.0717\n",
      "Epoch 8, Batch 1300/2700 (48.15%), Loss: 1.0052\n",
      "Epoch 8, Batch 1310/2700 (48.52%), Loss: 1.8730\n",
      "Epoch 8, Batch 1320/2700 (48.89%), Loss: 0.2219\n",
      "Epoch 8, Batch 1330/2700 (49.26%), Loss: 0.0171\n",
      "Epoch 8, Batch 1340/2700 (49.63%), Loss: 0.3297\n",
      "Epoch 8, Batch 1350/2700 (50.00%), Loss: 0.0948\n",
      "Epoch 8, Batch 1360/2700 (50.37%), Loss: 0.8148\n",
      "Epoch 8, Batch 1370/2700 (50.74%), Loss: 0.2339\n",
      "Epoch 8, Batch 1380/2700 (51.11%), Loss: 0.2224\n",
      "Epoch 8, Batch 1390/2700 (51.48%), Loss: 4.2648\n",
      "Epoch 8, Batch 1400/2700 (51.85%), Loss: 0.2443\n",
      "Epoch 8, Batch 1410/2700 (52.22%), Loss: 0.9332\n",
      "Epoch 8, Batch 1420/2700 (52.59%), Loss: 0.6899\n",
      "Epoch 8, Batch 1430/2700 (52.96%), Loss: 0.1318\n",
      "Epoch 8, Batch 1440/2700 (53.33%), Loss: 0.4850\n",
      "Epoch 8, Batch 1450/2700 (53.70%), Loss: 3.5013\n",
      "Epoch 8, Batch 1460/2700 (54.07%), Loss: 0.3015\n",
      "Epoch 8, Batch 1470/2700 (54.44%), Loss: 0.0090\n",
      "Epoch 8, Batch 1480/2700 (54.81%), Loss: 0.0285\n",
      "Epoch 8, Batch 1490/2700 (55.19%), Loss: 0.3746\n",
      "Epoch 8, Batch 1500/2700 (55.56%), Loss: 0.2095\n",
      "Epoch 8, Batch 1510/2700 (55.93%), Loss: 0.3943\n",
      "Epoch 8, Batch 1520/2700 (56.30%), Loss: 0.2556\n",
      "Epoch 8, Batch 1530/2700 (56.67%), Loss: 0.1678\n",
      "Epoch 8, Batch 1540/2700 (57.04%), Loss: 0.1137\n",
      "Epoch 8, Batch 1550/2700 (57.41%), Loss: 0.0306\n",
      "Epoch 8, Batch 1560/2700 (57.78%), Loss: 0.2674\n",
      "Epoch 8, Batch 1570/2700 (58.15%), Loss: 0.1183\n",
      "Epoch 8, Batch 1580/2700 (58.52%), Loss: 2.1569\n",
      "Epoch 8, Batch 1590/2700 (58.89%), Loss: 0.0417\n",
      "Epoch 8, Batch 1600/2700 (59.26%), Loss: 0.1678\n",
      "Epoch 8, Batch 1610/2700 (59.63%), Loss: 0.2471\n",
      "Epoch 8, Batch 1620/2700 (60.00%), Loss: 0.0281\n",
      "Epoch 8, Batch 1630/2700 (60.37%), Loss: 0.0545\n",
      "Epoch 8, Batch 1640/2700 (60.74%), Loss: 2.9835\n",
      "Epoch 8, Batch 1650/2700 (61.11%), Loss: 0.1671\n",
      "Epoch 8, Batch 1660/2700 (61.48%), Loss: 0.3533\n",
      "Epoch 8, Batch 1670/2700 (61.85%), Loss: 0.0109\n",
      "Epoch 8, Batch 1680/2700 (62.22%), Loss: 0.1517\n",
      "Epoch 8, Batch 1690/2700 (62.59%), Loss: 0.0324\n",
      "Epoch 8, Batch 1700/2700 (62.96%), Loss: 0.1186\n",
      "Epoch 8, Batch 1710/2700 (63.33%), Loss: 0.0204\n",
      "Epoch 8, Batch 1720/2700 (63.70%), Loss: 1.7598\n",
      "Epoch 8, Batch 1730/2700 (64.07%), Loss: 0.3625\n",
      "Epoch 8, Batch 1740/2700 (64.44%), Loss: 0.0606\n",
      "Epoch 8, Batch 1750/2700 (64.81%), Loss: 0.7350\n",
      "Epoch 8, Batch 1760/2700 (65.19%), Loss: 0.2740\n",
      "Epoch 8, Batch 1770/2700 (65.56%), Loss: 0.5786\n",
      "Epoch 8, Batch 1780/2700 (65.93%), Loss: 0.9956\n",
      "Epoch 8, Batch 1790/2700 (66.30%), Loss: 0.0251\n",
      "Epoch 8, Batch 1800/2700 (66.67%), Loss: 0.0093\n",
      "Epoch 8, Batch 1810/2700 (67.04%), Loss: 1.0887\n",
      "Epoch 8, Batch 1820/2700 (67.41%), Loss: 0.2465\n",
      "Epoch 8, Batch 1830/2700 (67.78%), Loss: 0.0153\n",
      "Epoch 8, Batch 1840/2700 (68.15%), Loss: 0.6129\n",
      "Epoch 8, Batch 1850/2700 (68.52%), Loss: 0.2201\n",
      "Epoch 8, Batch 1860/2700 (68.89%), Loss: 0.2915\n",
      "Epoch 8, Batch 1870/2700 (69.26%), Loss: 0.2913\n",
      "Epoch 8, Batch 1880/2700 (69.63%), Loss: 0.0079\n",
      "Epoch 8, Batch 1890/2700 (70.00%), Loss: 0.8541\n",
      "Epoch 8, Batch 1900/2700 (70.37%), Loss: 0.4817\n",
      "Epoch 8, Batch 1910/2700 (70.74%), Loss: 0.0869\n",
      "Epoch 8, Batch 1920/2700 (71.11%), Loss: 2.9845\n",
      "Epoch 8, Batch 1930/2700 (71.48%), Loss: 1.0866\n",
      "Epoch 8, Batch 1940/2700 (71.85%), Loss: 0.0594\n",
      "Epoch 8, Batch 1950/2700 (72.22%), Loss: 0.2806\n",
      "Epoch 8, Batch 1960/2700 (72.59%), Loss: 0.0208\n",
      "Epoch 8, Batch 1970/2700 (72.96%), Loss: 0.0171\n",
      "Epoch 8, Batch 1980/2700 (73.33%), Loss: 0.3662\n",
      "Epoch 8, Batch 1990/2700 (73.70%), Loss: 0.0029\n",
      "Epoch 8, Batch 2000/2700 (74.07%), Loss: 0.7170\n",
      "Epoch 8, Batch 2010/2700 (74.44%), Loss: 0.0764\n",
      "Epoch 8, Batch 2020/2700 (74.81%), Loss: 0.1611\n",
      "Epoch 8, Batch 2030/2700 (75.19%), Loss: 0.0587\n",
      "Epoch 8, Batch 2040/2700 (75.56%), Loss: 0.0118\n",
      "Epoch 8, Batch 2050/2700 (75.93%), Loss: 0.7406\n",
      "Epoch 8, Batch 2060/2700 (76.30%), Loss: 0.1219\n",
      "Epoch 8, Batch 2070/2700 (76.67%), Loss: 0.2984\n",
      "Epoch 8, Batch 2080/2700 (77.04%), Loss: 0.1866\n",
      "Epoch 8, Batch 2090/2700 (77.41%), Loss: 0.0040\n",
      "Epoch 8, Batch 2100/2700 (77.78%), Loss: 0.3737\n",
      "Epoch 8, Batch 2110/2700 (78.15%), Loss: 0.2698\n",
      "Epoch 8, Batch 2120/2700 (78.52%), Loss: 0.0086\n",
      "Epoch 8, Batch 2130/2700 (78.89%), Loss: 0.1997\n",
      "Epoch 8, Batch 2140/2700 (79.26%), Loss: 0.2766\n",
      "Epoch 8, Batch 2150/2700 (79.63%), Loss: 0.2831\n",
      "Epoch 8, Batch 2160/2700 (80.00%), Loss: 0.4698\n",
      "Epoch 8, Batch 2170/2700 (80.37%), Loss: 0.0076\n",
      "Epoch 8, Batch 2180/2700 (80.74%), Loss: 0.2024\n",
      "Epoch 8, Batch 2190/2700 (81.11%), Loss: 0.1216\n",
      "Epoch 8, Batch 2200/2700 (81.48%), Loss: 0.0020\n",
      "Epoch 8, Batch 2210/2700 (81.85%), Loss: 0.3143\n",
      "Epoch 8, Batch 2220/2700 (82.22%), Loss: 0.0841\n",
      "Epoch 8, Batch 2230/2700 (82.59%), Loss: 0.0289\n",
      "Epoch 8, Batch 2240/2700 (82.96%), Loss: 0.0102\n",
      "Epoch 8, Batch 2250/2700 (83.33%), Loss: 0.3031\n",
      "Epoch 8, Batch 2260/2700 (83.70%), Loss: 0.1637\n",
      "Epoch 8, Batch 2270/2700 (84.07%), Loss: 0.0598\n",
      "Epoch 8, Batch 2280/2700 (84.44%), Loss: 0.3698\n",
      "Epoch 8, Batch 2290/2700 (84.81%), Loss: 0.0541\n",
      "Epoch 8, Batch 2300/2700 (85.19%), Loss: 0.1021\n",
      "Epoch 8, Batch 2310/2700 (85.56%), Loss: 0.1476\n",
      "Epoch 8, Batch 2320/2700 (85.93%), Loss: 4.2132\n",
      "Epoch 8, Batch 2330/2700 (86.30%), Loss: 1.2196\n",
      "Epoch 8, Batch 2340/2700 (86.67%), Loss: 0.2596\n",
      "Epoch 8, Batch 2350/2700 (87.04%), Loss: 0.0693\n",
      "Epoch 8, Batch 2360/2700 (87.41%), Loss: 0.0858\n",
      "Epoch 8, Batch 2370/2700 (87.78%), Loss: 0.0482\n",
      "Epoch 8, Batch 2380/2700 (88.15%), Loss: 14.0529\n",
      "Epoch 8, Batch 2390/2700 (88.52%), Loss: 2.8008\n",
      "Epoch 8, Batch 2400/2700 (88.89%), Loss: 0.2653\n",
      "Epoch 8, Batch 2410/2700 (89.26%), Loss: 7.9038\n",
      "Epoch 8, Batch 2420/2700 (89.63%), Loss: 0.7552\n",
      "Epoch 8, Batch 2430/2700 (90.00%), Loss: 0.1066\n",
      "Epoch 8, Batch 2440/2700 (90.37%), Loss: 0.0157\n",
      "Epoch 8, Batch 2450/2700 (90.74%), Loss: 0.1915\n",
      "Epoch 8, Batch 2460/2700 (91.11%), Loss: 0.1780\n",
      "Epoch 8, Batch 2470/2700 (91.48%), Loss: 0.0813\n",
      "Epoch 8, Batch 2480/2700 (91.85%), Loss: 0.0276\n",
      "Epoch 8, Batch 2490/2700 (92.22%), Loss: 0.4410\n",
      "Epoch 8, Batch 2500/2700 (92.59%), Loss: 0.0878\n",
      "Epoch 8, Batch 2510/2700 (92.96%), Loss: 0.1042\n",
      "Epoch 8, Batch 2520/2700 (93.33%), Loss: 0.7007\n",
      "Epoch 8, Batch 2530/2700 (93.70%), Loss: 0.2154\n",
      "Epoch 8, Batch 2540/2700 (94.07%), Loss: 0.0134\n",
      "Epoch 8, Batch 2550/2700 (94.44%), Loss: 0.0057\n",
      "Epoch 8, Batch 2560/2700 (94.81%), Loss: 0.4724\n",
      "Epoch 8, Batch 2570/2700 (95.19%), Loss: 0.5770\n",
      "Epoch 8, Batch 2580/2700 (95.56%), Loss: 0.0585\n",
      "Epoch 8, Batch 2590/2700 (95.93%), Loss: 1.3911\n",
      "Epoch 8, Batch 2600/2700 (96.30%), Loss: 0.2044\n",
      "Epoch 8, Batch 2610/2700 (96.67%), Loss: 0.1355\n",
      "Epoch 8, Batch 2620/2700 (97.04%), Loss: 0.2888\n",
      "Epoch 8, Batch 2630/2700 (97.41%), Loss: 0.0177\n",
      "Epoch 8, Batch 2640/2700 (97.78%), Loss: 0.3028\n",
      "Epoch 8, Batch 2650/2700 (98.15%), Loss: 0.4391\n",
      "Epoch 8, Batch 2660/2700 (98.52%), Loss: 1.7449\n",
      "Epoch 8, Batch 2670/2700 (98.89%), Loss: 0.1752\n",
      "Epoch 8, Batch 2680/2700 (99.26%), Loss: 0.1513\n",
      "Epoch 8, Batch 2690/2700 (99.63%), Loss: 0.1022\n",
      "Epoch 8, Batch 2700/2700 (100.00%), Loss: 0.4552\n",
      "Epoch 8 completed. Average loss: 0.5646\n",
      "Epoch 9, Batch 10/2700 (0.37%), Loss: 0.1186\n",
      "Epoch 9, Batch 20/2700 (0.74%), Loss: 0.0193\n",
      "Epoch 9, Batch 30/2700 (1.11%), Loss: 0.0805\n",
      "Epoch 9, Batch 40/2700 (1.48%), Loss: 0.0668\n",
      "Epoch 9, Batch 50/2700 (1.85%), Loss: 0.1599\n",
      "Epoch 9, Batch 60/2700 (2.22%), Loss: 0.1278\n",
      "Epoch 9, Batch 70/2700 (2.59%), Loss: 0.1885\n",
      "Epoch 9, Batch 80/2700 (2.96%), Loss: 0.1155\n",
      "Epoch 9, Batch 90/2700 (3.33%), Loss: 0.1984\n",
      "Epoch 9, Batch 100/2700 (3.70%), Loss: 0.7377\n",
      "Epoch 9, Batch 110/2700 (4.07%), Loss: 0.1689\n",
      "Epoch 9, Batch 120/2700 (4.44%), Loss: 0.1420\n",
      "Epoch 9, Batch 130/2700 (4.81%), Loss: 1.0892\n",
      "Epoch 9, Batch 140/2700 (5.19%), Loss: 0.0778\n",
      "Epoch 9, Batch 150/2700 (5.56%), Loss: 0.0148\n",
      "Epoch 9, Batch 160/2700 (5.93%), Loss: 0.0489\n",
      "Epoch 9, Batch 170/2700 (6.30%), Loss: 0.4023\n",
      "Epoch 9, Batch 180/2700 (6.67%), Loss: 0.1557\n",
      "Epoch 9, Batch 190/2700 (7.04%), Loss: 0.6992\n",
      "Epoch 9, Batch 200/2700 (7.41%), Loss: 0.0048\n",
      "Epoch 9, Batch 210/2700 (7.78%), Loss: 0.2140\n",
      "Epoch 9, Batch 220/2700 (8.15%), Loss: 0.7408\n",
      "Epoch 9, Batch 230/2700 (8.52%), Loss: 0.1533\n",
      "Epoch 9, Batch 240/2700 (8.89%), Loss: 1.7169\n",
      "Epoch 9, Batch 250/2700 (9.26%), Loss: 0.0064\n",
      "Epoch 9, Batch 260/2700 (9.63%), Loss: 2.5146\n",
      "Epoch 9, Batch 270/2700 (10.00%), Loss: 8.7789\n",
      "Epoch 9, Batch 280/2700 (10.37%), Loss: 0.1305\n",
      "Epoch 9, Batch 290/2700 (10.74%), Loss: 0.2156\n",
      "Epoch 9, Batch 300/2700 (11.11%), Loss: 0.0525\n",
      "Epoch 9, Batch 310/2700 (11.48%), Loss: 4.3837\n",
      "Epoch 9, Batch 320/2700 (11.85%), Loss: 0.2468\n",
      "Epoch 9, Batch 330/2700 (12.22%), Loss: 0.3149\n",
      "Epoch 9, Batch 340/2700 (12.59%), Loss: 0.6498\n",
      "Epoch 9, Batch 350/2700 (12.96%), Loss: 0.0384\n",
      "Epoch 9, Batch 360/2700 (13.33%), Loss: 0.3538\n",
      "Epoch 9, Batch 370/2700 (13.70%), Loss: 0.3711\n",
      "Epoch 9, Batch 380/2700 (14.07%), Loss: 0.6179\n",
      "Epoch 9, Batch 390/2700 (14.44%), Loss: 0.0486\n",
      "Epoch 9, Batch 400/2700 (14.81%), Loss: 0.2182\n",
      "Epoch 9, Batch 410/2700 (15.19%), Loss: 2.7171\n",
      "Epoch 9, Batch 420/2700 (15.56%), Loss: 0.0107\n",
      "Epoch 9, Batch 430/2700 (15.93%), Loss: 1.2823\n",
      "Epoch 9, Batch 440/2700 (16.30%), Loss: 0.3694\n",
      "Epoch 9, Batch 450/2700 (16.67%), Loss: 0.4749\n",
      "Epoch 9, Batch 460/2700 (17.04%), Loss: 0.3744\n",
      "Epoch 9, Batch 470/2700 (17.41%), Loss: 0.0146\n",
      "Epoch 9, Batch 480/2700 (17.78%), Loss: 0.5637\n",
      "Epoch 9, Batch 490/2700 (18.15%), Loss: 0.0319\n",
      "Epoch 9, Batch 500/2700 (18.52%), Loss: 0.0322\n",
      "Epoch 9, Batch 510/2700 (18.89%), Loss: 0.4793\n",
      "Epoch 9, Batch 520/2700 (19.26%), Loss: 0.0798\n",
      "Epoch 9, Batch 530/2700 (19.63%), Loss: 0.9124\n",
      "Epoch 9, Batch 540/2700 (20.00%), Loss: 0.0453\n",
      "Epoch 9, Batch 550/2700 (20.37%), Loss: 0.1557\n",
      "Epoch 9, Batch 560/2700 (20.74%), Loss: 0.1069\n",
      "Epoch 9, Batch 570/2700 (21.11%), Loss: 0.2927\n",
      "Epoch 9, Batch 580/2700 (21.48%), Loss: 0.6908\n",
      "Epoch 9, Batch 590/2700 (21.85%), Loss: 0.3569\n",
      "Epoch 9, Batch 600/2700 (22.22%), Loss: 0.2288\n",
      "Epoch 9, Batch 610/2700 (22.59%), Loss: 0.1700\n",
      "Epoch 9, Batch 620/2700 (22.96%), Loss: 0.0909\n",
      "Epoch 9, Batch 630/2700 (23.33%), Loss: 0.1353\n",
      "Epoch 9, Batch 640/2700 (23.70%), Loss: 0.7278\n",
      "Epoch 9, Batch 650/2700 (24.07%), Loss: 0.2230\n",
      "Epoch 9, Batch 660/2700 (24.44%), Loss: 4.9520\n",
      "Epoch 9, Batch 670/2700 (24.81%), Loss: 0.0208\n",
      "Epoch 9, Batch 680/2700 (25.19%), Loss: 0.0072\n",
      "Epoch 9, Batch 690/2700 (25.56%), Loss: 0.3688\n",
      "Epoch 9, Batch 700/2700 (25.93%), Loss: 0.0073\n",
      "Epoch 9, Batch 710/2700 (26.30%), Loss: 0.2488\n",
      "Epoch 9, Batch 720/2700 (26.67%), Loss: 1.6833\n",
      "Epoch 9, Batch 730/2700 (27.04%), Loss: 0.8393\n",
      "Epoch 9, Batch 740/2700 (27.41%), Loss: 0.0098\n",
      "Epoch 9, Batch 750/2700 (27.78%), Loss: 0.4227\n",
      "Epoch 9, Batch 760/2700 (28.15%), Loss: 0.1599\n",
      "Epoch 9, Batch 770/2700 (28.52%), Loss: 0.5588\n",
      "Epoch 9, Batch 780/2700 (28.89%), Loss: 0.0486\n",
      "Epoch 9, Batch 790/2700 (29.26%), Loss: 0.7701\n",
      "Epoch 9, Batch 800/2700 (29.63%), Loss: 0.5332\n",
      "Epoch 9, Batch 810/2700 (30.00%), Loss: 0.0929\n",
      "Epoch 9, Batch 820/2700 (30.37%), Loss: 0.4322\n",
      "Epoch 9, Batch 830/2700 (30.74%), Loss: 0.2070\n",
      "Epoch 9, Batch 840/2700 (31.11%), Loss: 0.0040\n",
      "Epoch 9, Batch 850/2700 (31.48%), Loss: 3.5380\n",
      "Epoch 9, Batch 860/2700 (31.85%), Loss: 0.0360\n",
      "Epoch 9, Batch 870/2700 (32.22%), Loss: 0.3995\n",
      "Epoch 9, Batch 880/2700 (32.59%), Loss: 0.9913\n",
      "Epoch 9, Batch 890/2700 (32.96%), Loss: 0.5905\n",
      "Epoch 9, Batch 900/2700 (33.33%), Loss: 0.2589\n",
      "Epoch 9, Batch 910/2700 (33.70%), Loss: 0.0076\n",
      "Epoch 9, Batch 920/2700 (34.07%), Loss: 0.1255\n",
      "Epoch 9, Batch 930/2700 (34.44%), Loss: 0.1178\n",
      "Epoch 9, Batch 940/2700 (34.81%), Loss: 1.5570\n",
      "Epoch 9, Batch 950/2700 (35.19%), Loss: 0.6866\n",
      "Epoch 9, Batch 960/2700 (35.56%), Loss: 4.9210\n",
      "Epoch 9, Batch 970/2700 (35.93%), Loss: 0.5845\n",
      "Epoch 9, Batch 980/2700 (36.30%), Loss: 0.0359\n",
      "Epoch 9, Batch 990/2700 (36.67%), Loss: 0.9012\n",
      "Epoch 9, Batch 1000/2700 (37.04%), Loss: 0.0116\n",
      "Epoch 9, Batch 1010/2700 (37.41%), Loss: 0.0028\n",
      "Epoch 9, Batch 1020/2700 (37.78%), Loss: 1.0142\n",
      "Epoch 9, Batch 1030/2700 (38.15%), Loss: 0.4347\n",
      "Epoch 9, Batch 1040/2700 (38.52%), Loss: 0.4783\n",
      "Epoch 9, Batch 1050/2700 (38.89%), Loss: 0.2502\n",
      "Epoch 9, Batch 1060/2700 (39.26%), Loss: 1.1437\n",
      "Epoch 9, Batch 1070/2700 (39.63%), Loss: 0.1194\n",
      "Epoch 9, Batch 1080/2700 (40.00%), Loss: 0.0187\n",
      "Epoch 9, Batch 1090/2700 (40.37%), Loss: 0.1496\n",
      "Epoch 9, Batch 1100/2700 (40.74%), Loss: 1.5300\n",
      "Epoch 9, Batch 1110/2700 (41.11%), Loss: 0.0427\n",
      "Epoch 9, Batch 1120/2700 (41.48%), Loss: 1.0659\n",
      "Epoch 9, Batch 1130/2700 (41.85%), Loss: 0.0040\n",
      "Epoch 9, Batch 1140/2700 (42.22%), Loss: 0.0396\n",
      "Epoch 9, Batch 1150/2700 (42.59%), Loss: 0.7493\n",
      "Epoch 9, Batch 1160/2700 (42.96%), Loss: 0.0039\n",
      "Epoch 9, Batch 1170/2700 (43.33%), Loss: 0.6578\n",
      "Epoch 9, Batch 1180/2700 (43.70%), Loss: 0.9288\n",
      "Epoch 9, Batch 1190/2700 (44.07%), Loss: 0.1066\n",
      "Epoch 9, Batch 1200/2700 (44.44%), Loss: 0.0695\n",
      "Epoch 9, Batch 1210/2700 (44.81%), Loss: 0.8301\n",
      "Epoch 9, Batch 1220/2700 (45.19%), Loss: 0.2168\n",
      "Epoch 9, Batch 1230/2700 (45.56%), Loss: 0.2842\n",
      "Epoch 9, Batch 1240/2700 (45.93%), Loss: 0.0015\n",
      "Epoch 9, Batch 1250/2700 (46.30%), Loss: 0.1054\n",
      "Epoch 9, Batch 1260/2700 (46.67%), Loss: 0.1164\n",
      "Epoch 9, Batch 1270/2700 (47.04%), Loss: 0.0289\n",
      "Epoch 9, Batch 1280/2700 (47.41%), Loss: 0.2468\n",
      "Epoch 9, Batch 1290/2700 (47.78%), Loss: 0.0516\n",
      "Epoch 9, Batch 1300/2700 (48.15%), Loss: 0.7520\n",
      "Epoch 9, Batch 1310/2700 (48.52%), Loss: 1.0290\n",
      "Epoch 9, Batch 1320/2700 (48.89%), Loss: 0.1472\n",
      "Epoch 9, Batch 1330/2700 (49.26%), Loss: 0.0125\n",
      "Epoch 9, Batch 1340/2700 (49.63%), Loss: 0.2239\n",
      "Epoch 9, Batch 1350/2700 (50.00%), Loss: 0.0621\n",
      "Epoch 9, Batch 1360/2700 (50.37%), Loss: 0.5452\n",
      "Epoch 9, Batch 1370/2700 (50.74%), Loss: 0.1572\n",
      "Epoch 9, Batch 1380/2700 (51.11%), Loss: 0.1582\n",
      "Epoch 9, Batch 1390/2700 (51.48%), Loss: 4.3637\n",
      "Epoch 9, Batch 1400/2700 (51.85%), Loss: 0.1872\n",
      "Epoch 9, Batch 1410/2700 (52.22%), Loss: 0.5475\n",
      "Epoch 9, Batch 1420/2700 (52.59%), Loss: 0.5021\n",
      "Epoch 9, Batch 1430/2700 (52.96%), Loss: 0.1238\n",
      "Epoch 9, Batch 1440/2700 (53.33%), Loss: 0.3381\n",
      "Epoch 9, Batch 1450/2700 (53.70%), Loss: 2.6989\n",
      "Epoch 9, Batch 1460/2700 (54.07%), Loss: 0.2325\n",
      "Epoch 9, Batch 1470/2700 (54.44%), Loss: 0.0067\n",
      "Epoch 9, Batch 1480/2700 (54.81%), Loss: 0.0198\n",
      "Epoch 9, Batch 1490/2700 (55.19%), Loss: 0.3279\n",
      "Epoch 9, Batch 1500/2700 (55.56%), Loss: 0.1825\n",
      "Epoch 9, Batch 1510/2700 (55.93%), Loss: 0.3379\n",
      "Epoch 9, Batch 1520/2700 (56.30%), Loss: 0.2256\n",
      "Epoch 9, Batch 1530/2700 (56.67%), Loss: 0.1284\n",
      "Epoch 9, Batch 1540/2700 (57.04%), Loss: 0.0723\n",
      "Epoch 9, Batch 1550/2700 (57.41%), Loss: 0.0252\n",
      "Epoch 9, Batch 1560/2700 (57.78%), Loss: 0.2028\n",
      "Epoch 9, Batch 1570/2700 (58.15%), Loss: 0.0833\n",
      "Epoch 9, Batch 1580/2700 (58.52%), Loss: 1.4597\n",
      "Epoch 9, Batch 1590/2700 (58.89%), Loss: 0.0414\n",
      "Epoch 9, Batch 1600/2700 (59.26%), Loss: 0.1488\n",
      "Epoch 9, Batch 1610/2700 (59.63%), Loss: 0.1962\n",
      "Epoch 9, Batch 1620/2700 (60.00%), Loss: 0.0118\n",
      "Epoch 9, Batch 1630/2700 (60.37%), Loss: 0.0438\n",
      "Epoch 9, Batch 1640/2700 (60.74%), Loss: 2.4125\n",
      "Epoch 9, Batch 1650/2700 (61.11%), Loss: 0.1403\n",
      "Epoch 9, Batch 1660/2700 (61.48%), Loss: 0.2059\n",
      "Epoch 9, Batch 1670/2700 (61.85%), Loss: 0.0087\n",
      "Epoch 9, Batch 1680/2700 (62.22%), Loss: 0.0935\n",
      "Epoch 9, Batch 1690/2700 (62.59%), Loss: 0.0257\n",
      "Epoch 9, Batch 1700/2700 (62.96%), Loss: 0.0967\n",
      "Epoch 9, Batch 1710/2700 (63.33%), Loss: 0.0097\n",
      "Epoch 9, Batch 1720/2700 (63.70%), Loss: 0.9139\n",
      "Epoch 9, Batch 1730/2700 (64.07%), Loss: 0.1906\n",
      "Epoch 9, Batch 1740/2700 (64.44%), Loss: 0.0424\n",
      "Epoch 9, Batch 1750/2700 (64.81%), Loss: 0.4858\n",
      "Epoch 9, Batch 1760/2700 (65.19%), Loss: 0.2220\n",
      "Epoch 9, Batch 1770/2700 (65.56%), Loss: 0.4467\n",
      "Epoch 9, Batch 1780/2700 (65.93%), Loss: 0.7020\n",
      "Epoch 9, Batch 1790/2700 (66.30%), Loss: 0.0170\n",
      "Epoch 9, Batch 1800/2700 (66.67%), Loss: 0.0076\n",
      "Epoch 9, Batch 1810/2700 (67.04%), Loss: 0.5555\n",
      "Epoch 9, Batch 1820/2700 (67.41%), Loss: 0.2142\n",
      "Epoch 9, Batch 1830/2700 (67.78%), Loss: 0.0167\n",
      "Epoch 9, Batch 1840/2700 (68.15%), Loss: 0.3271\n",
      "Epoch 9, Batch 1850/2700 (68.52%), Loss: 0.1418\n",
      "Epoch 9, Batch 1860/2700 (68.89%), Loss: 0.2114\n",
      "Epoch 9, Batch 1870/2700 (69.26%), Loss: 0.2197\n",
      "Epoch 9, Batch 1880/2700 (69.63%), Loss: 0.0075\n",
      "Epoch 9, Batch 1890/2700 (70.00%), Loss: 0.6593\n",
      "Epoch 9, Batch 1900/2700 (70.37%), Loss: 0.3355\n",
      "Epoch 9, Batch 1910/2700 (70.74%), Loss: 0.0660\n",
      "Epoch 9, Batch 1920/2700 (71.11%), Loss: 2.5884\n",
      "Epoch 9, Batch 1930/2700 (71.48%), Loss: 0.8098\n",
      "Epoch 9, Batch 1940/2700 (71.85%), Loss: 0.0703\n",
      "Epoch 9, Batch 1950/2700 (72.22%), Loss: 0.1664\n",
      "Epoch 9, Batch 1960/2700 (72.59%), Loss: 0.0222\n",
      "Epoch 9, Batch 1970/2700 (72.96%), Loss: 0.0128\n",
      "Epoch 9, Batch 1980/2700 (73.33%), Loss: 0.2927\n",
      "Epoch 9, Batch 1990/2700 (73.70%), Loss: 0.0037\n",
      "Epoch 9, Batch 2000/2700 (74.07%), Loss: 0.4843\n",
      "Epoch 9, Batch 2010/2700 (74.44%), Loss: 0.0752\n",
      "Epoch 9, Batch 2020/2700 (74.81%), Loss: 0.1237\n",
      "Epoch 9, Batch 2030/2700 (75.19%), Loss: 0.0326\n",
      "Epoch 9, Batch 2040/2700 (75.56%), Loss: 0.0081\n",
      "Epoch 9, Batch 2050/2700 (75.93%), Loss: 0.6110\n",
      "Epoch 9, Batch 2060/2700 (76.30%), Loss: 0.0905\n",
      "Epoch 9, Batch 2070/2700 (76.67%), Loss: 0.2286\n",
      "Epoch 9, Batch 2080/2700 (77.04%), Loss: 0.1047\n",
      "Epoch 9, Batch 2090/2700 (77.41%), Loss: 0.0037\n",
      "Epoch 9, Batch 2100/2700 (77.78%), Loss: 0.3497\n",
      "Epoch 9, Batch 2110/2700 (78.15%), Loss: 0.2006\n",
      "Epoch 9, Batch 2120/2700 (78.52%), Loss: 0.0064\n",
      "Epoch 9, Batch 2130/2700 (78.89%), Loss: 0.1343\n",
      "Epoch 9, Batch 2140/2700 (79.26%), Loss: 0.1876\n",
      "Epoch 9, Batch 2150/2700 (79.63%), Loss: 0.3384\n",
      "Epoch 9, Batch 2160/2700 (80.00%), Loss: 0.3994\n",
      "Epoch 9, Batch 2170/2700 (80.37%), Loss: 0.0062\n",
      "Epoch 9, Batch 2180/2700 (80.74%), Loss: 0.1381\n",
      "Epoch 9, Batch 2190/2700 (81.11%), Loss: 0.0916\n",
      "Epoch 9, Batch 2200/2700 (81.48%), Loss: 0.0032\n",
      "Epoch 9, Batch 2210/2700 (81.85%), Loss: 0.1904\n",
      "Epoch 9, Batch 2220/2700 (82.22%), Loss: 0.0607\n",
      "Epoch 9, Batch 2230/2700 (82.59%), Loss: 0.0161\n",
      "Epoch 9, Batch 2240/2700 (82.96%), Loss: 0.0113\n",
      "Epoch 9, Batch 2250/2700 (83.33%), Loss: 0.2413\n",
      "Epoch 9, Batch 2260/2700 (83.70%), Loss: 0.1688\n",
      "Epoch 9, Batch 2270/2700 (84.07%), Loss: 0.0450\n",
      "Epoch 9, Batch 2280/2700 (84.44%), Loss: 0.2692\n",
      "Epoch 9, Batch 2290/2700 (84.81%), Loss: 0.0333\n",
      "Epoch 9, Batch 2300/2700 (85.19%), Loss: 0.1022\n",
      "Epoch 9, Batch 2310/2700 (85.56%), Loss: 0.1031\n",
      "Epoch 9, Batch 2320/2700 (85.93%), Loss: 3.2259\n",
      "Epoch 9, Batch 2330/2700 (86.30%), Loss: 0.9213\n",
      "Epoch 9, Batch 2340/2700 (86.67%), Loss: 0.2122\n",
      "Epoch 9, Batch 2350/2700 (87.04%), Loss: 0.0612\n",
      "Epoch 9, Batch 2360/2700 (87.41%), Loss: 0.0845\n",
      "Epoch 9, Batch 2370/2700 (87.78%), Loss: 0.0311\n",
      "Epoch 9, Batch 2380/2700 (88.15%), Loss: 9.1494\n",
      "Epoch 9, Batch 2390/2700 (88.52%), Loss: 1.5361\n",
      "Epoch 9, Batch 2400/2700 (88.89%), Loss: 0.2440\n",
      "Epoch 9, Batch 2410/2700 (89.26%), Loss: 7.5496\n",
      "Epoch 9, Batch 2420/2700 (89.63%), Loss: 0.5535\n",
      "Epoch 9, Batch 2430/2700 (90.00%), Loss: 0.0899\n",
      "Epoch 9, Batch 2440/2700 (90.37%), Loss: 0.0082\n",
      "Epoch 9, Batch 2450/2700 (90.74%), Loss: 0.1627\n",
      "Epoch 9, Batch 2460/2700 (91.11%), Loss: 0.1904\n",
      "Epoch 9, Batch 2470/2700 (91.48%), Loss: 0.0688\n",
      "Epoch 9, Batch 2480/2700 (91.85%), Loss: 0.0154\n",
      "Epoch 9, Batch 2490/2700 (92.22%), Loss: 0.2888\n",
      "Epoch 9, Batch 2500/2700 (92.59%), Loss: 0.1052\n",
      "Epoch 9, Batch 2510/2700 (92.96%), Loss: 0.0859\n",
      "Epoch 9, Batch 2520/2700 (93.33%), Loss: 0.4710\n",
      "Epoch 9, Batch 2530/2700 (93.70%), Loss: 0.1566\n",
      "Epoch 9, Batch 2540/2700 (94.07%), Loss: 0.0096\n",
      "Epoch 9, Batch 2550/2700 (94.44%), Loss: 0.0075\n",
      "Epoch 9, Batch 2560/2700 (94.81%), Loss: 0.4773\n",
      "Epoch 9, Batch 2570/2700 (95.19%), Loss: 0.3102\n",
      "Epoch 9, Batch 2580/2700 (95.56%), Loss: 0.0556\n",
      "Epoch 9, Batch 2590/2700 (95.93%), Loss: 0.9125\n",
      "Epoch 9, Batch 2600/2700 (96.30%), Loss: 0.1416\n",
      "Epoch 9, Batch 2610/2700 (96.67%), Loss: 0.1141\n",
      "Epoch 9, Batch 2620/2700 (97.04%), Loss: 0.2171\n",
      "Epoch 9, Batch 2630/2700 (97.41%), Loss: 0.0166\n",
      "Epoch 9, Batch 2640/2700 (97.78%), Loss: 0.2117\n",
      "Epoch 9, Batch 2650/2700 (98.15%), Loss: 0.3121\n",
      "Epoch 9, Batch 2660/2700 (98.52%), Loss: 1.1754\n",
      "Epoch 9, Batch 2670/2700 (98.89%), Loss: 0.1294\n",
      "Epoch 9, Batch 2680/2700 (99.26%), Loss: 0.0989\n",
      "Epoch 9, Batch 2690/2700 (99.63%), Loss: 0.0747\n",
      "Epoch 9, Batch 2700/2700 (100.00%), Loss: 0.3402\n",
      "Epoch 9 completed. Average loss: 0.4131\n",
      "Epoch 10, Batch 10/2700 (0.37%), Loss: 0.0884\n",
      "Epoch 10, Batch 20/2700 (0.74%), Loss: 0.0230\n",
      "Epoch 10, Batch 30/2700 (1.11%), Loss: 0.0497\n",
      "Epoch 10, Batch 40/2700 (1.48%), Loss: 0.0702\n",
      "Epoch 10, Batch 50/2700 (1.85%), Loss: 0.1146\n",
      "Epoch 10, Batch 60/2700 (2.22%), Loss: 0.0815\n",
      "Epoch 10, Batch 70/2700 (2.59%), Loss: 0.1530\n",
      "Epoch 10, Batch 80/2700 (2.96%), Loss: 0.0983\n",
      "Epoch 10, Batch 90/2700 (3.33%), Loss: 0.1682\n",
      "Epoch 10, Batch 100/2700 (3.70%), Loss: 0.4778\n",
      "Epoch 10, Batch 110/2700 (4.07%), Loss: 0.1392\n",
      "Epoch 10, Batch 120/2700 (4.44%), Loss: 0.1054\n",
      "Epoch 10, Batch 130/2700 (4.81%), Loss: 0.7110\n",
      "Epoch 10, Batch 140/2700 (5.19%), Loss: 0.0517\n",
      "Epoch 10, Batch 150/2700 (5.56%), Loss: 0.0086\n",
      "Epoch 10, Batch 160/2700 (5.93%), Loss: 0.0361\n",
      "Epoch 10, Batch 170/2700 (6.30%), Loss: 0.2504\n",
      "Epoch 10, Batch 180/2700 (6.67%), Loss: 0.1223\n",
      "Epoch 10, Batch 190/2700 (7.04%), Loss: 0.5801\n",
      "Epoch 10, Batch 200/2700 (7.41%), Loss: 0.0044\n",
      "Epoch 10, Batch 210/2700 (7.78%), Loss: 0.1400\n",
      "Epoch 10, Batch 220/2700 (8.15%), Loss: 0.5721\n",
      "Epoch 10, Batch 230/2700 (8.52%), Loss: 0.0886\n",
      "Epoch 10, Batch 240/2700 (8.89%), Loss: 1.3162\n",
      "Epoch 10, Batch 250/2700 (9.26%), Loss: 0.0041\n",
      "Epoch 10, Batch 260/2700 (9.63%), Loss: 1.9231\n",
      "Epoch 10, Batch 270/2700 (10.00%), Loss: 5.4811\n",
      "Epoch 10, Batch 280/2700 (10.37%), Loss: 0.0948\n",
      "Epoch 10, Batch 290/2700 (10.74%), Loss: 0.2053\n",
      "Epoch 10, Batch 300/2700 (11.11%), Loss: 0.0426\n",
      "Epoch 10, Batch 310/2700 (11.48%), Loss: 2.4304\n",
      "Epoch 10, Batch 320/2700 (11.85%), Loss: 0.1608\n",
      "Epoch 10, Batch 330/2700 (12.22%), Loss: 0.2153\n",
      "Epoch 10, Batch 340/2700 (12.59%), Loss: 0.6309\n",
      "Epoch 10, Batch 350/2700 (12.96%), Loss: 0.0439\n",
      "Epoch 10, Batch 360/2700 (13.33%), Loss: 0.5329\n",
      "Epoch 10, Batch 370/2700 (13.70%), Loss: 0.2914\n",
      "Epoch 10, Batch 380/2700 (14.07%), Loss: 0.4504\n",
      "Epoch 10, Batch 390/2700 (14.44%), Loss: 0.0372\n",
      "Epoch 10, Batch 400/2700 (14.81%), Loss: 0.1320\n",
      "Epoch 10, Batch 410/2700 (15.19%), Loss: 1.7494\n",
      "Epoch 10, Batch 420/2700 (15.56%), Loss: 0.0081\n",
      "Epoch 10, Batch 430/2700 (15.93%), Loss: 1.3632\n",
      "Epoch 10, Batch 440/2700 (16.30%), Loss: 0.3555\n",
      "Epoch 10, Batch 450/2700 (16.67%), Loss: 0.3889\n",
      "Epoch 10, Batch 460/2700 (17.04%), Loss: 0.2859\n",
      "Epoch 10, Batch 470/2700 (17.41%), Loss: 0.0068\n",
      "Epoch 10, Batch 480/2700 (17.78%), Loss: 0.4097\n",
      "Epoch 10, Batch 490/2700 (18.15%), Loss: 0.0225\n",
      "Epoch 10, Batch 500/2700 (18.52%), Loss: 0.0182\n",
      "Epoch 10, Batch 510/2700 (18.89%), Loss: 0.3355\n",
      "Epoch 10, Batch 520/2700 (19.26%), Loss: 0.0619\n",
      "Epoch 10, Batch 530/2700 (19.63%), Loss: 0.6185\n",
      "Epoch 10, Batch 540/2700 (20.00%), Loss: 0.0277\n",
      "Epoch 10, Batch 550/2700 (20.37%), Loss: 0.1349\n",
      "Epoch 10, Batch 560/2700 (20.74%), Loss: 0.0592\n",
      "Epoch 10, Batch 570/2700 (21.11%), Loss: 0.2370\n",
      "Epoch 10, Batch 580/2700 (21.48%), Loss: 0.4700\n",
      "Epoch 10, Batch 590/2700 (21.85%), Loss: 0.2634\n",
      "Epoch 10, Batch 600/2700 (22.22%), Loss: 0.1627\n",
      "Epoch 10, Batch 610/2700 (22.59%), Loss: 0.1042\n",
      "Epoch 10, Batch 620/2700 (22.96%), Loss: 0.0784\n",
      "Epoch 10, Batch 630/2700 (23.33%), Loss: 0.1216\n",
      "Epoch 10, Batch 640/2700 (23.70%), Loss: 0.4193\n",
      "Epoch 10, Batch 650/2700 (24.07%), Loss: 0.1600\n",
      "Epoch 10, Batch 660/2700 (24.44%), Loss: 3.2719\n",
      "Epoch 10, Batch 670/2700 (24.81%), Loss: 0.0175\n",
      "Epoch 10, Batch 680/2700 (25.19%), Loss: 0.0090\n",
      "Epoch 10, Batch 690/2700 (25.56%), Loss: 0.2191\n",
      "Epoch 10, Batch 700/2700 (25.93%), Loss: 0.0078\n",
      "Epoch 10, Batch 710/2700 (26.30%), Loss: 0.1532\n",
      "Epoch 10, Batch 720/2700 (26.67%), Loss: 1.2705\n",
      "Epoch 10, Batch 730/2700 (27.04%), Loss: 0.5685\n",
      "Epoch 10, Batch 740/2700 (27.41%), Loss: 0.0098\n",
      "Epoch 10, Batch 750/2700 (27.78%), Loss: 0.2456\n",
      "Epoch 10, Batch 760/2700 (28.15%), Loss: 0.1248\n",
      "Epoch 10, Batch 770/2700 (28.52%), Loss: 0.3331\n",
      "Epoch 10, Batch 780/2700 (28.89%), Loss: 0.0363\n",
      "Epoch 10, Batch 790/2700 (29.26%), Loss: 2.2355\n",
      "Epoch 10, Batch 800/2700 (29.63%), Loss: 0.4244\n",
      "Epoch 10, Batch 810/2700 (30.00%), Loss: 0.0394\n",
      "Epoch 10, Batch 820/2700 (30.37%), Loss: 0.3097\n",
      "Epoch 10, Batch 830/2700 (30.74%), Loss: 0.1503\n",
      "Epoch 10, Batch 840/2700 (31.11%), Loss: 0.0041\n",
      "Epoch 10, Batch 850/2700 (31.48%), Loss: 2.9245\n",
      "Epoch 10, Batch 860/2700 (31.85%), Loss: 0.0376\n",
      "Epoch 10, Batch 870/2700 (32.22%), Loss: 0.3569\n",
      "Epoch 10, Batch 880/2700 (32.59%), Loss: 0.6693\n",
      "Epoch 10, Batch 890/2700 (32.96%), Loss: 0.2059\n",
      "Epoch 10, Batch 900/2700 (33.33%), Loss: 0.2336\n",
      "Epoch 10, Batch 910/2700 (33.70%), Loss: 0.0051\n",
      "Epoch 10, Batch 920/2700 (34.07%), Loss: 0.1084\n",
      "Epoch 10, Batch 930/2700 (34.44%), Loss: 0.1181\n",
      "Epoch 10, Batch 940/2700 (34.81%), Loss: 0.8492\n",
      "Epoch 10, Batch 950/2700 (35.19%), Loss: 0.3263\n",
      "Epoch 10, Batch 960/2700 (35.56%), Loss: 6.8398\n",
      "Epoch 10, Batch 970/2700 (35.93%), Loss: 0.3331\n",
      "Epoch 10, Batch 980/2700 (36.30%), Loss: 0.0262\n",
      "Epoch 10, Batch 990/2700 (36.67%), Loss: 0.6617\n",
      "Epoch 10, Batch 1000/2700 (37.04%), Loss: 0.0079\n",
      "Epoch 10, Batch 1010/2700 (37.41%), Loss: 0.0030\n",
      "Epoch 10, Batch 1020/2700 (37.78%), Loss: 0.6625\n",
      "Epoch 10, Batch 1030/2700 (38.15%), Loss: 0.2446\n",
      "Epoch 10, Batch 1040/2700 (38.52%), Loss: 0.3051\n",
      "Epoch 10, Batch 1050/2700 (38.89%), Loss: 0.5574\n",
      "Epoch 10, Batch 1060/2700 (39.26%), Loss: 0.9438\n",
      "Epoch 10, Batch 1070/2700 (39.63%), Loss: 0.1061\n",
      "Epoch 10, Batch 1080/2700 (40.00%), Loss: 0.0164\n",
      "Epoch 10, Batch 1090/2700 (40.37%), Loss: 0.0854\n",
      "Epoch 10, Batch 1100/2700 (40.74%), Loss: 1.1991\n",
      "Epoch 10, Batch 1110/2700 (41.11%), Loss: 0.0331\n",
      "Epoch 10, Batch 1120/2700 (41.48%), Loss: 0.4553\n",
      "Epoch 10, Batch 1130/2700 (41.85%), Loss: 0.0039\n",
      "Epoch 10, Batch 1140/2700 (42.22%), Loss: 0.0237\n",
      "Epoch 10, Batch 1150/2700 (42.59%), Loss: 0.6132\n",
      "Epoch 10, Batch 1160/2700 (42.96%), Loss: 0.0038\n",
      "Epoch 10, Batch 1170/2700 (43.33%), Loss: 0.5488\n",
      "Epoch 10, Batch 1180/2700 (43.70%), Loss: 0.6920\n",
      "Epoch 10, Batch 1190/2700 (44.07%), Loss: 0.0363\n",
      "Epoch 10, Batch 1200/2700 (44.44%), Loss: 0.0490\n",
      "Epoch 10, Batch 1210/2700 (44.81%), Loss: 0.6019\n",
      "Epoch 10, Batch 1220/2700 (45.19%), Loss: 0.2053\n",
      "Epoch 10, Batch 1230/2700 (45.56%), Loss: 0.2383\n",
      "Epoch 10, Batch 1240/2700 (45.93%), Loss: 0.0023\n",
      "Epoch 10, Batch 1250/2700 (46.30%), Loss: 0.1020\n",
      "Epoch 10, Batch 1260/2700 (46.67%), Loss: 0.0826\n",
      "Epoch 10, Batch 1270/2700 (47.04%), Loss: 0.0240\n",
      "Epoch 10, Batch 1280/2700 (47.41%), Loss: 0.2485\n",
      "Epoch 10, Batch 1290/2700 (47.78%), Loss: 0.0308\n",
      "Epoch 10, Batch 1300/2700 (48.15%), Loss: 0.6296\n",
      "Epoch 10, Batch 1310/2700 (48.52%), Loss: 0.7485\n",
      "Epoch 10, Batch 1320/2700 (48.89%), Loss: 0.1840\n",
      "Epoch 10, Batch 1330/2700 (49.26%), Loss: 0.0107\n",
      "Epoch 10, Batch 1340/2700 (49.63%), Loss: 0.2522\n",
      "Epoch 10, Batch 1350/2700 (50.00%), Loss: 0.0392\n",
      "Epoch 10, Batch 1360/2700 (50.37%), Loss: 0.4619\n",
      "Epoch 10, Batch 1370/2700 (50.74%), Loss: 0.1216\n",
      "Epoch 10, Batch 1380/2700 (51.11%), Loss: 0.1210\n",
      "Epoch 10, Batch 1390/2700 (51.48%), Loss: 3.9164\n",
      "Epoch 10, Batch 1400/2700 (51.85%), Loss: 0.1857\n",
      "Epoch 10, Batch 1410/2700 (52.22%), Loss: 0.2939\n",
      "Epoch 10, Batch 1420/2700 (52.59%), Loss: 0.4409\n",
      "Epoch 10, Batch 1430/2700 (52.96%), Loss: 0.1080\n",
      "Epoch 10, Batch 1440/2700 (53.33%), Loss: 0.2328\n",
      "Epoch 10, Batch 1450/2700 (53.70%), Loss: 2.0853\n",
      "Epoch 10, Batch 1460/2700 (54.07%), Loss: 0.1603\n",
      "Epoch 10, Batch 1470/2700 (54.44%), Loss: 0.0073\n",
      "Epoch 10, Batch 1480/2700 (54.81%), Loss: 0.0153\n",
      "Epoch 10, Batch 1490/2700 (55.19%), Loss: 0.2593\n",
      "Epoch 10, Batch 1500/2700 (55.56%), Loss: 0.1465\n",
      "Epoch 10, Batch 1510/2700 (55.93%), Loss: 0.2247\n",
      "Epoch 10, Batch 1520/2700 (56.30%), Loss: 0.2571\n",
      "Epoch 10, Batch 1530/2700 (56.67%), Loss: 0.1266\n",
      "Epoch 10, Batch 1540/2700 (57.04%), Loss: 0.0660\n",
      "Epoch 10, Batch 1550/2700 (57.41%), Loss: 0.0234\n",
      "Epoch 10, Batch 1560/2700 (57.78%), Loss: 0.0956\n",
      "Epoch 10, Batch 1570/2700 (58.15%), Loss: 0.0598\n",
      "Epoch 10, Batch 1580/2700 (58.52%), Loss: 0.9123\n",
      "Epoch 10, Batch 1590/2700 (58.89%), Loss: 0.0214\n",
      "Epoch 10, Batch 1600/2700 (59.26%), Loss: 0.1350\n",
      "Epoch 10, Batch 1610/2700 (59.63%), Loss: 0.1523\n",
      "Epoch 10, Batch 1620/2700 (60.00%), Loss: 0.0084\n",
      "Epoch 10, Batch 1630/2700 (60.37%), Loss: 0.0262\n",
      "Epoch 10, Batch 1640/2700 (60.74%), Loss: 1.8055\n",
      "Epoch 10, Batch 1650/2700 (61.11%), Loss: 0.1359\n",
      "Epoch 10, Batch 1660/2700 (61.48%), Loss: 0.1685\n",
      "Epoch 10, Batch 1670/2700 (61.85%), Loss: 0.0057\n",
      "Epoch 10, Batch 1680/2700 (62.22%), Loss: 0.0754\n",
      "Epoch 10, Batch 1690/2700 (62.59%), Loss: 0.0195\n",
      "Epoch 10, Batch 1700/2700 (62.96%), Loss: 0.0919\n",
      "Epoch 10, Batch 1710/2700 (63.33%), Loss: 0.0052\n",
      "Epoch 10, Batch 1720/2700 (63.70%), Loss: 0.6146\n",
      "Epoch 10, Batch 1730/2700 (64.07%), Loss: 0.1324\n",
      "Epoch 10, Batch 1740/2700 (64.44%), Loss: 0.0385\n",
      "Epoch 10, Batch 1750/2700 (64.81%), Loss: 0.3525\n",
      "Epoch 10, Batch 1760/2700 (65.19%), Loss: 0.1997\n",
      "Epoch 10, Batch 1770/2700 (65.56%), Loss: 0.3382\n",
      "Epoch 10, Batch 1780/2700 (65.93%), Loss: 0.4882\n",
      "Epoch 10, Batch 1790/2700 (66.30%), Loss: 0.0125\n",
      "Epoch 10, Batch 1800/2700 (66.67%), Loss: 0.0057\n",
      "Epoch 10, Batch 1810/2700 (67.04%), Loss: 0.4513\n",
      "Epoch 10, Batch 1820/2700 (67.41%), Loss: 0.1590\n",
      "Epoch 10, Batch 1830/2700 (67.78%), Loss: 0.0178\n",
      "Epoch 10, Batch 1840/2700 (68.15%), Loss: 0.3228\n",
      "Epoch 10, Batch 1850/2700 (68.52%), Loss: 0.1078\n",
      "Epoch 10, Batch 1860/2700 (68.89%), Loss: 0.1665\n",
      "Epoch 10, Batch 1870/2700 (69.26%), Loss: 0.1854\n",
      "Epoch 10, Batch 1880/2700 (69.63%), Loss: 0.0050\n",
      "Epoch 10, Batch 1890/2700 (70.00%), Loss: 0.5752\n",
      "Epoch 10, Batch 1900/2700 (70.37%), Loss: 0.2274\n",
      "Epoch 10, Batch 1910/2700 (70.74%), Loss: 0.0474\n",
      "Epoch 10, Batch 1920/2700 (71.11%), Loss: 1.4824\n",
      "Epoch 10, Batch 1930/2700 (71.48%), Loss: 0.4720\n",
      "Epoch 10, Batch 1940/2700 (71.85%), Loss: 0.0708\n",
      "Epoch 10, Batch 1950/2700 (72.22%), Loss: 0.1842\n",
      "Epoch 10, Batch 1960/2700 (72.59%), Loss: 0.0191\n",
      "Epoch 10, Batch 1970/2700 (72.96%), Loss: 0.0134\n",
      "Epoch 10, Batch 1980/2700 (73.33%), Loss: 0.2352\n",
      "Epoch 10, Batch 1990/2700 (73.70%), Loss: 0.0037\n",
      "Epoch 10, Batch 2000/2700 (74.07%), Loss: 0.3948\n",
      "Epoch 10, Batch 2010/2700 (74.44%), Loss: 0.0555\n",
      "Epoch 10, Batch 2020/2700 (74.81%), Loss: 0.0845\n",
      "Epoch 10, Batch 2030/2700 (75.19%), Loss: 0.0178\n",
      "Epoch 10, Batch 2040/2700 (75.56%), Loss: 0.0074\n",
      "Epoch 10, Batch 2050/2700 (75.93%), Loss: 0.4537\n",
      "Epoch 10, Batch 2060/2700 (76.30%), Loss: 0.0570\n",
      "Epoch 10, Batch 2070/2700 (76.67%), Loss: 0.2043\n",
      "Epoch 10, Batch 2080/2700 (77.04%), Loss: 0.4533\n",
      "Epoch 10, Batch 2090/2700 (77.41%), Loss: 0.0021\n",
      "Epoch 10, Batch 2100/2700 (77.78%), Loss: 0.0540\n",
      "Epoch 10, Batch 2110/2700 (78.15%), Loss: 0.1423\n",
      "Epoch 10, Batch 2120/2700 (78.52%), Loss: 0.0054\n",
      "Epoch 10, Batch 2130/2700 (78.89%), Loss: 0.0980\n",
      "Epoch 10, Batch 2140/2700 (79.26%), Loss: 0.1551\n",
      "Epoch 10, Batch 2150/2700 (79.63%), Loss: 0.3182\n",
      "Epoch 10, Batch 2160/2700 (80.00%), Loss: 0.2682\n",
      "Epoch 10, Batch 2170/2700 (80.37%), Loss: 0.0063\n",
      "Epoch 10, Batch 2180/2700 (80.74%), Loss: 0.1096\n",
      "Epoch 10, Batch 2190/2700 (81.11%), Loss: 0.0790\n",
      "Epoch 10, Batch 2200/2700 (81.48%), Loss: 0.0015\n",
      "Epoch 10, Batch 2210/2700 (81.85%), Loss: 0.1364\n",
      "Epoch 10, Batch 2220/2700 (82.22%), Loss: 0.0366\n",
      "Epoch 10, Batch 2230/2700 (82.59%), Loss: 0.0126\n",
      "Epoch 10, Batch 2240/2700 (82.96%), Loss: 0.0089\n",
      "Epoch 10, Batch 2250/2700 (83.33%), Loss: 0.1301\n",
      "Epoch 10, Batch 2260/2700 (83.70%), Loss: 0.1617\n",
      "Epoch 10, Batch 2270/2700 (84.07%), Loss: 0.0408\n",
      "Epoch 10, Batch 2280/2700 (84.44%), Loss: 0.1744\n",
      "Epoch 10, Batch 2290/2700 (84.81%), Loss: 0.0377\n",
      "Epoch 10, Batch 2300/2700 (85.19%), Loss: 0.0403\n",
      "Epoch 10, Batch 2310/2700 (85.56%), Loss: 0.0916\n",
      "Epoch 10, Batch 2320/2700 (85.93%), Loss: 1.9266\n",
      "Epoch 10, Batch 2330/2700 (86.30%), Loss: 0.7090\n",
      "Epoch 10, Batch 2340/2700 (86.67%), Loss: 0.1240\n",
      "Epoch 10, Batch 2350/2700 (87.04%), Loss: 0.0396\n",
      "Epoch 10, Batch 2360/2700 (87.41%), Loss: 0.0770\n",
      "Epoch 10, Batch 2370/2700 (87.78%), Loss: 0.0179\n",
      "Epoch 10, Batch 2380/2700 (88.15%), Loss: 5.8091\n",
      "Epoch 10, Batch 2390/2700 (88.52%), Loss: 0.8219\n",
      "Epoch 10, Batch 2400/2700 (88.89%), Loss: 0.1926\n",
      "Epoch 10, Batch 2410/2700 (89.26%), Loss: 7.2780\n",
      "Epoch 10, Batch 2420/2700 (89.63%), Loss: 0.4310\n",
      "Epoch 10, Batch 2430/2700 (90.00%), Loss: 0.0665\n",
      "Epoch 10, Batch 2440/2700 (90.37%), Loss: 0.0051\n",
      "Epoch 10, Batch 2450/2700 (90.74%), Loss: 0.1476\n",
      "Epoch 10, Batch 2460/2700 (91.11%), Loss: 0.1863\n",
      "Epoch 10, Batch 2470/2700 (91.48%), Loss: 0.0557\n",
      "Epoch 10, Batch 2480/2700 (91.85%), Loss: 0.0131\n",
      "Epoch 10, Batch 2490/2700 (92.22%), Loss: 0.2346\n",
      "Epoch 10, Batch 2500/2700 (92.59%), Loss: 0.0428\n",
      "Epoch 10, Batch 2510/2700 (92.96%), Loss: 0.0389\n",
      "Epoch 10, Batch 2520/2700 (93.33%), Loss: 0.3828\n",
      "Epoch 10, Batch 2530/2700 (93.70%), Loss: 0.1144\n",
      "Epoch 10, Batch 2540/2700 (94.07%), Loss: 0.0096\n",
      "Epoch 10, Batch 2550/2700 (94.44%), Loss: 0.0052\n",
      "Epoch 10, Batch 2560/2700 (94.81%), Loss: 0.3210\n",
      "Epoch 10, Batch 2570/2700 (95.19%), Loss: 0.2336\n",
      "Epoch 10, Batch 2580/2700 (95.56%), Loss: 0.0471\n",
      "Epoch 10, Batch 2590/2700 (95.93%), Loss: 0.7006\n",
      "Epoch 10, Batch 2600/2700 (96.30%), Loss: 0.1106\n",
      "Epoch 10, Batch 2610/2700 (96.67%), Loss: 0.1013\n",
      "Epoch 10, Batch 2620/2700 (97.04%), Loss: 0.1664\n",
      "Epoch 10, Batch 2630/2700 (97.41%), Loss: 0.0144\n",
      "Epoch 10, Batch 2640/2700 (97.78%), Loss: 0.1450\n",
      "Epoch 10, Batch 2650/2700 (98.15%), Loss: 0.2062\n",
      "Epoch 10, Batch 2660/2700 (98.52%), Loss: 0.6836\n",
      "Epoch 10, Batch 2670/2700 (98.89%), Loss: 0.1130\n",
      "Epoch 10, Batch 2680/2700 (99.26%), Loss: 0.0900\n",
      "Epoch 10, Batch 2690/2700 (99.63%), Loss: 0.0615\n",
      "Epoch 10, Batch 2700/2700 (100.00%), Loss: 0.2682\n",
      "Epoch 10 completed. Average loss: 0.3080\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TAGSET_SIZE = len(tag2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "# Initialize the model\n",
    "model = BiLSTM_CRF(VOCAB_SIZE, TAGSET_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "print(\"Initialized BiLSTM-CRF model.\")\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "print(\"Initialized optimizer.\")\n",
    "\n",
    "# Step 3: Training Loop\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_words)\n",
    "    \n",
    "    for i in range(num_batches):  # Iterate over sentences\n",
    "        sentence = train_words[i]      # Word indices for the sentence\n",
    "        tags = train_tags[i]           # Tag indices for the sentence\n",
    "        \n",
    "        # Forward pass\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence)  # Emission scores from the BiLSTM\n",
    "        \n",
    "        # Compute loss using the CRF layer\n",
    "        loss = model.crf(tag_scores, tags)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress and loss\n",
    "        if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "            progress = (i + 1) / num_batches * 100\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}/{num_batches} ({progress:.2f}%), Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f'Epoch {epoch + 1} completed. Average loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359bdb1",
   "metadata": {
    "papermill": {
     "duration": 0.118579,
     "end_time": "2025-03-25T12:45:26.612979",
     "exception": false,
     "start_time": "2025-03-25T12:45:26.494400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Evalutation\n",
    "\n",
    "- Since because I already add the padding into each sentence to so the model will have likely to predict this padding leading the accuracy more higher. So I only count the padding that I predict fail and the padding I right will not add into accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78e5996",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-25T12:45:26.853950Z",
     "iopub.status.busy": "2025-03-25T12:45:26.853402Z",
     "iopub.status.idle": "2025-03-25T12:45:37.276938Z",
     "shell.execute_reply": "2025-03-25T12:45:37.276087Z"
    },
    "papermill": {
     "duration": 10.546547,
     "end_time": "2025-03-25T12:45:37.278357",
     "exception": false,
     "start_time": "2025-03-25T12:45:26.731810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (excluding padding): 93.03%\n",
      "Test Accuracy (including padding): 96.97%\n",
      "Number of non-\"O\" tags predicted: 836\n",
      "\n",
      "Tag prediction statistics:\n",
      "O: 5536\n",
      "B-tim: 112\n",
      "I-tim: 22\n",
      "B-org: 95\n",
      "I-org: 81\n",
      "B-per: 85\n",
      "I-per: 119\n",
      "B-gpe: 87\n",
      "I-art: 1\n",
      "B-geo: 192\n",
      "I-geo: 33\n",
      "B-nat: 1\n",
      "B-eve: 4\n",
      "I-eve: 4\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_with_padding = 0\n",
    "    total_with_padding = 0\n",
    "    non_o_count = 0  # Counter for non-'O' tags\n",
    "    tag_counter = {}  # Dictionary to count tag occurrences\n",
    "    \n",
    "    for i in range(len(test_words)):  # Iterate over sentences\n",
    "        sentence = test_words[i]      # Word indices for the sentence\n",
    "        tags = test_tags[i]           # Ground truth tags\n",
    "        \n",
    "        # Forward pass\n",
    "        tag_scores = model(sentence)  # Emission scores from the BiLSTM\n",
    "        \n",
    "        # Decode the most likely sequence of tags\n",
    "        predicted_tags = model.crf.viterbi_decode(tag_scores)\n",
    "        \n",
    "        # Calculate accuracy (both with and without padding)\n",
    "        for tag_idx, predicted_tag_idx in enumerate(predicted_tags):\n",
    "            # Accuracy including padding\n",
    "            total_with_padding += 1\n",
    "            if predicted_tag_idx == tags[tag_idx]:\n",
    "                correct_with_padding += 1\n",
    "                \n",
    "            # Accuracy excluding padding (original calculation)\n",
    "            if tags[tag_idx] != tag2idx['PAD'] or predicted_tag_idx != tag2idx['PAD']:\n",
    "                if predicted_tag_idx == tags[tag_idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        # Count non-'O' tags and tag statistics\n",
    "        for tag_idx, predicted_tag_idx in enumerate(predicted_tags):\n",
    "            # Skip when both are padding\n",
    "            if tags[tag_idx] != tag2idx['PAD'] or predicted_tag_idx != tag2idx['PAD']:\n",
    "                predicted_tag = list(tag2idx.keys())[list(tag2idx.values()).index(predicted_tag_idx)]  # Get tag name from index\n",
    "                if predicted_tag != 'O':  # Only count non-'O' tags\n",
    "                    non_o_count += 1\n",
    "                \n",
    "                # Count tag occurrences\n",
    "                if predicted_tag in tag_counter:\n",
    "                    tag_counter[predicted_tag] += 1\n",
    "                else:\n",
    "                    tag_counter[predicted_tag] = 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    accuracy_with_padding = correct_with_padding / total_with_padding if total_with_padding > 0 else 0\n",
    "    \n",
    "    print(f'Test Accuracy (excluding padding): {accuracy * 100:.2f}%')\n",
    "    print(f'Test Accuracy (including padding): {accuracy_with_padding * 100:.2f}%')\n",
    "    print(f'Number of non-\"O\" tags predicted: {non_o_count}')\n",
    "    \n",
    "    # Print tag statistics\n",
    "    print(\"\\nTag prediction statistics:\")\n",
    "    for tag, count in tag_counter.items():\n",
    "        print(f'{tag}: {count}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6263.722272,
   "end_time": "2025-03-25T12:45:38.931777",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-25T11:01:15.209505",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
