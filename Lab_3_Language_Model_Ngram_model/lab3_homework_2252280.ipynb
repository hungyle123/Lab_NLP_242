{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model and Application for Spelling Error Correction\n",
    "# Objective: Develop a simple English syntax error correction program\n",
    "Home Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Improve the model by using interpolation smoothing with the \"Stupid Backoff\" method (Brants et al., 2007)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Step 1: Import the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from difflib import get_close_matches\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Step 2: Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = \"1kuqGxsdtU9EMqDKv3cLEE6r7HIYJyzBw\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Step 3: Build the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Step 1: Clean text\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    \n",
    "    # Step 2: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 3: Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Step 4: Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Step 4: Build an old model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def train(self, tokenized_corpus):\n",
    "        for sentence in tokenized_corpus:\n",
    "            sentence = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "            self.vocab.update(sentence)\n",
    "            ngram_list = list(ngrams(sentence, self.n))\n",
    "            \n",
    "            for ngram in ngram_list:\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                self.ngram_counts[context][word] += 1\n",
    "                self.context_counts[context] += 1\n",
    "    \n",
    "    def probability(self, sentence):\n",
    "        tokenized_sentence = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "        ngram_list = list(ngrams(tokenized_sentence, self.n))\n",
    "        prob = 1.0\n",
    "        \n",
    "        for ngram in ngram_list:\n",
    "            context = ngram[:-1]\n",
    "            word = ngram[-1]\n",
    "            prob *= (self.ngram_counts[context][word] + 1) / (self.context_counts[context] + len(self.vocab))\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        prob = self.probability(sentence)\n",
    "        N = len(sentence) + 1  # Including end token\n",
    "        return math.pow(1/prob, 1/N) if prob > 0 else float('inf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Step 5: Build a model class with smoothing \"Stupid Backoff\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel_with_smoothing:\n",
    "    def __init__(self, n, alpha=0.4):\n",
    "        self.n = n  # n-gram size\n",
    "        self.alpha = alpha  # Stupid Backoff discount factor\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, tokenized_corpus):\n",
    "        for sentence in tokenized_corpus:\n",
    "            sentence = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "            self.vocab.update(sentence)\n",
    "            ngram_list = list(ngrams(sentence, self.n))\n",
    "            for ngram in ngram_list:\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                self.ngram_counts[context][word] += 1\n",
    "                self.context_counts[context] += 1\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        tokenized_sentence = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "        ngram_list = list(ngrams(tokenized_sentence, self.n))\n",
    "        prob = 1.0\n",
    "        \n",
    "        for ngram in ngram_list:\n",
    "            context = ngram[:-1]\n",
    "            word = ngram[-1]\n",
    "            while len(context) > 0:  # Backoff down to unigram\n",
    "                if context in self.ngram_counts and word in self.ngram_counts[context]:\n",
    "                    prob *= self.ngram_counts[context][word] / self.context_counts[context]\n",
    "                    break  # stop when found\n",
    "                else:\n",
    "                    prob *= self.alpha  \n",
    "                    context = context[1:]  \n",
    "\n",
    "            if len(context) == 0:  \n",
    "                prob *= (self.ngram_counts[context][word] + 1) / (self.context_counts[context] + len(self.vocab))\n",
    "                \n",
    "        return prob\n",
    "\n",
    "    def perplexity(self, sentence):\n",
    "        prob = self.probability(sentence)\n",
    "        N = len(sentence) + 1  # Including end token\n",
    "        return math.pow(1/max(prob, 1e-40), 1/N)\n",
    "    \n",
    "    def generate_next_word(self, context):\n",
    "        context = tuple(context[-(self.n - 1):]) \n",
    "\n",
    "        while len(context) > 0:\n",
    "            if context in self.ngram_counts:  \n",
    "                return max(self.ngram_counts[context], key=self.ngram_counts[context].get)  \n",
    "            context = context[1:]  \n",
    "\n",
    "        return random.choice(list(self.vocab))  \n",
    "    \n",
    "    def correct_spelling(self, word):\n",
    "        closest_matches = get_close_matches(word, self.vocab, n=1)\n",
    "        return closest_matches[0] if closest_matches else word\n",
    "    \n",
    "    def correct_sentence(self, sentence):\n",
    "        words = sentence\n",
    "        corrected_words = []\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "\n",
    "            if word in self.vocab:\n",
    "                corrected_words.append(word)\n",
    "                continue\n",
    "\n",
    "            closest_matches = get_close_matches(word, self.vocab, n=3, cutoff=0.7)\n",
    "\n",
    "            if not closest_matches:\n",
    "                corrected_words.append(word)\n",
    "                continue\n",
    "\n",
    "            best_word = closest_matches[0]\n",
    "            max_prob = 0\n",
    "\n",
    "            for candidate in closest_matches:\n",
    "                context = tuple(corrected_words[-(self.n - 1):])  \n",
    "                prob = self.ngram_counts[context][candidate] / self.context_counts[context] if context in self.ngram_counts and candidate in self.ngram_counts[context] else 0\n",
    "\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    best_word = candidate\n",
    "\n",
    "            corrected_words.append(best_word)\n",
    "\n",
    "        return \" \".join(corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and train model\n",
    "def load_corpus(data,test_size=0.2, random_state=42):\n",
    "    \n",
    "    sentences = sent_tokenize(data)\n",
    "    tokenized_corpus = [preprocess_text(sent) for sent in sentences]\n",
    "\n",
    "    train_corpus, test_corpus = train_test_split(tokenized_corpus, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return train_corpus, test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, test_corpus = load_corpus(data)\n",
    "n1_without_smoothing = NGramModel(1)\n",
    "n1_without_smoothing.train(train_corpus)\n",
    "n2_without_smoothing = NGramModel(2)\n",
    "n2_without_smoothing.train(train_corpus)\n",
    "n3_without_smoothing = NGramModel(3)\n",
    "n3_without_smoothing.train(train_corpus)\n",
    "\n",
    "\n",
    "n1_with_smoothing = NGramModel_with_smoothing(1)\n",
    "n1_with_smoothing.train(train_corpus)\n",
    "n2_with_smoothing = NGramModel_with_smoothing(2)\n",
    "n2_with_smoothing.train(train_corpus)\n",
    "n3_with_smoothing = NGramModel_with_smoothing(3)\n",
    "n3_with_smoothing.train(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Compare with the results from In Class Exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Take each model result for testing first about a simple sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_analysis = preprocess_text(\"I like to tell you the tale of one of my favorite projects.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------OLD MODEL (DO NOT HAVE SMOOTHING)-------------------\")\n",
    "print(f\"1-gram probability of sentence1 without smoothing phrase: {n1_without_smoothing.probability(sentence_analysis)}\")\n",
    "print(f\"1-gram perplexity of sentence1 without smoothing phrase: {n1_without_smoothing.perplexity(sentence_analysis)}\")\n",
    "print(f\"2-gram probability of sentence1 without smoothing phrase: {n2_without_smoothing.probability(sentence_analysis)}\")\n",
    "print(f\"2-gram perplexity of sentence1 without smoothing phrase: {n2_without_smoothing.perplexity(sentence_analysis)}\")\n",
    "print(f\"3-gram probability of sentence1 without smoothing phrase: {n3_without_smoothing.probability(sentence_analysis)}\")\n",
    "print(f\"3-gram perplexity of sentence1 without smoothing phrase: {n3_without_smoothing.perplexity(sentence_analysis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------NEW MODEL (DO HAVE SMOOTHING)-------------------\")\n",
    "\n",
    "print(f\"1-gram probability of sentence1 with smoothing phrase: {n1_with_smoothing.probability(sentence_analysis)}\")\n",
    "print(f\"1-gram perplexity of sentence1 with smoothing phrase: {n1_with_smoothing.perplexity(sentence_analysis)}\")\n",
    "print(f\"2-gram probability of sentence1 with smoothing phrase: {n2_with_smoothing.probability(sentence_analysis)}\")\n",
    "print(f\"2-gram perplexity of sentence1 with smoothing phrase: {n2_with_smoothing.perplexity(sentence_analysis)}\")\n",
    "print(f\"3-gram probability of sentence1 with smoothing phrase: {n3_with_smoothing.probability(sentence_analysis)}\")\n",
    "print(f\"3-gram perplexity of sentence1 with smoothing phrase: {n3_with_smoothing.perplexity(sentence_analysis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the result with a sentence example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison between the old model (without smoothing) and the new model (with smoothing) highlights the impact of smoothing, particularly for higher-order n-grams. \n",
    "\n",
    "While the 1-gram probabilities and perplexities remain identical in both models, the differences become apparent in 2-gram and 3-gram results. \n",
    "\n",
    "The new model significantly increases the probability estimates for 2-grams and 3-grams, which results in much lower perplexities (51.48 vs. 355.64 for 2-grams and 15.60 vs. 1620.35 for 3-grams). \n",
    "\n",
    "This suggests that the new model, likely using Stupid Backoff, better handles sparsity issues by assigning nonzero probabilities to unseen n-grams, leading to more stable and reasonable perplexity values. In contrast, the old model, lacking smoothing, suffers from extreme sparsity, making higher-order n-gram probabilities extremely small and their perplexities unrealistically high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We test on the test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------OLD MODEL (DO NOT HAVE SMOOTHING)-------------------\")\n",
    "\n",
    "for (index,i) in enumerate(test_corpus):\n",
    "    print(f\"INDEX: {index}\")\n",
    "    print(f\"1-gram probability of sentence1: {n1_without_smoothing.probability(i)}\")\n",
    "    print(f\"1-gram perplexity of sentence1: {n1_without_smoothing.perplexity(i)}\")\n",
    "    print(f\"2-gram probability of sentence1: {n2_without_smoothing.probability(i)}\")\n",
    "    print(f\"2-gram perplexity of sentence1: {n2_without_smoothing.perplexity(i)}\")\n",
    "    print(f\"3-gram probability of sentence1: {n3_without_smoothing.probability(i)}\")\n",
    "    print(f\"3-gram perplexity of sentence1: {n3_without_smoothing.perplexity(i)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------NEW MODEL (DO HAVE SMOOTHING)-------------------\")\n",
    "\n",
    "for (index,i) in enumerate(test_corpus):\n",
    "    print(f\"INDEX: {index}\")\n",
    "    print(f\"1-gram probability of sentence1: {n1_with_smoothing.probability(i)}\")\n",
    "    print(f\"1-gram perplexity of sentence1: {n1_with_smoothing.perplexity(i)}\")\n",
    "    print(f\"2-gram probability of sentence1: {n2_with_smoothing.probability(i)}\")\n",
    "    print(f\"2-gram perplexity of sentence1: {n2_with_smoothing.perplexity(i)}\")\n",
    "    print(f\"3-gram probability of sentence1: {n3_with_smoothing.probability(i)}\")\n",
    "    print(f\"3-gram perplexity of sentence1: {n3_with_smoothing.perplexity(i)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conclusion, using Stupid Backoff significantly improves the modelâ€™s performance by addressing sparsity issues in higher-order n-grams. The new model with smoothing assigns higher probabilities to n-grams, reducing perplexity and making the language model more robust. This improvement is particularly evident in 2-gram and 3-gram cases, where the perplexity drops drastically compared to the unsmoothed model. By incorporating Stupid Backoff, the model better generalizes to unseen data, leading to more stable and realistic probability distributions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Use the newly built model to generate the next words for a given word sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ First, test on some sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_1 = preprocess_text(\"may\")\n",
    "\n",
    "print(f\"1-gram generate_next_word testing:  {n1_with_smoothing.generate_next_word(sample_sentence_1)}\")\n",
    "print(f\"2-gram generate_next_word testing:  {n2_with_smoothing.generate_next_word(sample_sentence_1)}\")\n",
    "print(f\"3-gram generate_next_word testing:  {n3_with_smoothing.generate_next_word(sample_sentence_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_2 = preprocess_text(\"I like\")\n",
    "\n",
    "print(f\"1-gram generate_next_word testing:  {n1_with_smoothing.generate_next_word(sample_sentence_2)}\")\n",
    "print(f\"2-gram generate_next_word testing:  {n2_with_smoothing.generate_next_word(sample_sentence_2)}\")\n",
    "print(f\"3-gram generate_next_word testing:  {n3_with_smoothing.generate_next_word(sample_sentence_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_3 = preprocess_text(\"the tale of\")\n",
    "\n",
    "print(f\"1-gram generate_next_word testing:  {n1_with_smoothing.generate_next_word(sample_sentence_3)}\")\n",
    "print(f\"2-gram generate_next_word testing:  {n2_with_smoothing.generate_next_word(sample_sentence_3)}\")\n",
    "print(f\"3-gram generate_next_word testing:  {n3_with_smoothing.generate_next_word(sample_sentence_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Test on the test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (index,i) in enumerate(test_corpus):\n",
    "    print(f\"INDEX: {index}\")\n",
    "    print(f\"1-gram generate_next_word testing:  {n1_with_smoothing.generate_next_word(i)}\")\n",
    "    print(f\"2-gram generate_next_word testing:  {n2_with_smoothing.generate_next_word(i)}\")\n",
    "    print(f\"3-gram generate_next_word testing:  {n3_with_smoothing.generate_next_word(i)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d) Combine with a function that calculates the distance between words to predict the correct word for a misspelled word position. (from difflib import get_close_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we test on simple 1 word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Test on some simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = \"helo\"\n",
    "print(f\"Predict correct word in 1-gram: {n1_with_smoothing.correct_spelling(word_1)}\")\n",
    "print(f\"Predict correct word in 2-gram: {n2_with_smoothing.correct_spelling(word_1)}\")\n",
    "print(f\"Predict correct word in 3-gram: {n3_with_smoothing.correct_spelling(word_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2 = \"liek\"\n",
    "print(f\"Predict correct word in 1-gram: {n1_with_smoothing.correct_spelling(word_2)}\")\n",
    "print(f\"Predict correct word in 2-gram: {n2_with_smoothing.correct_spelling(word_2)}\")\n",
    "print(f\"Predict correct word in 3-gram: {n3_with_smoothing.correct_spelling(word_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now test on sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We test on some simple sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_correct_1 = preprocess_text(\"I liek to bep\")\n",
    "print(f\"Predict correct sentence in 1-gram: {n1_with_smoothing.correct_sentence(sentence_correct_1)}\")\n",
    "print(f\"Predict correct sentence in 2-gram: {n2_with_smoothing.correct_sentence(sentence_correct_1)}\")\n",
    "print(f\"Predict correct sentence in 3-gram: {n3_with_smoothing.correct_sentence(sentence_correct_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_correct_2 = preprocess_text(\"wishs yoau heva a \")\n",
    "print(f\"Predict correct sentence in 1-gram: {n1_with_smoothing.correct_sentence(sentence_correct_2)}\")\n",
    "print(f\"Predict correct sentence in 2-gram: {n2_with_smoothing.correct_sentence(sentence_correct_2)}\")\n",
    "print(f\"Predict correct sentence in 3-gram: {n3_with_smoothing.correct_sentence(sentence_correct_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Test on test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (index,i) in enumerate(test_corpus):\n",
    "    print(f\"INDEX: {index}\")\n",
    "    print(f\"Predict correct sentence in 1-gram: {n1_with_smoothing.correct_sentence(i)}\")\n",
    "    print(f\"Predict correct sentence in 2-gram: {n2_with_smoothing.correct_sentence(i)}\")\n",
    "    print(f\"Predict correct sentence in 3-gram: {n3_with_smoothing.correct_sentence(i)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
