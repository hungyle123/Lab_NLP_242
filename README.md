# Lab_NLP_242

This repository contains lab exercises for the Natural Language Processing (NLP) course 242. Each lab covers key topics and models commonly used in modern NLP pipelines.

## üìÅ Folder Structure

Each folder contains notebooks and code for a specific topic:

- `Lab_1_Data_Mining`: Introduction to data mining and text collection
- `Lab_2_Preprocessing_Data`: Tokenization, normalization, stopword removal, stemming, lemmatization
- `Lab_3_Language_Model_Ngram_model`: Statistical language modeling with N-grams
- `Lab_4_Vector_Semantic_and_Embedding`: Word2Vec, cosine similarity, semantic relationships
- `Lab_5_Embedding`: Advanced embeddings with Gensim and visualization
- `Lab_6_7_Linear_Model`: Text classification with logistic regression and Naive Bayes
- `Lab_8_Neural_Network`: Feedforward neural networks for text classification
- `Lab_9_Recurrent_Neural_Network`: RNNs for sequential data like sentences
- `Lab_10_Sequence_to_Sequence`: Sequence-to-sequence models for translation and summarization

## üìö What I Learned

Through these labs, I have learned:

- How to **preprocess and clean text data** effectively.
- The theory and implementation of **N-gram models** and their limitations.
- How to use **word embeddings** like Word2Vec to capture semantic similarity.
- Building **text classifiers** using traditional machine learning and linear models.
- Implementing **neural networks (NN, RNN, Seq2Seq)** for more complex NLP tasks like classification and translation.
- The strengths and limitations of each model type in NLP applications.
